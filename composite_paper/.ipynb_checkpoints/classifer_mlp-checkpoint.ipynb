{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.011834859848022\n"
     ]
    }
   ],
   "source": [
    "from math import*\n",
    "import __main__\n",
    "global PI\n",
    "import os\n",
    "import time\n",
    "PI=float(acos(-1))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "# random.seed()\n",
    "\n",
    "data_size = 10000;\n",
    "\n",
    "arr = np.random.randint(1, 10, size=[data_size, 8])\n",
    "data_input = arr.tolist()\n",
    "datasets = data_input\n",
    "\n",
    "def ssh(num):\n",
    "#     global datasets\n",
    "#     arr = np.random.randint(1, 10, size=[data_size, 8])\n",
    "#     datasets = arr.tolist()\n",
    "    ply_angle = datasets[num]\n",
    "    \n",
    "    bas_ply=[-60, -45, -30, -15, 0, 15, 30, 45, 60, 90]\n",
    "\n",
    "    AA1=bas_ply[ply_angle[0]]\n",
    "    AA2=bas_ply[ply_angle[1]]\n",
    "    AA3=bas_ply[ply_angle[2]]\n",
    "    AA4=bas_ply[ply_angle[3]]\n",
    "    AA5=bas_ply[ply_angle[4]]\n",
    "    AA6=bas_ply[ply_angle[5]]\n",
    "    AA7=bas_ply[ply_angle[6]]\n",
    "    AA8=bas_ply[ply_angle[7]]\n",
    "\n",
    "    ### ply stacking sequence###\n",
    "    AAA=[AA1/180.0*PI,AA2/180.0*PI,AA3/180.0*PI,AA4/180.0*PI,AA5/180.0*PI,AA6/180.0*PI,AA7/180.0*PI,AA8/180.0*PI,AA8/180.0*PI,AA7/180.0*PI,AA6/180.0*PI,AA5/180.0*PI,AA4/180.0*PI,AA3/180.0*PI,AA2/180.0*PI,AA1/180.0*PI]\n",
    "    pi=3.14159265358979\n",
    "\n",
    "    R=250.0   ##  radius##\n",
    "    H=510.0   ##  Height##\n",
    "    td=0.125  #layer thickness##\n",
    "\n",
    "    TTT=[-td*8,-td*7,-td*6,-td*5,-td*4,-td*3,-td*2,-td*1,td*0,td*1,td*2,td*3,td*4,td*5,td*6,td*7,td*8]\n",
    "\n",
    "    ###material property###\n",
    "\n",
    "    E1=123550.0  \n",
    "    E2=8707.9\n",
    "    G12=5695.0\n",
    "    miu12=0.31946\n",
    "\n",
    "    miu21=miu12*E2/E1\n",
    "    Q11=E1/(1-miu12*miu21)\n",
    "    Q12=miu21*E1/(1-miu12*miu21)\n",
    "    Q22=E2/(1-miu12*miu21)\n",
    "    Q66=G12\n",
    "\n",
    "    A11=0.0\n",
    "    A12=0.0\n",
    "    A22=0.0\n",
    "    A66=0.0\n",
    "\n",
    "    D11=0.0\n",
    "    D12=0.0\n",
    "    D22=0.0\n",
    "    D66=0.0\n",
    "\n",
    "    for i in range(0,16):\n",
    "        A11=A11+(Q11*cos(AAA[i])**4+2*(Q12+2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q22*sin(AAA[i])**4)*(TTT[i+1]-TTT[i])\n",
    "        A12=A12+((Q11+Q22-4*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q12*(sin(AAA[i])**4+cos(AAA[i])**4))*(TTT[i+1]-TTT[i])\n",
    "        A22=A22+(Q11*sin(AAA[i])**4+2*(Q12+2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q22*cos(AAA[i])**4)*(TTT[i+1]-TTT[i])\n",
    "        A66=A66+((Q11+Q22-2*Q12-2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q66*(sin(AAA[i])**4+cos(AAA[i])**4))*(TTT[i+1]-TTT[i])\n",
    "        D11=D11+(Q11*cos(AAA[i])**4+2*(Q12+2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q22*sin(AAA[i])**4)*(TTT[i+1]**3-TTT[i]**3)/3.0\n",
    "        D12=D12+((Q11+Q22-4*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q12*(sin(AAA[i])**4+cos(AAA[i])**4))*(TTT[i+1]**3-TTT[i]**3)/3.0\n",
    "        D22=D22+(Q11*sin(AAA[i])**4+2*(Q12+2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q22*cos(AAA[i])**4)*(TTT[i+1]**3-TTT[i]**3)/3.0\n",
    "        D66=D66+((Q11+Q22-2*Q12-2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q66*(sin(AAA[i])**4+cos(AAA[i])**4))*(TTT[i+1]**3-TTT[i]**3)/3.0\n",
    "\n",
    "    #####bianliang########\n",
    "\n",
    "    D= 2*R\n",
    "    L= H\n",
    "\n",
    "    #xian\n",
    "    a=[[A11,A12,0],[A12,A22,0],[0,0,A66]]\n",
    "    b=[[0,0,0],[0,0,0],[0,0,0]]\n",
    "    d=[[D11,D12,0],[D12,D22,0],[0,0,D66]]\n",
    "\n",
    "    alpha=PI/L\n",
    "    beta=2/D\n",
    "\n",
    "    mm=50\n",
    "    nn=50\n",
    "    kmm=0\n",
    "    knn=0\n",
    "    kmm11=0\n",
    "    knn11=0\n",
    "    F=[[0 for col in range(nn)] for row in range(mm)]\n",
    "    Fcr=1e16\n",
    "\n",
    "    for m in range(1,mm+1):\n",
    "        for n in range(1,nn+1):\n",
    "            xi11=2*a[0][0]*(m*alpha)**2+2*a[2][2]*(n*beta)**2\n",
    "            xi12=2*(a[0][1]+a[2][2])*m*alpha*n*beta\n",
    "            xi13=4*a[0][1]*m*alpha/D-2*b[0][0]*(m*alpha)**3-2*(b[0][1]+2*b[2][2])*m*alpha*(n*beta)**2\n",
    "            xi22=2*a[1][1]*(n*beta)**2+2*a[2][2]*(m*alpha)**2\n",
    "            xi23=4*a[1][1]*n*beta/D-2*b[1][1]*(n*beta)**3-2*(b[0][1]+2*b[2][2])*(m*alpha)**2*n*beta\n",
    "            xi33=4*(d[0][1]+2*d[2][2])*(m*alpha*n*beta)**2+8*a[1][1]/(D**2)\\\n",
    "            +2*d[0][0]*(m*alpha)**4+2*d[1][1]*(n*beta)**4-8*(b[1][1]*(n*beta)**2+b[0][1]*(m*alpha)**2)/D\n",
    "            xi21=xi12\n",
    "            xi31=xi13\n",
    "            xi32=xi23\n",
    "            det1=xi11*(xi22*xi33-xi32*xi23)-xi12*(xi21*xi33-xi31*xi23)+xi13*(xi21*xi32-xi31*xi22)\n",
    "            det2=xi11*xi22-xi21*xi12\n",
    "            Nx=det1/det2/(2*((m*alpha)**2))\n",
    "            F[m-1][n-1]=Nx*PI*D\n",
    "            if Fcr>F[m-1][n-1]:\n",
    "              Fcr=F[m-1][n-1]\n",
    "              kmm=m\n",
    "              knn=2*n\n",
    "\n",
    "    for m in range(1,mm+1):\n",
    "        xi11=2*a[0][0]*(m*alpha)**2\n",
    "        xi12=0\n",
    "        xi13=4*a[0][1]*m*alpha/D-2*b[0][0]*(m*alpha)**3\n",
    "        xi22=2*a[2][2]*(m*alpha)**2\n",
    "        xi23=0\n",
    "        xi33=8*a[1][1]/(D**2)+2*d[0][0]*(m*alpha)**4-8*b[0][1]*(m*alpha)**2/D\n",
    "        xi21=xi12\n",
    "        xi31=xi13\n",
    "        xi32=xi23\n",
    "        det1=xi11*(xi22*xi33-xi32*xi23)-xi12*(xi21*xi33-xi31*xi23)+xi13*(xi21*xi32-xi31*xi22)\n",
    "        det2=xi11*xi22-xi21*xi12\n",
    "        Nx=det1/det2/(2*((m*alpha)**2))\n",
    "        Fn1=Nx*PI*D\n",
    "        if Fcr>Fn1:\n",
    "          Fcr=Fn1\n",
    "          kmm=m\n",
    "          knn=1\n",
    "\n",
    "    return Fcr\n",
    "\n",
    "\n",
    "# map async parallel \n",
    "# pool = mp.Pool(mp.cpu_count())\n",
    "pool = mp.Pool(processes = 8)\n",
    "\n",
    "start = time.time()\n",
    "results_map = pool.map(ssh, range(data_size))\n",
    "end = time.time() \n",
    "print(end - start) #0.0037827491760253906\n",
    "# print(results_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597568.9929844592\n",
      "torch.Size([10000, 8])\n",
      "torch.Size([10000, 1])\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "results_map_sort = sorted(results_map) # note that sorted change the sort but didn't change the original one\n",
    "# print(results_map_sort)\n",
    "median_value = results_map_sort[data_size//2]\n",
    "print(median_value)\n",
    "\n",
    "results_map_new = np.zeros((data_size,1))\n",
    "# print(type(results_map))\n",
    "output_scikit = np.zeros(data_size)\n",
    "\n",
    "for i in range(data_size):\n",
    "    if results_map[i] > median_value:\n",
    "        results_map_new[i,:] = 1\n",
    "        output_scikit[i] = 1\n",
    "    else:\n",
    "        results_map_new[i,:] = 0\n",
    "        output_scikit[i] = 0\n",
    "        \n",
    "# print(results_map) # new results_map transfer to [0,1]\n",
    "# print(datasets)\n",
    "# type(datasets)\n",
    "# type(results_map)\n",
    "\n",
    "X = torch.FloatTensor(datasets)\n",
    "Y = torch.FloatTensor(results_map_new)\n",
    "# type(data_tensor)\n",
    "#type(output_tensor)\n",
    "print(X.size())\n",
    "print(Y.size())\n",
    "# print(X)\n",
    "# print(Y)\n",
    "\n",
    "\n",
    "print(type(datasets))\n",
    "print(type(output_scikit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training by classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import svm\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# clf = svm.SVC(gamma=0.001,probability=True)\n",
    "# X = datasets\n",
    "# y = results_map\n",
    "\n",
    "# clf.fit(X, y) \n",
    "\n",
    "# score_svm = cross_val_score(clf, X, y, scoring='recall_macro',cv=5)  \n",
    "# print(score_svm)\n",
    "\n",
    "# clf.predict([datasets[3]])\n",
    "# clf.predict_proba([datasets[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69678213\n",
      "Iteration 2, loss = 0.68928098\n",
      "Iteration 3, loss = 0.68244081\n",
      "Iteration 4, loss = 0.67640279\n",
      "Iteration 5, loss = 0.67089795\n",
      "Iteration 6, loss = 0.66591191\n",
      "Iteration 7, loss = 0.66131278\n",
      "Iteration 8, loss = 0.65719275\n",
      "Iteration 9, loss = 0.65313393\n",
      "Iteration 10, loss = 0.64949860\n",
      "Iteration 11, loss = 0.64615978\n",
      "Iteration 12, loss = 0.64305233\n",
      "Iteration 13, loss = 0.64020376\n",
      "Iteration 14, loss = 0.63759449\n",
      "Iteration 15, loss = 0.63518182\n",
      "Iteration 16, loss = 0.63301965\n",
      "Iteration 17, loss = 0.63094703\n",
      "Iteration 18, loss = 0.62911052\n",
      "Iteration 19, loss = 0.62741564\n",
      "Iteration 20, loss = 0.62575368\n",
      "Iteration 21, loss = 0.62429983\n",
      "Iteration 22, loss = 0.62286744\n",
      "Iteration 23, loss = 0.62155508\n",
      "Iteration 24, loss = 0.62031930\n",
      "Iteration 25, loss = 0.61913780\n",
      "Iteration 26, loss = 0.61799141\n",
      "Iteration 27, loss = 0.61695135\n",
      "Iteration 28, loss = 0.61590872\n",
      "Iteration 29, loss = 0.61484772\n",
      "Iteration 30, loss = 0.61387530\n",
      "Iteration 31, loss = 0.61288495\n",
      "Iteration 32, loss = 0.61191949\n",
      "Iteration 33, loss = 0.61101592\n",
      "Iteration 34, loss = 0.61006495\n",
      "Iteration 35, loss = 0.60916322\n",
      "Iteration 36, loss = 0.60827990\n",
      "Iteration 37, loss = 0.60737505\n",
      "Iteration 38, loss = 0.60649910\n",
      "Iteration 39, loss = 0.60570198\n",
      "Iteration 40, loss = 0.60479905\n",
      "Iteration 41, loss = 0.60397299\n",
      "Iteration 42, loss = 0.60309140\n",
      "Iteration 43, loss = 0.60225483\n",
      "Iteration 44, loss = 0.60139865\n",
      "Iteration 45, loss = 0.60059141\n",
      "Iteration 46, loss = 0.59977770\n",
      "Iteration 47, loss = 0.59898661\n",
      "Iteration 48, loss = 0.59811288\n",
      "Iteration 49, loss = 0.59729362\n",
      "Iteration 50, loss = 0.59649642\n",
      "Iteration 51, loss = 0.59569491\n",
      "Iteration 52, loss = 0.59487248\n",
      "Iteration 53, loss = 0.59409110\n",
      "Iteration 54, loss = 0.59329473\n",
      "Iteration 55, loss = 0.59249046\n",
      "Iteration 56, loss = 0.59170111\n",
      "Iteration 57, loss = 0.59092748\n",
      "Iteration 58, loss = 0.59011943\n",
      "Iteration 59, loss = 0.58936769\n",
      "Iteration 60, loss = 0.58853810\n",
      "Iteration 61, loss = 0.58775281\n",
      "Iteration 62, loss = 0.58700763\n",
      "Iteration 63, loss = 0.58624770\n",
      "Iteration 64, loss = 0.58545093\n",
      "Iteration 65, loss = 0.58470101\n",
      "Iteration 66, loss = 0.58403080\n",
      "Iteration 67, loss = 0.58318356\n",
      "Iteration 68, loss = 0.58245893\n",
      "Iteration 69, loss = 0.58171271\n",
      "Iteration 70, loss = 0.58100392\n",
      "Iteration 71, loss = 0.58022934\n",
      "Iteration 72, loss = 0.57950502\n",
      "Iteration 73, loss = 0.57875580\n",
      "Iteration 74, loss = 0.57807908\n",
      "Iteration 75, loss = 0.57734424\n",
      "Iteration 76, loss = 0.57658695\n",
      "Iteration 77, loss = 0.57587280\n",
      "Iteration 78, loss = 0.57516443\n",
      "Iteration 79, loss = 0.57446940\n",
      "Iteration 80, loss = 0.57376697\n",
      "Iteration 81, loss = 0.57309821\n",
      "Iteration 82, loss = 0.57248430\n",
      "Iteration 83, loss = 0.57176445\n",
      "Iteration 84, loss = 0.57104051\n",
      "Iteration 85, loss = 0.57042826\n",
      "Iteration 86, loss = 0.56971755\n",
      "Iteration 87, loss = 0.56903392\n",
      "Iteration 88, loss = 0.56836037\n",
      "Iteration 89, loss = 0.56771713\n",
      "Iteration 90, loss = 0.56707876\n",
      "Iteration 91, loss = 0.56648025\n",
      "Iteration 92, loss = 0.56585795\n",
      "Iteration 93, loss = 0.56521468\n",
      "Iteration 94, loss = 0.56455499\n",
      "Iteration 95, loss = 0.56392936\n",
      "Iteration 96, loss = 0.56329868\n",
      "Iteration 97, loss = 0.56267511\n",
      "Iteration 98, loss = 0.56209196\n",
      "Iteration 99, loss = 0.56147998\n",
      "Iteration 100, loss = 0.56087751\n",
      "Iteration 101, loss = 0.56031881\n",
      "Iteration 102, loss = 0.55973025\n",
      "Iteration 103, loss = 0.55910959\n",
      "Iteration 104, loss = 0.55846803\n",
      "Iteration 105, loss = 0.55785868\n",
      "Iteration 106, loss = 0.55731166\n",
      "Iteration 107, loss = 0.55673762\n",
      "Iteration 108, loss = 0.55608375\n",
      "Iteration 109, loss = 0.55555780\n",
      "Iteration 110, loss = 0.55496461\n",
      "Iteration 111, loss = 0.55451164\n",
      "Iteration 112, loss = 0.55388334\n",
      "Iteration 113, loss = 0.55333894\n",
      "Iteration 114, loss = 0.55275099\n",
      "Iteration 115, loss = 0.55221698\n",
      "Iteration 116, loss = 0.55161806\n",
      "Iteration 117, loss = 0.55102902\n",
      "Iteration 118, loss = 0.55057902\n",
      "Iteration 119, loss = 0.55000201\n",
      "Iteration 120, loss = 0.54936167\n",
      "Iteration 121, loss = 0.54878800\n",
      "Iteration 122, loss = 0.54825692\n",
      "Iteration 123, loss = 0.54772905\n",
      "Iteration 124, loss = 0.54722567\n",
      "Iteration 125, loss = 0.54666211\n",
      "Iteration 126, loss = 0.54606450\n",
      "Iteration 127, loss = 0.54553329\n",
      "Iteration 128, loss = 0.54508324\n",
      "Iteration 129, loss = 0.54446490\n",
      "Iteration 130, loss = 0.54400737\n",
      "Iteration 131, loss = 0.54347669\n",
      "Iteration 132, loss = 0.54302786\n",
      "Iteration 133, loss = 0.54245393\n",
      "Iteration 134, loss = 0.54187162\n",
      "Iteration 135, loss = 0.54133482\n",
      "Iteration 136, loss = 0.54081549\n",
      "Iteration 137, loss = 0.54036006\n",
      "Iteration 138, loss = 0.53979569\n",
      "Iteration 139, loss = 0.53927283\n",
      "Iteration 140, loss = 0.53875569\n",
      "Iteration 141, loss = 0.53823989\n",
      "Iteration 142, loss = 0.53776538\n",
      "Iteration 143, loss = 0.53739102\n",
      "Iteration 144, loss = 0.53671003\n",
      "Iteration 145, loss = 0.53614476\n",
      "Iteration 146, loss = 0.53574104\n",
      "Iteration 147, loss = 0.53521544\n",
      "Iteration 148, loss = 0.53482061\n",
      "Iteration 149, loss = 0.53413422\n",
      "Iteration 150, loss = 0.53377723\n",
      "Iteration 151, loss = 0.53327123\n",
      "Iteration 152, loss = 0.53282839\n",
      "Iteration 153, loss = 0.53213010\n",
      "Iteration 154, loss = 0.53171163\n",
      "Iteration 155, loss = 0.53124885\n",
      "Iteration 156, loss = 0.53069075\n",
      "Iteration 157, loss = 0.53019306\n",
      "Iteration 158, loss = 0.52981349\n",
      "Iteration 159, loss = 0.52919602\n",
      "Iteration 160, loss = 0.52867360\n",
      "Iteration 161, loss = 0.52818414\n",
      "Iteration 162, loss = 0.52773503\n",
      "Iteration 163, loss = 0.52719365\n",
      "Iteration 164, loss = 0.52669255\n",
      "Iteration 165, loss = 0.52624374\n",
      "Iteration 166, loss = 0.52575817\n",
      "Iteration 167, loss = 0.52532783\n",
      "Iteration 168, loss = 0.52470025\n",
      "Iteration 169, loss = 0.52426921\n",
      "Iteration 170, loss = 0.52384399\n",
      "Iteration 171, loss = 0.52336140\n",
      "Iteration 172, loss = 0.52291111\n",
      "Iteration 173, loss = 0.52234878\n",
      "Iteration 174, loss = 0.52186386\n",
      "Iteration 175, loss = 0.52128409\n",
      "Iteration 176, loss = 0.52087235\n",
      "Iteration 177, loss = 0.52035530\n",
      "Iteration 178, loss = 0.51983648\n",
      "Iteration 179, loss = 0.51941813\n",
      "Iteration 180, loss = 0.51880509\n",
      "Iteration 181, loss = 0.51848307\n",
      "Iteration 182, loss = 0.51781554\n",
      "Iteration 183, loss = 0.51741610\n",
      "Iteration 184, loss = 0.51697866\n",
      "Iteration 185, loss = 0.51644547\n",
      "Iteration 186, loss = 0.51596119\n",
      "Iteration 187, loss = 0.51547558\n",
      "Iteration 188, loss = 0.51502282\n",
      "Iteration 189, loss = 0.51458705\n",
      "Iteration 190, loss = 0.51402163\n",
      "Iteration 191, loss = 0.51351075\n",
      "Iteration 192, loss = 0.51305647\n",
      "Iteration 193, loss = 0.51248240\n",
      "Iteration 194, loss = 0.51211066\n",
      "Iteration 195, loss = 0.51154402\n",
      "Iteration 196, loss = 0.51100410\n",
      "Iteration 197, loss = 0.51055554\n",
      "Iteration 198, loss = 0.51012156\n",
      "Iteration 199, loss = 0.50956647\n",
      "Iteration 200, loss = 0.50908231\n",
      "Iteration 201, loss = 0.50859528\n",
      "Iteration 202, loss = 0.50813683\n",
      "Iteration 203, loss = 0.50768830\n",
      "Iteration 204, loss = 0.50716249\n",
      "Iteration 205, loss = 0.50666269\n",
      "Iteration 206, loss = 0.50626150\n",
      "Iteration 207, loss = 0.50567558\n",
      "Iteration 208, loss = 0.50514755\n",
      "Iteration 209, loss = 0.50457510\n",
      "Iteration 210, loss = 0.50414716\n",
      "Iteration 211, loss = 0.50358650\n",
      "Iteration 212, loss = 0.50312268\n",
      "Iteration 213, loss = 0.50256916\n",
      "Iteration 214, loss = 0.50210401\n",
      "Iteration 215, loss = 0.50161541\n",
      "Iteration 216, loss = 0.50104075\n",
      "Iteration 217, loss = 0.50065806\n",
      "Iteration 218, loss = 0.50020306\n",
      "Iteration 219, loss = 0.49963392\n",
      "Iteration 220, loss = 0.49911626\n",
      "Iteration 221, loss = 0.49854013\n",
      "Iteration 222, loss = 0.49806380\n",
      "Iteration 223, loss = 0.49773979\n",
      "Iteration 224, loss = 0.49713170\n",
      "Iteration 225, loss = 0.49663542\n",
      "Iteration 226, loss = 0.49631040\n",
      "Iteration 227, loss = 0.49559394\n",
      "Iteration 228, loss = 0.49517964\n",
      "Iteration 229, loss = 0.49464573\n",
      "Iteration 230, loss = 0.49411555\n",
      "Iteration 231, loss = 0.49347600\n",
      "Iteration 232, loss = 0.49306759\n",
      "Iteration 233, loss = 0.49259728\n",
      "Iteration 234, loss = 0.49201547\n",
      "Iteration 235, loss = 0.49156712\n",
      "Iteration 236, loss = 0.49108795\n",
      "Iteration 237, loss = 0.49054518\n",
      "Iteration 238, loss = 0.49000021\n",
      "Iteration 239, loss = 0.48950910\n",
      "Iteration 240, loss = 0.48926011\n",
      "Iteration 241, loss = 0.48857430\n",
      "Iteration 242, loss = 0.48782279\n",
      "Iteration 243, loss = 0.48735005\n",
      "Iteration 244, loss = 0.48681145\n",
      "Iteration 245, loss = 0.48628535\n",
      "Iteration 246, loss = 0.48585757\n",
      "Iteration 247, loss = 0.48535716\n",
      "Iteration 248, loss = 0.48470676\n",
      "Iteration 249, loss = 0.48424141\n",
      "Iteration 250, loss = 0.48367784\n",
      "Iteration 251, loss = 0.48326230\n",
      "Iteration 252, loss = 0.48274893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.48228312\n",
      "Iteration 254, loss = 0.48163029\n",
      "Iteration 255, loss = 0.48100577\n",
      "Iteration 256, loss = 0.48046357\n",
      "Iteration 257, loss = 0.47999434\n",
      "Iteration 258, loss = 0.47939774\n",
      "Iteration 259, loss = 0.47890104\n",
      "Iteration 260, loss = 0.47873062\n",
      "Iteration 261, loss = 0.47792484\n",
      "Iteration 262, loss = 0.47746378\n",
      "Iteration 263, loss = 0.47673140\n",
      "Iteration 264, loss = 0.47625946\n",
      "Iteration 265, loss = 0.47576633\n",
      "Iteration 266, loss = 0.47519835\n",
      "Iteration 267, loss = 0.47463497\n",
      "Iteration 268, loss = 0.47398654\n",
      "Iteration 269, loss = 0.47341149\n",
      "Iteration 270, loss = 0.47295495\n",
      "Iteration 271, loss = 0.47238662\n",
      "Iteration 272, loss = 0.47182572\n",
      "Iteration 273, loss = 0.47136804\n",
      "Iteration 274, loss = 0.47074587\n",
      "Iteration 275, loss = 0.47021464\n",
      "Iteration 276, loss = 0.46961001\n",
      "Iteration 277, loss = 0.46918614\n",
      "Iteration 278, loss = 0.46857328\n",
      "Iteration 279, loss = 0.46797083\n",
      "Iteration 280, loss = 0.46750110\n",
      "Iteration 281, loss = 0.46694085\n",
      "Iteration 282, loss = 0.46642870\n",
      "Iteration 283, loss = 0.46580310\n",
      "Iteration 284, loss = 0.46534074\n",
      "Iteration 285, loss = 0.46476008\n",
      "Iteration 286, loss = 0.46422266\n",
      "Iteration 287, loss = 0.46356411\n",
      "Iteration 288, loss = 0.46309476\n",
      "Iteration 289, loss = 0.46254390\n",
      "Iteration 290, loss = 0.46205909\n",
      "Iteration 291, loss = 0.46139407\n",
      "Iteration 292, loss = 0.46078478\n",
      "Iteration 293, loss = 0.46037869\n",
      "Iteration 294, loss = 0.45974359\n",
      "Iteration 295, loss = 0.45938782\n",
      "Iteration 296, loss = 0.45874640\n",
      "Iteration 297, loss = 0.45828890\n",
      "Iteration 298, loss = 0.45766625\n",
      "Iteration 299, loss = 0.45706179\n",
      "Iteration 300, loss = 0.45676547\n",
      "Iteration 301, loss = 0.45617476\n",
      "Iteration 302, loss = 0.45566482\n",
      "Iteration 303, loss = 0.45507818\n",
      "Iteration 304, loss = 0.45473210\n",
      "Iteration 305, loss = 0.45387990\n",
      "Iteration 306, loss = 0.45331654\n",
      "Iteration 307, loss = 0.45297099\n",
      "Iteration 308, loss = 0.45253032\n",
      "Iteration 309, loss = 0.45182009\n",
      "Iteration 310, loss = 0.45137033\n",
      "Iteration 311, loss = 0.45096285\n",
      "Iteration 312, loss = 0.45026707\n",
      "Iteration 313, loss = 0.44986697\n",
      "Iteration 314, loss = 0.44945371\n",
      "Iteration 315, loss = 0.44870630\n",
      "Iteration 316, loss = 0.44832081\n",
      "Iteration 317, loss = 0.44769599\n",
      "Iteration 318, loss = 0.44739754\n",
      "Iteration 319, loss = 0.44666685\n",
      "Iteration 320, loss = 0.44612082\n",
      "Iteration 321, loss = 0.44570321\n",
      "Iteration 322, loss = 0.44541436\n",
      "Iteration 323, loss = 0.44489675\n",
      "Iteration 324, loss = 0.44417813\n",
      "Iteration 325, loss = 0.44383100\n",
      "Iteration 326, loss = 0.44324345\n",
      "Iteration 327, loss = 0.44263487\n",
      "Iteration 328, loss = 0.44232733\n",
      "Iteration 329, loss = 0.44171182\n",
      "Iteration 330, loss = 0.44120586\n",
      "Iteration 331, loss = 0.44099916\n",
      "Iteration 332, loss = 0.44023851\n",
      "Iteration 333, loss = 0.43979155\n",
      "Iteration 334, loss = 0.43926487\n",
      "Iteration 335, loss = 0.43909212\n",
      "Iteration 336, loss = 0.43839835\n",
      "Iteration 337, loss = 0.43805140\n",
      "Iteration 338, loss = 0.43726334\n",
      "Iteration 339, loss = 0.43682004\n",
      "Iteration 340, loss = 0.43648002\n",
      "Iteration 341, loss = 0.43576039\n",
      "Iteration 342, loss = 0.43540860\n",
      "Iteration 343, loss = 0.43479999\n",
      "Iteration 344, loss = 0.43459299\n",
      "Iteration 345, loss = 0.43409753\n",
      "Iteration 346, loss = 0.43370573\n",
      "Iteration 347, loss = 0.43320525\n",
      "Iteration 348, loss = 0.43252770\n",
      "Iteration 349, loss = 0.43211071\n",
      "Iteration 350, loss = 0.43153124\n",
      "Iteration 351, loss = 0.43113583\n",
      "Iteration 352, loss = 0.43076584\n",
      "Iteration 353, loss = 0.43047681\n",
      "Iteration 354, loss = 0.42979578\n",
      "Iteration 355, loss = 0.42916427\n",
      "Iteration 356, loss = 0.42864192\n",
      "Iteration 357, loss = 0.42815957\n",
      "Iteration 358, loss = 0.42794701\n",
      "Iteration 359, loss = 0.42730493\n",
      "Iteration 360, loss = 0.42663022\n",
      "Iteration 361, loss = 0.42623249\n",
      "Iteration 362, loss = 0.42583675\n",
      "Iteration 363, loss = 0.42554657\n",
      "Iteration 364, loss = 0.42508492\n",
      "Iteration 365, loss = 0.42459803\n",
      "Iteration 366, loss = 0.42407494\n",
      "Iteration 367, loss = 0.42352369\n",
      "Iteration 368, loss = 0.42324618\n",
      "Iteration 369, loss = 0.42274202\n",
      "Iteration 370, loss = 0.42221512\n",
      "Iteration 371, loss = 0.42175661\n",
      "Iteration 372, loss = 0.42138666\n",
      "Iteration 373, loss = 0.42089073\n",
      "Iteration 374, loss = 0.42087714\n",
      "Iteration 375, loss = 0.42011534\n",
      "Iteration 376, loss = 0.41937500\n",
      "Iteration 377, loss = 0.41909106\n",
      "Iteration 378, loss = 0.41871079\n",
      "Iteration 379, loss = 0.41840612\n",
      "Iteration 380, loss = 0.41762574\n",
      "Iteration 381, loss = 0.41743517\n",
      "Iteration 382, loss = 0.41685644\n",
      "Iteration 383, loss = 0.41660089\n",
      "Iteration 384, loss = 0.41603517\n",
      "Iteration 385, loss = 0.41542290\n",
      "Iteration 386, loss = 0.41514528\n",
      "Iteration 387, loss = 0.41491681\n",
      "Iteration 388, loss = 0.41458475\n",
      "Iteration 389, loss = 0.41395703\n",
      "Iteration 390, loss = 0.41339900\n",
      "Iteration 391, loss = 0.41297878\n",
      "Iteration 392, loss = 0.41247786\n",
      "Iteration 393, loss = 0.41233527\n",
      "Iteration 394, loss = 0.41163625\n",
      "Iteration 395, loss = 0.41125462\n",
      "Iteration 396, loss = 0.41077139\n",
      "Iteration 397, loss = 0.41030567\n",
      "Iteration 398, loss = 0.40999270\n",
      "Iteration 399, loss = 0.40958985\n",
      "Iteration 400, loss = 0.40910108\n",
      "Iteration 401, loss = 0.40876864\n",
      "Iteration 402, loss = 0.40823846\n",
      "Iteration 403, loss = 0.40789981\n",
      "Iteration 404, loss = 0.40746562\n",
      "Iteration 405, loss = 0.40699771\n",
      "Iteration 406, loss = 0.40647334\n",
      "Iteration 407, loss = 0.40622901\n",
      "Iteration 408, loss = 0.40556706\n",
      "Iteration 409, loss = 0.40537958\n",
      "Iteration 410, loss = 0.40475097\n",
      "Iteration 411, loss = 0.40466185\n",
      "Iteration 412, loss = 0.40412200\n",
      "Iteration 413, loss = 0.40363350\n",
      "Iteration 414, loss = 0.40301721\n",
      "Iteration 415, loss = 0.40298710\n",
      "Iteration 416, loss = 0.40245587\n",
      "Iteration 417, loss = 0.40205213\n",
      "Iteration 418, loss = 0.40141023\n",
      "Iteration 419, loss = 0.40105213\n",
      "Iteration 420, loss = 0.40070425\n",
      "Iteration 421, loss = 0.40058273\n",
      "Iteration 422, loss = 0.39979525\n",
      "Iteration 423, loss = 0.39955262\n",
      "Iteration 424, loss = 0.39938044\n",
      "Iteration 425, loss = 0.39866422\n",
      "Iteration 426, loss = 0.39822527\n",
      "Iteration 427, loss = 0.39809463\n",
      "Iteration 428, loss = 0.39760658\n",
      "Iteration 429, loss = 0.39731341\n",
      "Iteration 430, loss = 0.39673559\n",
      "Iteration 431, loss = 0.39623143\n",
      "Iteration 432, loss = 0.39603543\n",
      "Iteration 433, loss = 0.39556593\n",
      "Iteration 434, loss = 0.39538133\n",
      "Iteration 435, loss = 0.39442474\n",
      "Iteration 436, loss = 0.39444141\n",
      "Iteration 437, loss = 0.39395671\n",
      "Iteration 438, loss = 0.39370263\n",
      "Iteration 439, loss = 0.39321411\n",
      "Iteration 440, loss = 0.39273294\n",
      "Iteration 441, loss = 0.39251286\n",
      "Iteration 442, loss = 0.39209517\n",
      "Iteration 443, loss = 0.39151407\n",
      "Iteration 444, loss = 0.39119268\n",
      "Iteration 445, loss = 0.39093172\n",
      "Iteration 446, loss = 0.39040442\n",
      "Iteration 447, loss = 0.38981903\n",
      "Iteration 448, loss = 0.38967218\n",
      "Iteration 449, loss = 0.38917034\n",
      "Iteration 450, loss = 0.38885466\n",
      "Iteration 451, loss = 0.38838801\n",
      "Iteration 452, loss = 0.38791011\n",
      "Iteration 453, loss = 0.38794951\n",
      "Iteration 454, loss = 0.38738032\n",
      "Iteration 455, loss = 0.38726004\n",
      "Iteration 456, loss = 0.38664454\n",
      "Iteration 457, loss = 0.38587934\n",
      "Iteration 458, loss = 0.38582359\n",
      "Iteration 459, loss = 0.38518463\n",
      "Iteration 460, loss = 0.38490609\n",
      "Iteration 461, loss = 0.38439093\n",
      "Iteration 462, loss = 0.38421758\n",
      "Iteration 463, loss = 0.38372860\n",
      "Iteration 464, loss = 0.38375430\n",
      "Iteration 465, loss = 0.38329365\n",
      "Iteration 466, loss = 0.38271816\n",
      "Iteration 467, loss = 0.38214800\n",
      "Iteration 468, loss = 0.38180611\n",
      "Iteration 469, loss = 0.38136123\n",
      "Iteration 470, loss = 0.38143140\n",
      "Iteration 471, loss = 0.38061484\n",
      "Iteration 472, loss = 0.38040773\n",
      "Iteration 473, loss = 0.38005709\n",
      "Iteration 474, loss = 0.37947188\n",
      "Iteration 475, loss = 0.37902821\n",
      "Iteration 476, loss = 0.37874049\n",
      "Iteration 477, loss = 0.37865490\n",
      "Iteration 478, loss = 0.37811734\n",
      "Iteration 479, loss = 0.37787871\n",
      "Iteration 480, loss = 0.37715042\n",
      "Iteration 481, loss = 0.37688998\n",
      "Iteration 482, loss = 0.37653221\n",
      "Iteration 483, loss = 0.37618012\n",
      "Iteration 484, loss = 0.37555867\n",
      "Iteration 485, loss = 0.37543457\n",
      "Iteration 486, loss = 0.37499296\n",
      "Iteration 487, loss = 0.37437988\n",
      "Iteration 488, loss = 0.37396625\n",
      "Iteration 489, loss = 0.37391839\n",
      "Iteration 490, loss = 0.37349123\n",
      "Iteration 491, loss = 0.37299667\n",
      "Iteration 492, loss = 0.37275107\n",
      "Iteration 493, loss = 0.37222575\n",
      "Iteration 494, loss = 0.37184469\n",
      "Iteration 495, loss = 0.37148486\n",
      "Iteration 496, loss = 0.37133152\n",
      "Iteration 497, loss = 0.37066820\n",
      "Iteration 498, loss = 0.37031963\n",
      "Iteration 499, loss = 0.37026477\n",
      "Iteration 500, loss = 0.36948203\n",
      "Iteration 501, loss = 0.36928514\n",
      "Iteration 502, loss = 0.36905583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 503, loss = 0.36857278\n",
      "Iteration 504, loss = 0.36821234\n",
      "Iteration 505, loss = 0.36817424\n",
      "Iteration 506, loss = 0.36768997\n",
      "Iteration 507, loss = 0.36717292\n",
      "Iteration 508, loss = 0.36709881\n",
      "Iteration 509, loss = 0.36630526\n",
      "Iteration 510, loss = 0.36617038\n",
      "Iteration 511, loss = 0.36610028\n",
      "Iteration 512, loss = 0.36543610\n",
      "Iteration 513, loss = 0.36485423\n",
      "Iteration 514, loss = 0.36463316\n",
      "Iteration 515, loss = 0.36428800\n",
      "Iteration 516, loss = 0.36404218\n",
      "Iteration 517, loss = 0.36373921\n",
      "Iteration 518, loss = 0.36318267\n",
      "Iteration 519, loss = 0.36284680\n",
      "Iteration 520, loss = 0.36263303\n",
      "Iteration 521, loss = 0.36235591\n",
      "Iteration 522, loss = 0.36183922\n",
      "Iteration 523, loss = 0.36160168\n",
      "Iteration 524, loss = 0.36136641\n",
      "Iteration 525, loss = 0.36069398\n",
      "Iteration 526, loss = 0.36096166\n",
      "Iteration 527, loss = 0.36054954\n",
      "Iteration 528, loss = 0.35995172\n",
      "Iteration 529, loss = 0.35974288\n",
      "Iteration 530, loss = 0.35932360\n",
      "Iteration 531, loss = 0.35877344\n",
      "Iteration 532, loss = 0.35889391\n",
      "Iteration 533, loss = 0.35812991\n",
      "Iteration 534, loss = 0.35806966\n",
      "Iteration 535, loss = 0.35787026\n",
      "Iteration 536, loss = 0.35756449\n",
      "Iteration 537, loss = 0.35681499\n",
      "Iteration 538, loss = 0.35729212\n",
      "Iteration 539, loss = 0.35635833\n",
      "Iteration 540, loss = 0.35593983\n",
      "Iteration 541, loss = 0.35582121\n",
      "Iteration 542, loss = 0.35520384\n",
      "Iteration 543, loss = 0.35488685\n",
      "Iteration 544, loss = 0.35491984\n",
      "Iteration 545, loss = 0.35436452\n",
      "Iteration 546, loss = 0.35400040\n",
      "Iteration 547, loss = 0.35401088\n",
      "Iteration 548, loss = 0.35381916\n",
      "Iteration 549, loss = 0.35310634\n",
      "Iteration 550, loss = 0.35286409\n",
      "Iteration 551, loss = 0.35277675\n",
      "Iteration 552, loss = 0.35210985\n",
      "Iteration 553, loss = 0.35196091\n",
      "Iteration 554, loss = 0.35134407\n",
      "Iteration 555, loss = 0.35115079\n",
      "Iteration 556, loss = 0.35090275\n",
      "Iteration 557, loss = 0.35048040\n",
      "Iteration 558, loss = 0.35034967\n",
      "Iteration 559, loss = 0.34993197\n",
      "Iteration 560, loss = 0.34968620\n",
      "Iteration 561, loss = 0.34942550\n",
      "Iteration 562, loss = 0.34913978\n",
      "Iteration 563, loss = 0.34855949\n",
      "Iteration 564, loss = 0.34824188\n",
      "Iteration 565, loss = 0.34839356\n",
      "Iteration 566, loss = 0.34812439\n",
      "Iteration 567, loss = 0.34735077\n",
      "Iteration 568, loss = 0.34739606\n",
      "Iteration 569, loss = 0.34705833\n",
      "Iteration 570, loss = 0.34662338\n",
      "Iteration 571, loss = 0.34652444\n",
      "Iteration 572, loss = 0.34601227\n",
      "Iteration 573, loss = 0.34552403\n",
      "Iteration 574, loss = 0.34531026\n",
      "Iteration 575, loss = 0.34514740\n",
      "Iteration 576, loss = 0.34540569\n",
      "Iteration 577, loss = 0.34464802\n",
      "Iteration 578, loss = 0.34411776\n",
      "Iteration 579, loss = 0.34401414\n",
      "Iteration 580, loss = 0.34358520\n",
      "Iteration 581, loss = 0.34385929\n",
      "Iteration 582, loss = 0.34319656\n",
      "Iteration 583, loss = 0.34274941\n",
      "Iteration 584, loss = 0.34235815\n",
      "Iteration 585, loss = 0.34235610\n",
      "Iteration 586, loss = 0.34217949\n",
      "Iteration 587, loss = 0.34173904\n",
      "Iteration 588, loss = 0.34169465\n",
      "Iteration 589, loss = 0.34131265\n",
      "Iteration 590, loss = 0.34079813\n",
      "Iteration 591, loss = 0.34056618\n",
      "Iteration 592, loss = 0.34017717\n",
      "Iteration 593, loss = 0.33953329\n",
      "Iteration 594, loss = 0.33979919\n",
      "Iteration 595, loss = 0.33951674\n",
      "Iteration 596, loss = 0.33891788\n",
      "Iteration 597, loss = 0.33871305\n",
      "Iteration 598, loss = 0.33916185\n",
      "Iteration 599, loss = 0.33830856\n",
      "Iteration 600, loss = 0.33782316\n",
      "Iteration 601, loss = 0.33734131\n",
      "Iteration 602, loss = 0.33715938\n",
      "Iteration 603, loss = 0.33700260\n",
      "Iteration 604, loss = 0.33674487\n",
      "Iteration 605, loss = 0.33648304\n",
      "Iteration 606, loss = 0.33645128\n",
      "Iteration 607, loss = 0.33549211\n",
      "Iteration 608, loss = 0.33559218\n",
      "Iteration 609, loss = 0.33547565\n",
      "Iteration 610, loss = 0.33506172\n",
      "Iteration 611, loss = 0.33491414\n",
      "Iteration 612, loss = 0.33434880\n",
      "Iteration 613, loss = 0.33459988\n",
      "Iteration 614, loss = 0.33381582\n",
      "Iteration 615, loss = 0.33372720\n",
      "Iteration 616, loss = 0.33325275\n",
      "Iteration 617, loss = 0.33312590\n",
      "Iteration 618, loss = 0.33274609\n",
      "Iteration 619, loss = 0.33235663\n",
      "Iteration 620, loss = 0.33212198\n",
      "Iteration 621, loss = 0.33166826\n",
      "Iteration 622, loss = 0.33189634\n",
      "Iteration 623, loss = 0.33112299\n",
      "Iteration 624, loss = 0.33105081\n",
      "Iteration 625, loss = 0.33037921\n",
      "Iteration 626, loss = 0.33124293\n",
      "Iteration 627, loss = 0.33047959\n",
      "Iteration 628, loss = 0.32986263\n",
      "Iteration 629, loss = 0.32971043\n",
      "Iteration 630, loss = 0.32915370\n",
      "Iteration 631, loss = 0.32901394\n",
      "Iteration 632, loss = 0.32882262\n",
      "Iteration 633, loss = 0.32842559\n",
      "Iteration 634, loss = 0.32815044\n",
      "Iteration 635, loss = 0.32820606\n",
      "Iteration 636, loss = 0.32774474\n",
      "Iteration 637, loss = 0.32733019\n",
      "Iteration 638, loss = 0.32751039\n",
      "Iteration 639, loss = 0.32682118\n",
      "Iteration 640, loss = 0.32660404\n",
      "Iteration 641, loss = 0.32692225\n",
      "Iteration 642, loss = 0.32628430\n",
      "Iteration 643, loss = 0.32578405\n",
      "Iteration 644, loss = 0.32595444\n",
      "Iteration 645, loss = 0.32529525\n",
      "Iteration 646, loss = 0.32500323\n",
      "Iteration 647, loss = 0.32465057\n",
      "Iteration 648, loss = 0.32453711\n",
      "Iteration 649, loss = 0.32444533\n",
      "Iteration 650, loss = 0.32402906\n",
      "Iteration 651, loss = 0.32381596\n",
      "Iteration 652, loss = 0.32375634\n",
      "Iteration 653, loss = 0.32296153\n",
      "Iteration 654, loss = 0.32302152\n",
      "Iteration 655, loss = 0.32345227\n",
      "Iteration 656, loss = 0.32286486\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69727844\n",
      "Iteration 2, loss = 0.68946053\n",
      "Iteration 3, loss = 0.68238690\n",
      "Iteration 4, loss = 0.67596323\n",
      "Iteration 5, loss = 0.67031399\n",
      "Iteration 6, loss = 0.66513675\n",
      "Iteration 7, loss = 0.66031178\n",
      "Iteration 8, loss = 0.65593729\n",
      "Iteration 9, loss = 0.65186568\n",
      "Iteration 10, loss = 0.64814665\n",
      "Iteration 11, loss = 0.64474254\n",
      "Iteration 12, loss = 0.64157060\n",
      "Iteration 13, loss = 0.63869577\n",
      "Iteration 14, loss = 0.63604627\n",
      "Iteration 15, loss = 0.63365316\n",
      "Iteration 16, loss = 0.63145094\n",
      "Iteration 17, loss = 0.62943249\n",
      "Iteration 18, loss = 0.62760848\n",
      "Iteration 19, loss = 0.62588316\n",
      "Iteration 20, loss = 0.62432990\n",
      "Iteration 21, loss = 0.62284542\n",
      "Iteration 22, loss = 0.62145869\n",
      "Iteration 23, loss = 0.62020049\n",
      "Iteration 24, loss = 0.61896194\n",
      "Iteration 25, loss = 0.61781215\n",
      "Iteration 26, loss = 0.61670137\n",
      "Iteration 27, loss = 0.61565248\n",
      "Iteration 28, loss = 0.61456842\n",
      "Iteration 29, loss = 0.61360595\n",
      "Iteration 30, loss = 0.61262225\n",
      "Iteration 31, loss = 0.61169043\n",
      "Iteration 32, loss = 0.61075383\n",
      "Iteration 33, loss = 0.60983687\n",
      "Iteration 34, loss = 0.60894083\n",
      "Iteration 35, loss = 0.60801660\n",
      "Iteration 36, loss = 0.60711944\n",
      "Iteration 37, loss = 0.60629885\n",
      "Iteration 38, loss = 0.60541787\n",
      "Iteration 39, loss = 0.60453627\n",
      "Iteration 40, loss = 0.60366301\n",
      "Iteration 41, loss = 0.60278144\n",
      "Iteration 42, loss = 0.60200134\n",
      "Iteration 43, loss = 0.60114948\n",
      "Iteration 44, loss = 0.60031805\n",
      "Iteration 45, loss = 0.59950087\n",
      "Iteration 46, loss = 0.59868822\n",
      "Iteration 47, loss = 0.59784890\n",
      "Iteration 48, loss = 0.59703840\n",
      "Iteration 49, loss = 0.59623110\n",
      "Iteration 50, loss = 0.59544707\n",
      "Iteration 51, loss = 0.59461864\n",
      "Iteration 52, loss = 0.59386693\n",
      "Iteration 53, loss = 0.59303484\n",
      "Iteration 54, loss = 0.59222681\n",
      "Iteration 55, loss = 0.59146082\n",
      "Iteration 56, loss = 0.59063619\n",
      "Iteration 57, loss = 0.58989255\n",
      "Iteration 58, loss = 0.58912792\n",
      "Iteration 59, loss = 0.58829023\n",
      "Iteration 60, loss = 0.58748143\n",
      "Iteration 61, loss = 0.58674425\n",
      "Iteration 62, loss = 0.58596846\n",
      "Iteration 63, loss = 0.58520802\n",
      "Iteration 64, loss = 0.58445363\n",
      "Iteration 65, loss = 0.58367234\n",
      "Iteration 66, loss = 0.58293014\n",
      "Iteration 67, loss = 0.58225591\n",
      "Iteration 68, loss = 0.58150087\n",
      "Iteration 69, loss = 0.58068762\n",
      "Iteration 70, loss = 0.57997289\n",
      "Iteration 71, loss = 0.57923313\n",
      "Iteration 72, loss = 0.57851838\n",
      "Iteration 73, loss = 0.57780538\n",
      "Iteration 74, loss = 0.57704286\n",
      "Iteration 75, loss = 0.57630930\n",
      "Iteration 76, loss = 0.57565069\n",
      "Iteration 77, loss = 0.57489340\n",
      "Iteration 78, loss = 0.57418573\n",
      "Iteration 79, loss = 0.57349811\n",
      "Iteration 80, loss = 0.57282459\n",
      "Iteration 81, loss = 0.57210271\n",
      "Iteration 82, loss = 0.57139846\n",
      "Iteration 83, loss = 0.57074786\n",
      "Iteration 84, loss = 0.57002766\n",
      "Iteration 85, loss = 0.56943023\n",
      "Iteration 86, loss = 0.56872445\n",
      "Iteration 87, loss = 0.56807266\n",
      "Iteration 88, loss = 0.56740000\n",
      "Iteration 89, loss = 0.56677359\n",
      "Iteration 90, loss = 0.56613351\n",
      "Iteration 91, loss = 0.56547897\n",
      "Iteration 92, loss = 0.56486843\n",
      "Iteration 93, loss = 0.56421824\n",
      "Iteration 94, loss = 0.56357906\n",
      "Iteration 95, loss = 0.56295497\n",
      "Iteration 96, loss = 0.56231654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 97, loss = 0.56172893\n",
      "Iteration 98, loss = 0.56117695\n",
      "Iteration 99, loss = 0.56053603\n",
      "Iteration 100, loss = 0.55996876\n",
      "Iteration 101, loss = 0.55936160\n",
      "Iteration 102, loss = 0.55874093\n",
      "Iteration 103, loss = 0.55813657\n",
      "Iteration 104, loss = 0.55758037\n",
      "Iteration 105, loss = 0.55700693\n",
      "Iteration 106, loss = 0.55643111\n",
      "Iteration 107, loss = 0.55585585\n",
      "Iteration 108, loss = 0.55528665\n",
      "Iteration 109, loss = 0.55469970\n",
      "Iteration 110, loss = 0.55414623\n",
      "Iteration 111, loss = 0.55356975\n",
      "Iteration 112, loss = 0.55302817\n",
      "Iteration 113, loss = 0.55257228\n",
      "Iteration 114, loss = 0.55194834\n",
      "Iteration 115, loss = 0.55137412\n",
      "Iteration 116, loss = 0.55087113\n",
      "Iteration 117, loss = 0.55030048\n",
      "Iteration 118, loss = 0.54980816\n",
      "Iteration 119, loss = 0.54924898\n",
      "Iteration 120, loss = 0.54870764\n",
      "Iteration 121, loss = 0.54824084\n",
      "Iteration 122, loss = 0.54766794\n",
      "Iteration 123, loss = 0.54713922\n",
      "Iteration 124, loss = 0.54658586\n",
      "Iteration 125, loss = 0.54607990\n",
      "Iteration 126, loss = 0.54553380\n",
      "Iteration 127, loss = 0.54507443\n",
      "Iteration 128, loss = 0.54448723\n",
      "Iteration 129, loss = 0.54401453\n",
      "Iteration 130, loss = 0.54347904\n",
      "Iteration 131, loss = 0.54298556\n",
      "Iteration 132, loss = 0.54244978\n",
      "Iteration 133, loss = 0.54190575\n",
      "Iteration 134, loss = 0.54152885\n",
      "Iteration 135, loss = 0.54086737\n",
      "Iteration 136, loss = 0.54032318\n",
      "Iteration 137, loss = 0.53990125\n",
      "Iteration 138, loss = 0.53941011\n",
      "Iteration 139, loss = 0.53889240\n",
      "Iteration 140, loss = 0.53839717\n",
      "Iteration 141, loss = 0.53788864\n",
      "Iteration 142, loss = 0.53731907\n",
      "Iteration 143, loss = 0.53687532\n",
      "Iteration 144, loss = 0.53638392\n",
      "Iteration 145, loss = 0.53591640\n",
      "Iteration 146, loss = 0.53536948\n",
      "Iteration 147, loss = 0.53488829\n",
      "Iteration 148, loss = 0.53445467\n",
      "Iteration 149, loss = 0.53394424\n",
      "Iteration 150, loss = 0.53337276\n",
      "Iteration 151, loss = 0.53288713\n",
      "Iteration 152, loss = 0.53259326\n",
      "Iteration 153, loss = 0.53194386\n",
      "Iteration 154, loss = 0.53150609\n",
      "Iteration 155, loss = 0.53096759\n",
      "Iteration 156, loss = 0.53044573\n",
      "Iteration 157, loss = 0.53001731\n",
      "Iteration 158, loss = 0.52951049\n",
      "Iteration 159, loss = 0.52905180\n",
      "Iteration 160, loss = 0.52855071\n",
      "Iteration 161, loss = 0.52819888\n",
      "Iteration 162, loss = 0.52750485\n",
      "Iteration 163, loss = 0.52700246\n",
      "Iteration 164, loss = 0.52656524\n",
      "Iteration 165, loss = 0.52612949\n",
      "Iteration 166, loss = 0.52576615\n",
      "Iteration 167, loss = 0.52520296\n",
      "Iteration 168, loss = 0.52464268\n",
      "Iteration 169, loss = 0.52423115\n",
      "Iteration 170, loss = 0.52376118\n",
      "Iteration 171, loss = 0.52323415\n",
      "Iteration 172, loss = 0.52274298\n",
      "Iteration 173, loss = 0.52236673\n",
      "Iteration 174, loss = 0.52188814\n",
      "Iteration 175, loss = 0.52137199\n",
      "Iteration 176, loss = 0.52093960\n",
      "Iteration 177, loss = 0.52050556\n",
      "Iteration 178, loss = 0.51990792\n",
      "Iteration 179, loss = 0.51950129\n",
      "Iteration 180, loss = 0.51907662\n",
      "Iteration 181, loss = 0.51864181\n",
      "Iteration 182, loss = 0.51812928\n",
      "Iteration 183, loss = 0.51765905\n",
      "Iteration 184, loss = 0.51723817\n",
      "Iteration 185, loss = 0.51675134\n",
      "Iteration 186, loss = 0.51628396\n",
      "Iteration 187, loss = 0.51576472\n",
      "Iteration 188, loss = 0.51534671\n",
      "Iteration 189, loss = 0.51488856\n",
      "Iteration 190, loss = 0.51436340\n",
      "Iteration 191, loss = 0.51392688\n",
      "Iteration 192, loss = 0.51349154\n",
      "Iteration 193, loss = 0.51295030\n",
      "Iteration 194, loss = 0.51247976\n",
      "Iteration 195, loss = 0.51199440\n",
      "Iteration 196, loss = 0.51163033\n",
      "Iteration 197, loss = 0.51110623\n",
      "Iteration 198, loss = 0.51070946\n",
      "Iteration 199, loss = 0.51013994\n",
      "Iteration 200, loss = 0.50968970\n",
      "Iteration 201, loss = 0.50923477\n",
      "Iteration 202, loss = 0.50877223\n",
      "Iteration 203, loss = 0.50833705\n",
      "Iteration 204, loss = 0.50779227\n",
      "Iteration 205, loss = 0.50735118\n",
      "Iteration 206, loss = 0.50680760\n",
      "Iteration 207, loss = 0.50647804\n",
      "Iteration 208, loss = 0.50595252\n",
      "Iteration 209, loss = 0.50541651\n",
      "Iteration 210, loss = 0.50497937\n",
      "Iteration 211, loss = 0.50457136\n",
      "Iteration 212, loss = 0.50405955\n",
      "Iteration 213, loss = 0.50359545\n",
      "Iteration 214, loss = 0.50318904\n",
      "Iteration 215, loss = 0.50254655\n",
      "Iteration 216, loss = 0.50219461\n",
      "Iteration 217, loss = 0.50164232\n",
      "Iteration 218, loss = 0.50110769\n",
      "Iteration 219, loss = 0.50067451\n",
      "Iteration 220, loss = 0.50015533\n",
      "Iteration 221, loss = 0.49976758\n",
      "Iteration 222, loss = 0.49928344\n",
      "Iteration 223, loss = 0.49866096\n",
      "Iteration 224, loss = 0.49819109\n",
      "Iteration 225, loss = 0.49771351\n",
      "Iteration 226, loss = 0.49724651\n",
      "Iteration 227, loss = 0.49676903\n",
      "Iteration 228, loss = 0.49625994\n",
      "Iteration 229, loss = 0.49579572\n",
      "Iteration 230, loss = 0.49535928\n",
      "Iteration 231, loss = 0.49478463\n",
      "Iteration 232, loss = 0.49424877\n",
      "Iteration 233, loss = 0.49373465\n",
      "Iteration 234, loss = 0.49323066\n",
      "Iteration 235, loss = 0.49284587\n",
      "Iteration 236, loss = 0.49221210\n",
      "Iteration 237, loss = 0.49173245\n",
      "Iteration 238, loss = 0.49131984\n",
      "Iteration 239, loss = 0.49079270\n",
      "Iteration 240, loss = 0.49027250\n",
      "Iteration 241, loss = 0.48988668\n",
      "Iteration 242, loss = 0.48942165\n",
      "Iteration 243, loss = 0.48879477\n",
      "Iteration 244, loss = 0.48825525\n",
      "Iteration 245, loss = 0.48774723\n",
      "Iteration 246, loss = 0.48732503\n",
      "Iteration 247, loss = 0.48682308\n",
      "Iteration 248, loss = 0.48623246\n",
      "Iteration 249, loss = 0.48582576\n",
      "Iteration 250, loss = 0.48520741\n",
      "Iteration 251, loss = 0.48473871\n",
      "Iteration 252, loss = 0.48415427\n",
      "Iteration 253, loss = 0.48374373\n",
      "Iteration 254, loss = 0.48324993\n",
      "Iteration 255, loss = 0.48265128\n",
      "Iteration 256, loss = 0.48220231\n",
      "Iteration 257, loss = 0.48155972\n",
      "Iteration 258, loss = 0.48110494\n",
      "Iteration 259, loss = 0.48064742\n",
      "Iteration 260, loss = 0.48017367\n",
      "Iteration 261, loss = 0.47958993\n",
      "Iteration 262, loss = 0.47900008\n",
      "Iteration 263, loss = 0.47863642\n",
      "Iteration 264, loss = 0.47801828\n",
      "Iteration 265, loss = 0.47757996\n",
      "Iteration 266, loss = 0.47704361\n",
      "Iteration 267, loss = 0.47649388\n",
      "Iteration 268, loss = 0.47605092\n",
      "Iteration 269, loss = 0.47545823\n",
      "Iteration 270, loss = 0.47510696\n",
      "Iteration 271, loss = 0.47480581\n",
      "Iteration 272, loss = 0.47413355\n",
      "Iteration 273, loss = 0.47351893\n",
      "Iteration 274, loss = 0.47290522\n",
      "Iteration 275, loss = 0.47258904\n",
      "Iteration 276, loss = 0.47196717\n",
      "Iteration 277, loss = 0.47148967\n",
      "Iteration 278, loss = 0.47090575\n",
      "Iteration 279, loss = 0.47044270\n",
      "Iteration 280, loss = 0.46991054\n",
      "Iteration 281, loss = 0.46939397\n",
      "Iteration 282, loss = 0.46908429\n",
      "Iteration 283, loss = 0.46834244\n",
      "Iteration 284, loss = 0.46796181\n",
      "Iteration 285, loss = 0.46737567\n",
      "Iteration 286, loss = 0.46687629\n",
      "Iteration 287, loss = 0.46626233\n",
      "Iteration 288, loss = 0.46576864\n",
      "Iteration 289, loss = 0.46545149\n",
      "Iteration 290, loss = 0.46485518\n",
      "Iteration 291, loss = 0.46449894\n",
      "Iteration 292, loss = 0.46395727\n",
      "Iteration 293, loss = 0.46341522\n",
      "Iteration 294, loss = 0.46264140\n",
      "Iteration 295, loss = 0.46225717\n",
      "Iteration 296, loss = 0.46162684\n",
      "Iteration 297, loss = 0.46117239\n",
      "Iteration 298, loss = 0.46067964\n",
      "Iteration 299, loss = 0.46011561\n",
      "Iteration 300, loss = 0.45969116\n",
      "Iteration 301, loss = 0.45903013\n",
      "Iteration 302, loss = 0.45881360\n",
      "Iteration 303, loss = 0.45801631\n",
      "Iteration 304, loss = 0.45748972\n",
      "Iteration 305, loss = 0.45702156\n",
      "Iteration 306, loss = 0.45668228\n",
      "Iteration 307, loss = 0.45599158\n",
      "Iteration 308, loss = 0.45560254\n",
      "Iteration 309, loss = 0.45491945\n",
      "Iteration 310, loss = 0.45450045\n",
      "Iteration 311, loss = 0.45395082\n",
      "Iteration 312, loss = 0.45339108\n",
      "Iteration 313, loss = 0.45290356\n",
      "Iteration 314, loss = 0.45229554\n",
      "Iteration 315, loss = 0.45187708\n",
      "Iteration 316, loss = 0.45136912\n",
      "Iteration 317, loss = 0.45099197\n",
      "Iteration 318, loss = 0.45047374\n",
      "Iteration 319, loss = 0.44980692\n",
      "Iteration 320, loss = 0.44939718\n",
      "Iteration 321, loss = 0.44856415\n",
      "Iteration 322, loss = 0.44815826\n",
      "Iteration 323, loss = 0.44771275\n",
      "Iteration 324, loss = 0.44721562\n",
      "Iteration 325, loss = 0.44678145\n",
      "Iteration 326, loss = 0.44643790\n",
      "Iteration 327, loss = 0.44556164\n",
      "Iteration 328, loss = 0.44512976\n",
      "Iteration 329, loss = 0.44463406\n",
      "Iteration 330, loss = 0.44400837\n",
      "Iteration 331, loss = 0.44345492\n",
      "Iteration 332, loss = 0.44304250\n",
      "Iteration 333, loss = 0.44246764\n",
      "Iteration 334, loss = 0.44194694\n",
      "Iteration 335, loss = 0.44149228\n",
      "Iteration 336, loss = 0.44098526\n",
      "Iteration 337, loss = 0.44042680\n",
      "Iteration 338, loss = 0.43991747\n",
      "Iteration 339, loss = 0.43934069\n",
      "Iteration 340, loss = 0.43899470\n",
      "Iteration 341, loss = 0.43833426\n",
      "Iteration 342, loss = 0.43780913\n",
      "Iteration 343, loss = 0.43739485\n",
      "Iteration 344, loss = 0.43675943\n",
      "Iteration 345, loss = 0.43605269\n",
      "Iteration 346, loss = 0.43560863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 347, loss = 0.43501160\n",
      "Iteration 348, loss = 0.43470176\n",
      "Iteration 349, loss = 0.43417166\n",
      "Iteration 350, loss = 0.43370322\n",
      "Iteration 351, loss = 0.43325243\n",
      "Iteration 352, loss = 0.43244916\n",
      "Iteration 353, loss = 0.43220538\n",
      "Iteration 354, loss = 0.43170599\n",
      "Iteration 355, loss = 0.43111935\n",
      "Iteration 356, loss = 0.43058573\n",
      "Iteration 357, loss = 0.43013045\n",
      "Iteration 358, loss = 0.42982242\n",
      "Iteration 359, loss = 0.42907149\n",
      "Iteration 360, loss = 0.42844198\n",
      "Iteration 361, loss = 0.42792460\n",
      "Iteration 362, loss = 0.42741809\n",
      "Iteration 363, loss = 0.42718975\n",
      "Iteration 364, loss = 0.42659884\n",
      "Iteration 365, loss = 0.42583565\n",
      "Iteration 366, loss = 0.42540711\n",
      "Iteration 367, loss = 0.42501875\n",
      "Iteration 368, loss = 0.42444477\n",
      "Iteration 369, loss = 0.42398652\n",
      "Iteration 370, loss = 0.42343189\n",
      "Iteration 371, loss = 0.42280522\n",
      "Iteration 372, loss = 0.42240768\n",
      "Iteration 373, loss = 0.42200618\n",
      "Iteration 374, loss = 0.42149418\n",
      "Iteration 375, loss = 0.42085491\n",
      "Iteration 376, loss = 0.42027052\n",
      "Iteration 377, loss = 0.41995163\n",
      "Iteration 378, loss = 0.41917095\n",
      "Iteration 379, loss = 0.41892398\n",
      "Iteration 380, loss = 0.41826313\n",
      "Iteration 381, loss = 0.41780487\n",
      "Iteration 382, loss = 0.41738209\n",
      "Iteration 383, loss = 0.41701521\n",
      "Iteration 384, loss = 0.41639546\n",
      "Iteration 385, loss = 0.41590361\n",
      "Iteration 386, loss = 0.41545560\n",
      "Iteration 387, loss = 0.41475553\n",
      "Iteration 388, loss = 0.41427136\n",
      "Iteration 389, loss = 0.41411647\n",
      "Iteration 390, loss = 0.41340732\n",
      "Iteration 391, loss = 0.41283589\n",
      "Iteration 392, loss = 0.41247333\n",
      "Iteration 393, loss = 0.41171603\n",
      "Iteration 394, loss = 0.41133631\n",
      "Iteration 395, loss = 0.41073582\n",
      "Iteration 396, loss = 0.41094189\n",
      "Iteration 397, loss = 0.41010610\n",
      "Iteration 398, loss = 0.40946826\n",
      "Iteration 399, loss = 0.40906916\n",
      "Iteration 400, loss = 0.40848365\n",
      "Iteration 401, loss = 0.40820638\n",
      "Iteration 402, loss = 0.40749694\n",
      "Iteration 403, loss = 0.40701141\n",
      "Iteration 404, loss = 0.40649970\n",
      "Iteration 405, loss = 0.40623210\n",
      "Iteration 406, loss = 0.40558172\n",
      "Iteration 407, loss = 0.40522567\n",
      "Iteration 408, loss = 0.40480874\n",
      "Iteration 409, loss = 0.40399499\n",
      "Iteration 410, loss = 0.40356654\n",
      "Iteration 411, loss = 0.40321106\n",
      "Iteration 412, loss = 0.40275133\n",
      "Iteration 413, loss = 0.40230953\n",
      "Iteration 414, loss = 0.40181468\n",
      "Iteration 415, loss = 0.40103347\n",
      "Iteration 416, loss = 0.40092431\n",
      "Iteration 417, loss = 0.40042995\n",
      "Iteration 418, loss = 0.39983916\n",
      "Iteration 419, loss = 0.39928921\n",
      "Iteration 420, loss = 0.39899459\n",
      "Iteration 421, loss = 0.39842284\n",
      "Iteration 422, loss = 0.39798621\n",
      "Iteration 423, loss = 0.39745148\n",
      "Iteration 424, loss = 0.39709501\n",
      "Iteration 425, loss = 0.39645896\n",
      "Iteration 426, loss = 0.39615502\n",
      "Iteration 427, loss = 0.39546267\n",
      "Iteration 428, loss = 0.39542320\n",
      "Iteration 429, loss = 0.39498890\n",
      "Iteration 430, loss = 0.39428282\n",
      "Iteration 431, loss = 0.39371430\n",
      "Iteration 432, loss = 0.39347473\n",
      "Iteration 433, loss = 0.39278060\n",
      "Iteration 434, loss = 0.39218284\n",
      "Iteration 435, loss = 0.39192985\n",
      "Iteration 436, loss = 0.39148456\n",
      "Iteration 437, loss = 0.39119698\n",
      "Iteration 438, loss = 0.39052210\n",
      "Iteration 439, loss = 0.39005370\n",
      "Iteration 440, loss = 0.38951550\n",
      "Iteration 441, loss = 0.38932133\n",
      "Iteration 442, loss = 0.38867915\n",
      "Iteration 443, loss = 0.38868620\n",
      "Iteration 444, loss = 0.38787693\n",
      "Iteration 445, loss = 0.38747189\n",
      "Iteration 446, loss = 0.38700167\n",
      "Iteration 447, loss = 0.38668635\n",
      "Iteration 448, loss = 0.38614148\n",
      "Iteration 449, loss = 0.38568904\n",
      "Iteration 450, loss = 0.38534808\n",
      "Iteration 451, loss = 0.38482034\n",
      "Iteration 452, loss = 0.38417224\n",
      "Iteration 453, loss = 0.38385463\n",
      "Iteration 454, loss = 0.38339669\n",
      "Iteration 455, loss = 0.38329908\n",
      "Iteration 456, loss = 0.38254462\n",
      "Iteration 457, loss = 0.38219139\n",
      "Iteration 458, loss = 0.38202882\n",
      "Iteration 459, loss = 0.38132176\n",
      "Iteration 460, loss = 0.38083082\n",
      "Iteration 461, loss = 0.38058214\n",
      "Iteration 462, loss = 0.37990140\n",
      "Iteration 463, loss = 0.37954967\n",
      "Iteration 464, loss = 0.37934605\n",
      "Iteration 465, loss = 0.37862498\n",
      "Iteration 466, loss = 0.37829910\n",
      "Iteration 467, loss = 0.37783693\n",
      "Iteration 468, loss = 0.37757369\n",
      "Iteration 469, loss = 0.37702224\n",
      "Iteration 470, loss = 0.37712511\n",
      "Iteration 471, loss = 0.37621717\n",
      "Iteration 472, loss = 0.37596595\n",
      "Iteration 473, loss = 0.37534517\n",
      "Iteration 474, loss = 0.37506704\n",
      "Iteration 475, loss = 0.37445983\n",
      "Iteration 476, loss = 0.37477367\n",
      "Iteration 477, loss = 0.37384023\n",
      "Iteration 478, loss = 0.37334962\n",
      "Iteration 479, loss = 0.37292852\n",
      "Iteration 480, loss = 0.37283225\n",
      "Iteration 481, loss = 0.37208372\n",
      "Iteration 482, loss = 0.37188108\n",
      "Iteration 483, loss = 0.37127200\n",
      "Iteration 484, loss = 0.37099292\n",
      "Iteration 485, loss = 0.37054129\n",
      "Iteration 486, loss = 0.36993504\n",
      "Iteration 487, loss = 0.36964879\n",
      "Iteration 488, loss = 0.36939488\n",
      "Iteration 489, loss = 0.36931250\n",
      "Iteration 490, loss = 0.36864417\n",
      "Iteration 491, loss = 0.36793517\n",
      "Iteration 492, loss = 0.36773485\n",
      "Iteration 493, loss = 0.36723275\n",
      "Iteration 494, loss = 0.36679346\n",
      "Iteration 495, loss = 0.36636789\n",
      "Iteration 496, loss = 0.36608378\n",
      "Iteration 497, loss = 0.36561473\n",
      "Iteration 498, loss = 0.36511910\n",
      "Iteration 499, loss = 0.36485049\n",
      "Iteration 500, loss = 0.36475595\n",
      "Iteration 501, loss = 0.36409263\n",
      "Iteration 502, loss = 0.36414661\n",
      "Iteration 503, loss = 0.36345945\n",
      "Iteration 504, loss = 0.36304941\n",
      "Iteration 505, loss = 0.36264918\n",
      "Iteration 506, loss = 0.36221025\n",
      "Iteration 507, loss = 0.36180817\n",
      "Iteration 508, loss = 0.36197340\n",
      "Iteration 509, loss = 0.36127624\n",
      "Iteration 510, loss = 0.36082481\n",
      "Iteration 511, loss = 0.36025944\n",
      "Iteration 512, loss = 0.36010185\n",
      "Iteration 513, loss = 0.35934302\n",
      "Iteration 514, loss = 0.35924772\n",
      "Iteration 515, loss = 0.35879727\n",
      "Iteration 516, loss = 0.35854002\n",
      "Iteration 517, loss = 0.35797205\n",
      "Iteration 518, loss = 0.35780451\n",
      "Iteration 519, loss = 0.35734045\n",
      "Iteration 520, loss = 0.35694177\n",
      "Iteration 521, loss = 0.35662124\n",
      "Iteration 522, loss = 0.35629675\n",
      "Iteration 523, loss = 0.35607779\n",
      "Iteration 524, loss = 0.35594283\n",
      "Iteration 525, loss = 0.35494933\n",
      "Iteration 526, loss = 0.35465944\n",
      "Iteration 527, loss = 0.35481191\n",
      "Iteration 528, loss = 0.35440497\n",
      "Iteration 529, loss = 0.35344676\n",
      "Iteration 530, loss = 0.35367225\n",
      "Iteration 531, loss = 0.35313039\n",
      "Iteration 532, loss = 0.35274367\n",
      "Iteration 533, loss = 0.35213604\n",
      "Iteration 534, loss = 0.35174449\n",
      "Iteration 535, loss = 0.35158146\n",
      "Iteration 536, loss = 0.35127198\n",
      "Iteration 537, loss = 0.35079369\n",
      "Iteration 538, loss = 0.35052462\n",
      "Iteration 539, loss = 0.34997524\n",
      "Iteration 540, loss = 0.34988617\n",
      "Iteration 541, loss = 0.34939486\n",
      "Iteration 542, loss = 0.34919033\n",
      "Iteration 543, loss = 0.34889973\n",
      "Iteration 544, loss = 0.34821173\n",
      "Iteration 545, loss = 0.34805801\n",
      "Iteration 546, loss = 0.34791767\n",
      "Iteration 547, loss = 0.34753325\n",
      "Iteration 548, loss = 0.34756820\n",
      "Iteration 549, loss = 0.34714727\n",
      "Iteration 550, loss = 0.34661004\n",
      "Iteration 551, loss = 0.34577291\n",
      "Iteration 552, loss = 0.34566415\n",
      "Iteration 553, loss = 0.34514414\n",
      "Iteration 554, loss = 0.34517352\n",
      "Iteration 555, loss = 0.34476999\n",
      "Iteration 556, loss = 0.34459793\n",
      "Iteration 557, loss = 0.34384481\n",
      "Iteration 558, loss = 0.34350712\n",
      "Iteration 559, loss = 0.34335585\n",
      "Iteration 560, loss = 0.34281631\n",
      "Iteration 561, loss = 0.34232367\n",
      "Iteration 562, loss = 0.34208471\n",
      "Iteration 563, loss = 0.34201291\n",
      "Iteration 564, loss = 0.34167948\n",
      "Iteration 565, loss = 0.34121060\n",
      "Iteration 566, loss = 0.34096386\n",
      "Iteration 567, loss = 0.34072815\n",
      "Iteration 568, loss = 0.34024428\n",
      "Iteration 569, loss = 0.34012287\n",
      "Iteration 570, loss = 0.33919245\n",
      "Iteration 571, loss = 0.33970553\n",
      "Iteration 572, loss = 0.33939726\n",
      "Iteration 573, loss = 0.33884429\n",
      "Iteration 574, loss = 0.33813411\n",
      "Iteration 575, loss = 0.33798874\n",
      "Iteration 576, loss = 0.33747416\n",
      "Iteration 577, loss = 0.33763108\n",
      "Iteration 578, loss = 0.33686461\n",
      "Iteration 579, loss = 0.33702437\n",
      "Iteration 580, loss = 0.33677080\n",
      "Iteration 581, loss = 0.33587521\n",
      "Iteration 582, loss = 0.33585825\n",
      "Iteration 583, loss = 0.33555841\n",
      "Iteration 584, loss = 0.33506054\n",
      "Iteration 585, loss = 0.33470109\n",
      "Iteration 586, loss = 0.33455509\n",
      "Iteration 587, loss = 0.33424657\n",
      "Iteration 588, loss = 0.33415485\n",
      "Iteration 589, loss = 0.33348891\n",
      "Iteration 590, loss = 0.33328068\n",
      "Iteration 591, loss = 0.33324139\n",
      "Iteration 592, loss = 0.33248639\n",
      "Iteration 593, loss = 0.33271582\n",
      "Iteration 594, loss = 0.33232660\n",
      "Iteration 595, loss = 0.33174273\n",
      "Iteration 596, loss = 0.33129212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 597, loss = 0.33090510\n",
      "Iteration 598, loss = 0.33073386\n",
      "Iteration 599, loss = 0.33082764\n",
      "Iteration 600, loss = 0.33036265\n",
      "Iteration 601, loss = 0.32995751\n",
      "Iteration 602, loss = 0.32960365\n",
      "Iteration 603, loss = 0.32997185\n",
      "Iteration 604, loss = 0.32876849\n",
      "Iteration 605, loss = 0.32839330\n",
      "Iteration 606, loss = 0.32841692\n",
      "Iteration 607, loss = 0.32783023\n",
      "Iteration 608, loss = 0.32768931\n",
      "Iteration 609, loss = 0.32740311\n",
      "Iteration 610, loss = 0.32769750\n",
      "Iteration 611, loss = 0.32672904\n",
      "Iteration 612, loss = 0.32637320\n",
      "Iteration 613, loss = 0.32622709\n",
      "Iteration 614, loss = 0.32576080\n",
      "Iteration 615, loss = 0.32586540\n",
      "Iteration 616, loss = 0.32531179\n",
      "Iteration 617, loss = 0.32499022\n",
      "Iteration 618, loss = 0.32478370\n",
      "Iteration 619, loss = 0.32439932\n",
      "Iteration 620, loss = 0.32465714\n",
      "Iteration 621, loss = 0.32404115\n",
      "Iteration 622, loss = 0.32343541\n",
      "Iteration 623, loss = 0.32342020\n",
      "Iteration 624, loss = 0.32328584\n",
      "Iteration 625, loss = 0.32264232\n",
      "Iteration 626, loss = 0.32257422\n",
      "Iteration 627, loss = 0.32232373\n",
      "Iteration 628, loss = 0.32222694\n",
      "Iteration 629, loss = 0.32149946\n",
      "Iteration 630, loss = 0.32143523\n",
      "Iteration 631, loss = 0.32118481\n",
      "Iteration 632, loss = 0.32052653\n",
      "Iteration 633, loss = 0.32109482\n",
      "Iteration 634, loss = 0.32009152\n",
      "Iteration 635, loss = 0.31979580\n",
      "Iteration 636, loss = 0.32002709\n",
      "Iteration 637, loss = 0.31931168\n",
      "Iteration 638, loss = 0.31909953\n",
      "Iteration 639, loss = 0.31906926\n",
      "Iteration 640, loss = 0.31914402\n",
      "Iteration 641, loss = 0.31818714\n",
      "Iteration 642, loss = 0.31810567\n",
      "Iteration 643, loss = 0.31802914\n",
      "Iteration 644, loss = 0.31760899\n",
      "Iteration 645, loss = 0.31760281\n",
      "Iteration 646, loss = 0.31704163\n",
      "Iteration 647, loss = 0.31671726\n",
      "Iteration 648, loss = 0.31683103\n",
      "Iteration 649, loss = 0.31613972\n",
      "Iteration 650, loss = 0.31575972\n",
      "Iteration 651, loss = 0.31578129\n",
      "Iteration 652, loss = 0.31544474\n",
      "Iteration 653, loss = 0.31500046\n",
      "Iteration 654, loss = 0.31487533\n",
      "Iteration 655, loss = 0.31497191\n",
      "Iteration 656, loss = 0.31438400\n",
      "Iteration 657, loss = 0.31392843\n",
      "Iteration 658, loss = 0.31350770\n",
      "Iteration 659, loss = 0.31383979\n",
      "Iteration 660, loss = 0.31359767\n",
      "Iteration 661, loss = 0.31363326\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69682633\n",
      "Iteration 2, loss = 0.68910794\n",
      "Iteration 3, loss = 0.68203276\n",
      "Iteration 4, loss = 0.67566938\n",
      "Iteration 5, loss = 0.66998887\n",
      "Iteration 6, loss = 0.66480742\n",
      "Iteration 7, loss = 0.66000216\n",
      "Iteration 8, loss = 0.65556656\n",
      "Iteration 9, loss = 0.65146768\n",
      "Iteration 10, loss = 0.64771443\n",
      "Iteration 11, loss = 0.64424238\n",
      "Iteration 12, loss = 0.64101861\n",
      "Iteration 13, loss = 0.63808613\n",
      "Iteration 14, loss = 0.63537459\n",
      "Iteration 15, loss = 0.63291098\n",
      "Iteration 16, loss = 0.63066337\n",
      "Iteration 17, loss = 0.62861842\n",
      "Iteration 18, loss = 0.62673116\n",
      "Iteration 19, loss = 0.62494294\n",
      "Iteration 20, loss = 0.62334387\n",
      "Iteration 21, loss = 0.62183814\n",
      "Iteration 22, loss = 0.62042437\n",
      "Iteration 23, loss = 0.61911555\n",
      "Iteration 24, loss = 0.61788752\n",
      "Iteration 25, loss = 0.61672750\n",
      "Iteration 26, loss = 0.61559541\n",
      "Iteration 27, loss = 0.61454384\n",
      "Iteration 28, loss = 0.61344508\n",
      "Iteration 29, loss = 0.61245314\n",
      "Iteration 30, loss = 0.61147509\n",
      "Iteration 31, loss = 0.61052407\n",
      "Iteration 32, loss = 0.60956601\n",
      "Iteration 33, loss = 0.60865043\n",
      "Iteration 34, loss = 0.60773973\n",
      "Iteration 35, loss = 0.60684238\n",
      "Iteration 36, loss = 0.60594560\n",
      "Iteration 37, loss = 0.60512694\n",
      "Iteration 38, loss = 0.60421562\n",
      "Iteration 39, loss = 0.60339002\n",
      "Iteration 40, loss = 0.60251636\n",
      "Iteration 41, loss = 0.60166941\n",
      "Iteration 42, loss = 0.60087475\n",
      "Iteration 43, loss = 0.60001969\n",
      "Iteration 44, loss = 0.59916817\n",
      "Iteration 45, loss = 0.59835586\n",
      "Iteration 46, loss = 0.59752213\n",
      "Iteration 47, loss = 0.59671247\n",
      "Iteration 48, loss = 0.59590660\n",
      "Iteration 49, loss = 0.59509361\n",
      "Iteration 50, loss = 0.59429813\n",
      "Iteration 51, loss = 0.59348410\n",
      "Iteration 52, loss = 0.59277117\n",
      "Iteration 53, loss = 0.59190756\n",
      "Iteration 54, loss = 0.59111849\n",
      "Iteration 55, loss = 0.59036331\n",
      "Iteration 56, loss = 0.58952553\n",
      "Iteration 57, loss = 0.58877010\n",
      "Iteration 58, loss = 0.58798987\n",
      "Iteration 59, loss = 0.58720397\n",
      "Iteration 60, loss = 0.58640162\n",
      "Iteration 61, loss = 0.58568659\n",
      "Iteration 62, loss = 0.58491987\n",
      "Iteration 63, loss = 0.58418625\n",
      "Iteration 64, loss = 0.58342039\n",
      "Iteration 65, loss = 0.58266143\n",
      "Iteration 66, loss = 0.58190865\n",
      "Iteration 67, loss = 0.58124932\n",
      "Iteration 68, loss = 0.58048354\n",
      "Iteration 69, loss = 0.57972002\n",
      "Iteration 70, loss = 0.57900054\n",
      "Iteration 71, loss = 0.57830358\n",
      "Iteration 72, loss = 0.57761111\n",
      "Iteration 73, loss = 0.57688270\n",
      "Iteration 74, loss = 0.57611820\n",
      "Iteration 75, loss = 0.57539488\n",
      "Iteration 76, loss = 0.57473641\n",
      "Iteration 77, loss = 0.57404924\n",
      "Iteration 78, loss = 0.57340879\n",
      "Iteration 79, loss = 0.57264996\n",
      "Iteration 80, loss = 0.57201268\n",
      "Iteration 81, loss = 0.57130166\n",
      "Iteration 82, loss = 0.57060425\n",
      "Iteration 83, loss = 0.56994059\n",
      "Iteration 84, loss = 0.56929461\n",
      "Iteration 85, loss = 0.56867844\n",
      "Iteration 86, loss = 0.56799911\n",
      "Iteration 87, loss = 0.56736284\n",
      "Iteration 88, loss = 0.56670230\n",
      "Iteration 89, loss = 0.56604059\n",
      "Iteration 90, loss = 0.56540599\n",
      "Iteration 91, loss = 0.56483607\n",
      "Iteration 92, loss = 0.56422481\n",
      "Iteration 93, loss = 0.56356758\n",
      "Iteration 94, loss = 0.56293463\n",
      "Iteration 95, loss = 0.56230733\n",
      "Iteration 96, loss = 0.56163541\n",
      "Iteration 97, loss = 0.56104271\n",
      "Iteration 98, loss = 0.56047854\n",
      "Iteration 99, loss = 0.55986455\n",
      "Iteration 100, loss = 0.55926582\n",
      "Iteration 101, loss = 0.55865972\n",
      "Iteration 102, loss = 0.55810420\n",
      "Iteration 103, loss = 0.55747852\n",
      "Iteration 104, loss = 0.55700972\n",
      "Iteration 105, loss = 0.55644173\n",
      "Iteration 106, loss = 0.55582094\n",
      "Iteration 107, loss = 0.55528013\n",
      "Iteration 108, loss = 0.55465969\n",
      "Iteration 109, loss = 0.55413946\n",
      "Iteration 110, loss = 0.55356358\n",
      "Iteration 111, loss = 0.55301529\n",
      "Iteration 112, loss = 0.55249292\n",
      "Iteration 113, loss = 0.55192441\n",
      "Iteration 114, loss = 0.55138019\n",
      "Iteration 115, loss = 0.55083014\n",
      "Iteration 116, loss = 0.55027986\n",
      "Iteration 117, loss = 0.54973379\n",
      "Iteration 118, loss = 0.54932884\n",
      "Iteration 119, loss = 0.54867044\n",
      "Iteration 120, loss = 0.54813790\n",
      "Iteration 121, loss = 0.54761994\n",
      "Iteration 122, loss = 0.54710821\n",
      "Iteration 123, loss = 0.54658548\n",
      "Iteration 124, loss = 0.54608735\n",
      "Iteration 125, loss = 0.54556560\n",
      "Iteration 126, loss = 0.54497072\n",
      "Iteration 127, loss = 0.54451067\n",
      "Iteration 128, loss = 0.54400489\n",
      "Iteration 129, loss = 0.54347857\n",
      "Iteration 130, loss = 0.54296072\n",
      "Iteration 131, loss = 0.54252763\n",
      "Iteration 132, loss = 0.54194741\n",
      "Iteration 133, loss = 0.54137201\n",
      "Iteration 134, loss = 0.54100232\n",
      "Iteration 135, loss = 0.54037071\n",
      "Iteration 136, loss = 0.53985616\n",
      "Iteration 137, loss = 0.53937177\n",
      "Iteration 138, loss = 0.53884212\n",
      "Iteration 139, loss = 0.53836291\n",
      "Iteration 140, loss = 0.53784875\n",
      "Iteration 141, loss = 0.53734408\n",
      "Iteration 142, loss = 0.53685153\n",
      "Iteration 143, loss = 0.53638047\n",
      "Iteration 144, loss = 0.53585243\n",
      "Iteration 145, loss = 0.53537643\n",
      "Iteration 146, loss = 0.53493573\n",
      "Iteration 147, loss = 0.53440885\n",
      "Iteration 148, loss = 0.53401736\n",
      "Iteration 149, loss = 0.53345318\n",
      "Iteration 150, loss = 0.53293697\n",
      "Iteration 151, loss = 0.53246868\n",
      "Iteration 152, loss = 0.53213589\n",
      "Iteration 153, loss = 0.53146386\n",
      "Iteration 154, loss = 0.53108902\n",
      "Iteration 155, loss = 0.53054463\n",
      "Iteration 156, loss = 0.53008039\n",
      "Iteration 157, loss = 0.52958315\n",
      "Iteration 158, loss = 0.52915950\n",
      "Iteration 159, loss = 0.52871664\n",
      "Iteration 160, loss = 0.52815513\n",
      "Iteration 161, loss = 0.52782706\n",
      "Iteration 162, loss = 0.52713512\n",
      "Iteration 163, loss = 0.52669019\n",
      "Iteration 164, loss = 0.52631461\n",
      "Iteration 165, loss = 0.52578553\n",
      "Iteration 166, loss = 0.52544656\n",
      "Iteration 167, loss = 0.52488313\n",
      "Iteration 168, loss = 0.52436835\n",
      "Iteration 169, loss = 0.52391773\n",
      "Iteration 170, loss = 0.52342766\n",
      "Iteration 171, loss = 0.52291918\n",
      "Iteration 172, loss = 0.52256305\n",
      "Iteration 173, loss = 0.52202777\n",
      "Iteration 174, loss = 0.52161882\n",
      "Iteration 175, loss = 0.52112382\n",
      "Iteration 176, loss = 0.52067050\n",
      "Iteration 177, loss = 0.52026987\n",
      "Iteration 178, loss = 0.51967014\n",
      "Iteration 179, loss = 0.51926421\n",
      "Iteration 180, loss = 0.51888132\n",
      "Iteration 181, loss = 0.51831519\n",
      "Iteration 182, loss = 0.51793075\n",
      "Iteration 183, loss = 0.51749070\n",
      "Iteration 184, loss = 0.51700908\n",
      "Iteration 185, loss = 0.51656661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 186, loss = 0.51605871\n",
      "Iteration 187, loss = 0.51553253\n",
      "Iteration 188, loss = 0.51512435\n",
      "Iteration 189, loss = 0.51474155\n",
      "Iteration 190, loss = 0.51419808\n",
      "Iteration 191, loss = 0.51376950\n",
      "Iteration 192, loss = 0.51335710\n",
      "Iteration 193, loss = 0.51281807\n",
      "Iteration 194, loss = 0.51248628\n",
      "Iteration 195, loss = 0.51185897\n",
      "Iteration 196, loss = 0.51147850\n",
      "Iteration 197, loss = 0.51095464\n",
      "Iteration 198, loss = 0.51065396\n",
      "Iteration 199, loss = 0.51002400\n",
      "Iteration 200, loss = 0.50957867\n",
      "Iteration 201, loss = 0.50908528\n",
      "Iteration 202, loss = 0.50863841\n",
      "Iteration 203, loss = 0.50828203\n",
      "Iteration 204, loss = 0.50771327\n",
      "Iteration 205, loss = 0.50729251\n",
      "Iteration 206, loss = 0.50683356\n",
      "Iteration 207, loss = 0.50647526\n",
      "Iteration 208, loss = 0.50588807\n",
      "Iteration 209, loss = 0.50542783\n",
      "Iteration 210, loss = 0.50505958\n",
      "Iteration 211, loss = 0.50462782\n",
      "Iteration 212, loss = 0.50416456\n",
      "Iteration 213, loss = 0.50364727\n",
      "Iteration 214, loss = 0.50330420\n",
      "Iteration 215, loss = 0.50277022\n",
      "Iteration 216, loss = 0.50233660\n",
      "Iteration 217, loss = 0.50180856\n",
      "Iteration 218, loss = 0.50126766\n",
      "Iteration 219, loss = 0.50083218\n",
      "Iteration 220, loss = 0.50034982\n",
      "Iteration 221, loss = 0.50006970\n",
      "Iteration 222, loss = 0.49948618\n",
      "Iteration 223, loss = 0.49893856\n",
      "Iteration 224, loss = 0.49850847\n",
      "Iteration 225, loss = 0.49806570\n",
      "Iteration 226, loss = 0.49761907\n",
      "Iteration 227, loss = 0.49710178\n",
      "Iteration 228, loss = 0.49668831\n",
      "Iteration 229, loss = 0.49608006\n",
      "Iteration 230, loss = 0.49562141\n",
      "Iteration 231, loss = 0.49515387\n",
      "Iteration 232, loss = 0.49462840\n",
      "Iteration 233, loss = 0.49419406\n",
      "Iteration 234, loss = 0.49371069\n",
      "Iteration 235, loss = 0.49329832\n",
      "Iteration 236, loss = 0.49285234\n",
      "Iteration 237, loss = 0.49233275\n",
      "Iteration 238, loss = 0.49188726\n",
      "Iteration 239, loss = 0.49138205\n",
      "Iteration 240, loss = 0.49083904\n",
      "Iteration 241, loss = 0.49051204\n",
      "Iteration 242, loss = 0.49004591\n",
      "Iteration 243, loss = 0.48947493\n",
      "Iteration 244, loss = 0.48895880\n",
      "Iteration 245, loss = 0.48847247\n",
      "Iteration 246, loss = 0.48802149\n",
      "Iteration 247, loss = 0.48761451\n",
      "Iteration 248, loss = 0.48708731\n",
      "Iteration 249, loss = 0.48658654\n",
      "Iteration 250, loss = 0.48611638\n",
      "Iteration 251, loss = 0.48564569\n",
      "Iteration 252, loss = 0.48502236\n",
      "Iteration 253, loss = 0.48468900\n",
      "Iteration 254, loss = 0.48418802\n",
      "Iteration 255, loss = 0.48372961\n",
      "Iteration 256, loss = 0.48332496\n",
      "Iteration 257, loss = 0.48274335\n",
      "Iteration 258, loss = 0.48221148\n",
      "Iteration 259, loss = 0.48174359\n",
      "Iteration 260, loss = 0.48150508\n",
      "Iteration 261, loss = 0.48075305\n",
      "Iteration 262, loss = 0.48025877\n",
      "Iteration 263, loss = 0.47986074\n",
      "Iteration 264, loss = 0.47934244\n",
      "Iteration 265, loss = 0.47884573\n",
      "Iteration 266, loss = 0.47825300\n",
      "Iteration 267, loss = 0.47784529\n",
      "Iteration 268, loss = 0.47747202\n",
      "Iteration 269, loss = 0.47674968\n",
      "Iteration 270, loss = 0.47651171\n",
      "Iteration 271, loss = 0.47623703\n",
      "Iteration 272, loss = 0.47550711\n",
      "Iteration 273, loss = 0.47504664\n",
      "Iteration 274, loss = 0.47437160\n",
      "Iteration 275, loss = 0.47407921\n",
      "Iteration 276, loss = 0.47341498\n",
      "Iteration 277, loss = 0.47311417\n",
      "Iteration 278, loss = 0.47256348\n",
      "Iteration 279, loss = 0.47223594\n",
      "Iteration 280, loss = 0.47155326\n",
      "Iteration 281, loss = 0.47111203\n",
      "Iteration 282, loss = 0.47074364\n",
      "Iteration 283, loss = 0.47014731\n",
      "Iteration 284, loss = 0.46969795\n",
      "Iteration 285, loss = 0.46921014\n",
      "Iteration 286, loss = 0.46869653\n",
      "Iteration 287, loss = 0.46819760\n",
      "Iteration 288, loss = 0.46760803\n",
      "Iteration 289, loss = 0.46720508\n",
      "Iteration 290, loss = 0.46680687\n",
      "Iteration 291, loss = 0.46636849\n",
      "Iteration 292, loss = 0.46586616\n",
      "Iteration 293, loss = 0.46549469\n",
      "Iteration 294, loss = 0.46473125\n",
      "Iteration 295, loss = 0.46427890\n",
      "Iteration 296, loss = 0.46374445\n",
      "Iteration 297, loss = 0.46337964\n",
      "Iteration 298, loss = 0.46272766\n",
      "Iteration 299, loss = 0.46238959\n",
      "Iteration 300, loss = 0.46178677\n",
      "Iteration 301, loss = 0.46142099\n",
      "Iteration 302, loss = 0.46107753\n",
      "Iteration 303, loss = 0.46045530\n",
      "Iteration 304, loss = 0.45999482\n",
      "Iteration 305, loss = 0.45936140\n",
      "Iteration 306, loss = 0.45917889\n",
      "Iteration 307, loss = 0.45849903\n",
      "Iteration 308, loss = 0.45812054\n",
      "Iteration 309, loss = 0.45743414\n",
      "Iteration 310, loss = 0.45706638\n",
      "Iteration 311, loss = 0.45647636\n",
      "Iteration 312, loss = 0.45598644\n",
      "Iteration 313, loss = 0.45575934\n",
      "Iteration 314, loss = 0.45509268\n",
      "Iteration 315, loss = 0.45469666\n",
      "Iteration 316, loss = 0.45411414\n",
      "Iteration 317, loss = 0.45372907\n",
      "Iteration 318, loss = 0.45328229\n",
      "Iteration 319, loss = 0.45271606\n",
      "Iteration 320, loss = 0.45210773\n",
      "Iteration 321, loss = 0.45161400\n",
      "Iteration 322, loss = 0.45130060\n",
      "Iteration 323, loss = 0.45076104\n",
      "Iteration 324, loss = 0.45032154\n",
      "Iteration 325, loss = 0.45023144\n",
      "Iteration 326, loss = 0.44946894\n",
      "Iteration 327, loss = 0.44888136\n",
      "Iteration 328, loss = 0.44855577\n",
      "Iteration 329, loss = 0.44806137\n",
      "Iteration 330, loss = 0.44742407\n",
      "Iteration 331, loss = 0.44701864\n",
      "Iteration 332, loss = 0.44655730\n",
      "Iteration 333, loss = 0.44610112\n",
      "Iteration 334, loss = 0.44548203\n",
      "Iteration 335, loss = 0.44526457\n",
      "Iteration 336, loss = 0.44455990\n",
      "Iteration 337, loss = 0.44409119\n",
      "Iteration 338, loss = 0.44364084\n",
      "Iteration 339, loss = 0.44334840\n",
      "Iteration 340, loss = 0.44266659\n",
      "Iteration 341, loss = 0.44219952\n",
      "Iteration 342, loss = 0.44178561\n",
      "Iteration 343, loss = 0.44124331\n",
      "Iteration 344, loss = 0.44097371\n",
      "Iteration 345, loss = 0.44029040\n",
      "Iteration 346, loss = 0.43976019\n",
      "Iteration 347, loss = 0.43918330\n",
      "Iteration 348, loss = 0.43896586\n",
      "Iteration 349, loss = 0.43836067\n",
      "Iteration 350, loss = 0.43813353\n",
      "Iteration 351, loss = 0.43770302\n",
      "Iteration 352, loss = 0.43703462\n",
      "Iteration 353, loss = 0.43654321\n",
      "Iteration 354, loss = 0.43599651\n",
      "Iteration 355, loss = 0.43560034\n",
      "Iteration 356, loss = 0.43521125\n",
      "Iteration 357, loss = 0.43458349\n",
      "Iteration 358, loss = 0.43411687\n",
      "Iteration 359, loss = 0.43360067\n",
      "Iteration 360, loss = 0.43296013\n",
      "Iteration 361, loss = 0.43254181\n",
      "Iteration 362, loss = 0.43201397\n",
      "Iteration 363, loss = 0.43157962\n",
      "Iteration 364, loss = 0.43112017\n",
      "Iteration 365, loss = 0.43043040\n",
      "Iteration 366, loss = 0.43016916\n",
      "Iteration 367, loss = 0.42960377\n",
      "Iteration 368, loss = 0.42900621\n",
      "Iteration 369, loss = 0.42851570\n",
      "Iteration 370, loss = 0.42804850\n",
      "Iteration 371, loss = 0.42763889\n",
      "Iteration 372, loss = 0.42709614\n",
      "Iteration 373, loss = 0.42673429\n",
      "Iteration 374, loss = 0.42614523\n",
      "Iteration 375, loss = 0.42539947\n",
      "Iteration 376, loss = 0.42503777\n",
      "Iteration 377, loss = 0.42476236\n",
      "Iteration 378, loss = 0.42400012\n",
      "Iteration 379, loss = 0.42387991\n",
      "Iteration 380, loss = 0.42320338\n",
      "Iteration 381, loss = 0.42255123\n",
      "Iteration 382, loss = 0.42243590\n",
      "Iteration 383, loss = 0.42191752\n",
      "Iteration 384, loss = 0.42125553\n",
      "Iteration 385, loss = 0.42071150\n",
      "Iteration 386, loss = 0.42020581\n",
      "Iteration 387, loss = 0.41964384\n",
      "Iteration 388, loss = 0.41949668\n",
      "Iteration 389, loss = 0.41878836\n",
      "Iteration 390, loss = 0.41833048\n",
      "Iteration 391, loss = 0.41788820\n",
      "Iteration 392, loss = 0.41765040\n",
      "Iteration 393, loss = 0.41675323\n",
      "Iteration 394, loss = 0.41639434\n",
      "Iteration 395, loss = 0.41587035\n",
      "Iteration 396, loss = 0.41569981\n",
      "Iteration 397, loss = 0.41514188\n",
      "Iteration 398, loss = 0.41463439\n",
      "Iteration 399, loss = 0.41432691\n",
      "Iteration 400, loss = 0.41387952\n",
      "Iteration 401, loss = 0.41338762\n",
      "Iteration 402, loss = 0.41294774\n",
      "Iteration 403, loss = 0.41248062\n",
      "Iteration 404, loss = 0.41179822\n",
      "Iteration 405, loss = 0.41166829\n",
      "Iteration 406, loss = 0.41098704\n",
      "Iteration 407, loss = 0.41061719\n",
      "Iteration 408, loss = 0.41027646\n",
      "Iteration 409, loss = 0.40954982\n",
      "Iteration 410, loss = 0.40913619\n",
      "Iteration 411, loss = 0.40900813\n",
      "Iteration 412, loss = 0.40835079\n",
      "Iteration 413, loss = 0.40805966\n",
      "Iteration 414, loss = 0.40740800\n",
      "Iteration 415, loss = 0.40691576\n",
      "Iteration 416, loss = 0.40666532\n",
      "Iteration 417, loss = 0.40628580\n",
      "Iteration 418, loss = 0.40578870\n",
      "Iteration 419, loss = 0.40526083\n",
      "Iteration 420, loss = 0.40518888\n",
      "Iteration 421, loss = 0.40422302\n",
      "Iteration 422, loss = 0.40399711\n",
      "Iteration 423, loss = 0.40347837\n",
      "Iteration 424, loss = 0.40351370\n",
      "Iteration 425, loss = 0.40279806\n",
      "Iteration 426, loss = 0.40216414\n",
      "Iteration 427, loss = 0.40184860\n",
      "Iteration 428, loss = 0.40158883\n",
      "Iteration 429, loss = 0.40101371\n",
      "Iteration 430, loss = 0.40048092\n",
      "Iteration 431, loss = 0.40016270\n",
      "Iteration 432, loss = 0.40006552\n",
      "Iteration 433, loss = 0.39919258\n",
      "Iteration 434, loss = 0.39890385\n",
      "Iteration 435, loss = 0.39873886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 436, loss = 0.39804106\n",
      "Iteration 437, loss = 0.39765418\n",
      "Iteration 438, loss = 0.39706457\n",
      "Iteration 439, loss = 0.39685451\n",
      "Iteration 440, loss = 0.39631937\n",
      "Iteration 441, loss = 0.39601439\n",
      "Iteration 442, loss = 0.39544215\n",
      "Iteration 443, loss = 0.39521384\n",
      "Iteration 444, loss = 0.39452406\n",
      "Iteration 445, loss = 0.39427371\n",
      "Iteration 446, loss = 0.39375386\n",
      "Iteration 447, loss = 0.39352614\n",
      "Iteration 448, loss = 0.39300198\n",
      "Iteration 449, loss = 0.39254466\n",
      "Iteration 450, loss = 0.39218366\n",
      "Iteration 451, loss = 0.39181746\n",
      "Iteration 452, loss = 0.39122539\n",
      "Iteration 453, loss = 0.39113266\n",
      "Iteration 454, loss = 0.39053665\n",
      "Iteration 455, loss = 0.39006444\n",
      "Iteration 456, loss = 0.38960515\n",
      "Iteration 457, loss = 0.38925846\n",
      "Iteration 458, loss = 0.38908910\n",
      "Iteration 459, loss = 0.38840600\n",
      "Iteration 460, loss = 0.38815103\n",
      "Iteration 461, loss = 0.38792847\n",
      "Iteration 462, loss = 0.38735996\n",
      "Iteration 463, loss = 0.38672043\n",
      "Iteration 464, loss = 0.38666014\n",
      "Iteration 465, loss = 0.38619967\n",
      "Iteration 466, loss = 0.38592017\n",
      "Iteration 467, loss = 0.38523160\n",
      "Iteration 468, loss = 0.38480292\n",
      "Iteration 469, loss = 0.38428022\n",
      "Iteration 470, loss = 0.38459618\n",
      "Iteration 471, loss = 0.38384895\n",
      "Iteration 472, loss = 0.38335786\n",
      "Iteration 473, loss = 0.38272691\n",
      "Iteration 474, loss = 0.38258924\n",
      "Iteration 475, loss = 0.38229146\n",
      "Iteration 476, loss = 0.38208115\n",
      "Iteration 477, loss = 0.38126783\n",
      "Iteration 478, loss = 0.38119711\n",
      "Iteration 479, loss = 0.38035606\n",
      "Iteration 480, loss = 0.38066049\n",
      "Iteration 481, loss = 0.37980017\n",
      "Iteration 482, loss = 0.37959545\n",
      "Iteration 483, loss = 0.37916966\n",
      "Iteration 484, loss = 0.37841990\n",
      "Iteration 485, loss = 0.37830169\n",
      "Iteration 486, loss = 0.37763859\n",
      "Iteration 487, loss = 0.37753334\n",
      "Iteration 488, loss = 0.37700103\n",
      "Iteration 489, loss = 0.37715723\n",
      "Iteration 490, loss = 0.37666330\n",
      "Iteration 491, loss = 0.37552012\n",
      "Iteration 492, loss = 0.37559320\n",
      "Iteration 493, loss = 0.37505641\n",
      "Iteration 494, loss = 0.37471701\n",
      "Iteration 495, loss = 0.37432825\n",
      "Iteration 496, loss = 0.37406520\n",
      "Iteration 497, loss = 0.37366179\n",
      "Iteration 498, loss = 0.37318557\n",
      "Iteration 499, loss = 0.37282053\n",
      "Iteration 500, loss = 0.37230614\n",
      "Iteration 501, loss = 0.37189035\n",
      "Iteration 502, loss = 0.37185163\n",
      "Iteration 503, loss = 0.37129057\n",
      "Iteration 504, loss = 0.37097699\n",
      "Iteration 505, loss = 0.37039666\n",
      "Iteration 506, loss = 0.37007588\n",
      "Iteration 507, loss = 0.36948593\n",
      "Iteration 508, loss = 0.36960140\n",
      "Iteration 509, loss = 0.36893827\n",
      "Iteration 510, loss = 0.36845757\n",
      "Iteration 511, loss = 0.36842636\n",
      "Iteration 512, loss = 0.36772366\n",
      "Iteration 513, loss = 0.36746447\n",
      "Iteration 514, loss = 0.36742562\n",
      "Iteration 515, loss = 0.36667802\n",
      "Iteration 516, loss = 0.36642122\n",
      "Iteration 517, loss = 0.36601570\n",
      "Iteration 518, loss = 0.36591713\n",
      "Iteration 519, loss = 0.36535887\n",
      "Iteration 520, loss = 0.36462730\n",
      "Iteration 521, loss = 0.36447929\n",
      "Iteration 522, loss = 0.36412582\n",
      "Iteration 523, loss = 0.36353161\n",
      "Iteration 524, loss = 0.36352458\n",
      "Iteration 525, loss = 0.36311353\n",
      "Iteration 526, loss = 0.36252171\n",
      "Iteration 527, loss = 0.36262289\n",
      "Iteration 528, loss = 0.36194827\n",
      "Iteration 529, loss = 0.36161624\n",
      "Iteration 530, loss = 0.36135275\n",
      "Iteration 531, loss = 0.36074108\n",
      "Iteration 532, loss = 0.36053799\n",
      "Iteration 533, loss = 0.35978940\n",
      "Iteration 534, loss = 0.35944814\n",
      "Iteration 535, loss = 0.35924362\n",
      "Iteration 536, loss = 0.35875633\n",
      "Iteration 537, loss = 0.35828697\n",
      "Iteration 538, loss = 0.35797438\n",
      "Iteration 539, loss = 0.35746488\n",
      "Iteration 540, loss = 0.35742965\n",
      "Iteration 541, loss = 0.35696078\n",
      "Iteration 542, loss = 0.35642524\n",
      "Iteration 543, loss = 0.35615901\n",
      "Iteration 544, loss = 0.35577933\n",
      "Iteration 545, loss = 0.35535613\n",
      "Iteration 546, loss = 0.35500966\n",
      "Iteration 547, loss = 0.35480797\n",
      "Iteration 548, loss = 0.35468117\n",
      "Iteration 549, loss = 0.35456574\n",
      "Iteration 550, loss = 0.35381315\n",
      "Iteration 551, loss = 0.35298147\n",
      "Iteration 552, loss = 0.35329849\n",
      "Iteration 553, loss = 0.35250736\n",
      "Iteration 554, loss = 0.35227528\n",
      "Iteration 555, loss = 0.35176512\n",
      "Iteration 556, loss = 0.35181488\n",
      "Iteration 557, loss = 0.35115935\n",
      "Iteration 558, loss = 0.35073475\n",
      "Iteration 559, loss = 0.35036011\n",
      "Iteration 560, loss = 0.35015361\n",
      "Iteration 561, loss = 0.34966024\n",
      "Iteration 562, loss = 0.34932367\n",
      "Iteration 563, loss = 0.34886779\n",
      "Iteration 564, loss = 0.34892152\n",
      "Iteration 565, loss = 0.34818817\n",
      "Iteration 566, loss = 0.34784551\n",
      "Iteration 567, loss = 0.34768398\n",
      "Iteration 568, loss = 0.34718969\n",
      "Iteration 569, loss = 0.34707590\n",
      "Iteration 570, loss = 0.34620319\n",
      "Iteration 571, loss = 0.34646420\n",
      "Iteration 572, loss = 0.34609958\n",
      "Iteration 573, loss = 0.34547256\n",
      "Iteration 574, loss = 0.34491194\n",
      "Iteration 575, loss = 0.34505572\n",
      "Iteration 576, loss = 0.34428631\n",
      "Iteration 577, loss = 0.34435328\n",
      "Iteration 578, loss = 0.34371704\n",
      "Iteration 579, loss = 0.34340558\n",
      "Iteration 580, loss = 0.34327112\n",
      "Iteration 581, loss = 0.34231587\n",
      "Iteration 582, loss = 0.34269331\n",
      "Iteration 583, loss = 0.34172229\n",
      "Iteration 584, loss = 0.34161803\n",
      "Iteration 585, loss = 0.34092974\n",
      "Iteration 586, loss = 0.34112932\n",
      "Iteration 587, loss = 0.34062419\n",
      "Iteration 588, loss = 0.34079680\n",
      "Iteration 589, loss = 0.34004294\n",
      "Iteration 590, loss = 0.33949480\n",
      "Iteration 591, loss = 0.33961734\n",
      "Iteration 592, loss = 0.33894721\n",
      "Iteration 593, loss = 0.33893395\n",
      "Iteration 594, loss = 0.33833209\n",
      "Iteration 595, loss = 0.33787554\n",
      "Iteration 596, loss = 0.33786042\n",
      "Iteration 597, loss = 0.33716517\n",
      "Iteration 598, loss = 0.33699661\n",
      "Iteration 599, loss = 0.33699322\n",
      "Iteration 600, loss = 0.33669794\n",
      "Iteration 601, loss = 0.33605946\n",
      "Iteration 602, loss = 0.33552356\n",
      "Iteration 603, loss = 0.33566882\n",
      "Iteration 604, loss = 0.33489024\n",
      "Iteration 605, loss = 0.33456464\n",
      "Iteration 606, loss = 0.33436352\n",
      "Iteration 607, loss = 0.33416446\n",
      "Iteration 608, loss = 0.33367162\n",
      "Iteration 609, loss = 0.33374807\n",
      "Iteration 610, loss = 0.33337582\n",
      "Iteration 611, loss = 0.33283906\n",
      "Iteration 612, loss = 0.33255686\n",
      "Iteration 613, loss = 0.33181271\n",
      "Iteration 614, loss = 0.33167392\n",
      "Iteration 615, loss = 0.33156315\n",
      "Iteration 616, loss = 0.33106046\n",
      "Iteration 617, loss = 0.33072029\n",
      "Iteration 618, loss = 0.33081468\n",
      "Iteration 619, loss = 0.33010951\n",
      "Iteration 620, loss = 0.32977904\n",
      "Iteration 621, loss = 0.32952475\n",
      "Iteration 622, loss = 0.32895848\n",
      "Iteration 623, loss = 0.32895691\n",
      "Iteration 624, loss = 0.32874064\n",
      "Iteration 625, loss = 0.32843349\n",
      "Iteration 626, loss = 0.32803793\n",
      "Iteration 627, loss = 0.32812231\n",
      "Iteration 628, loss = 0.32757714\n",
      "Iteration 629, loss = 0.32704870\n",
      "Iteration 630, loss = 0.32710186\n",
      "Iteration 631, loss = 0.32665990\n",
      "Iteration 632, loss = 0.32594874\n",
      "Iteration 633, loss = 0.32663994\n",
      "Iteration 634, loss = 0.32538681\n",
      "Iteration 635, loss = 0.32510245\n",
      "Iteration 636, loss = 0.32509727\n",
      "Iteration 637, loss = 0.32479025\n",
      "Iteration 638, loss = 0.32433784\n",
      "Iteration 639, loss = 0.32406185\n",
      "Iteration 640, loss = 0.32361739\n",
      "Iteration 641, loss = 0.32320055\n",
      "Iteration 642, loss = 0.32310759\n",
      "Iteration 643, loss = 0.32301195\n",
      "Iteration 644, loss = 0.32261938\n",
      "Iteration 645, loss = 0.32221151\n",
      "Iteration 646, loss = 0.32225956\n",
      "Iteration 647, loss = 0.32148173\n",
      "Iteration 648, loss = 0.32144234\n",
      "Iteration 649, loss = 0.32085547\n",
      "Iteration 650, loss = 0.32089178\n",
      "Iteration 651, loss = 0.32044852\n",
      "Iteration 652, loss = 0.32046425\n",
      "Iteration 653, loss = 0.31975269\n",
      "Iteration 654, loss = 0.31961385\n",
      "Iteration 655, loss = 0.31940490\n",
      "Iteration 656, loss = 0.31901164\n",
      "Iteration 657, loss = 0.31851114\n",
      "Iteration 658, loss = 0.31817353\n",
      "Iteration 659, loss = 0.31827018\n",
      "Iteration 660, loss = 0.31772688\n",
      "Iteration 661, loss = 0.31818802\n",
      "Iteration 662, loss = 0.31717678\n",
      "Iteration 663, loss = 0.31697134\n",
      "Iteration 664, loss = 0.31657389\n",
      "Iteration 665, loss = 0.31627110\n",
      "Iteration 666, loss = 0.31618039\n",
      "Iteration 667, loss = 0.31602421\n",
      "Iteration 668, loss = 0.31536552\n",
      "Iteration 669, loss = 0.31510476\n",
      "Iteration 670, loss = 0.31497417\n",
      "Iteration 671, loss = 0.31431401\n",
      "Iteration 672, loss = 0.31411227\n",
      "Iteration 673, loss = 0.31381794\n",
      "Iteration 674, loss = 0.31376378\n",
      "Iteration 675, loss = 0.31352298\n",
      "Iteration 676, loss = 0.31339434\n",
      "Iteration 677, loss = 0.31281875\n",
      "Iteration 678, loss = 0.31268760\n",
      "Iteration 679, loss = 0.31274300\n",
      "Iteration 680, loss = 0.31199597\n",
      "Iteration 681, loss = 0.31219778\n",
      "Iteration 682, loss = 0.31135544\n",
      "Iteration 683, loss = 0.31170172\n",
      "Iteration 684, loss = 0.31118719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 685, loss = 0.31054214\n",
      "Iteration 686, loss = 0.31093125\n",
      "Iteration 687, loss = 0.31076122\n",
      "Iteration 688, loss = 0.30973997\n",
      "Iteration 689, loss = 0.30971673\n",
      "Iteration 690, loss = 0.30932701\n",
      "Iteration 691, loss = 0.30873887\n",
      "Iteration 692, loss = 0.30902485\n",
      "Iteration 693, loss = 0.30872589\n",
      "Iteration 694, loss = 0.30852679\n",
      "Iteration 695, loss = 0.30788611\n",
      "Iteration 696, loss = 0.30812521\n",
      "Iteration 697, loss = 0.30767834\n",
      "Iteration 698, loss = 0.30704101\n",
      "Iteration 699, loss = 0.30697314\n",
      "Iteration 700, loss = 0.30659665\n",
      "Iteration 701, loss = 0.30669214\n",
      "Iteration 702, loss = 0.30607961\n",
      "Iteration 703, loss = 0.30607259\n",
      "Iteration 704, loss = 0.30522630\n",
      "Iteration 705, loss = 0.30521808\n",
      "Iteration 706, loss = 0.30492693\n",
      "Iteration 707, loss = 0.30457960\n",
      "Iteration 708, loss = 0.30461022\n",
      "Iteration 709, loss = 0.30417265\n",
      "Iteration 710, loss = 0.30427350\n",
      "Iteration 711, loss = 0.30328857\n",
      "Iteration 712, loss = 0.30367819\n",
      "Iteration 713, loss = 0.30343882\n",
      "Iteration 714, loss = 0.30350577\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69663893\n",
      "Iteration 2, loss = 0.68939766\n",
      "Iteration 3, loss = 0.68270676\n",
      "Iteration 4, loss = 0.67671314\n",
      "Iteration 5, loss = 0.67135407\n",
      "Iteration 6, loss = 0.66646309\n",
      "Iteration 7, loss = 0.66190605\n",
      "Iteration 8, loss = 0.65764935\n",
      "Iteration 9, loss = 0.65377359\n",
      "Iteration 10, loss = 0.65016171\n",
      "Iteration 11, loss = 0.64679293\n",
      "Iteration 12, loss = 0.64372773\n",
      "Iteration 13, loss = 0.64084488\n",
      "Iteration 14, loss = 0.63820367\n",
      "Iteration 15, loss = 0.63577574\n",
      "Iteration 16, loss = 0.63352382\n",
      "Iteration 17, loss = 0.63147125\n",
      "Iteration 18, loss = 0.62957379\n",
      "Iteration 19, loss = 0.62779976\n",
      "Iteration 20, loss = 0.62614803\n",
      "Iteration 21, loss = 0.62459887\n",
      "Iteration 22, loss = 0.62314918\n",
      "Iteration 23, loss = 0.62178333\n",
      "Iteration 24, loss = 0.62047021\n",
      "Iteration 25, loss = 0.61927088\n",
      "Iteration 26, loss = 0.61808451\n",
      "Iteration 27, loss = 0.61692811\n",
      "Iteration 28, loss = 0.61579452\n",
      "Iteration 29, loss = 0.61472808\n",
      "Iteration 30, loss = 0.61370369\n",
      "Iteration 31, loss = 0.61265735\n",
      "Iteration 32, loss = 0.61167244\n",
      "Iteration 33, loss = 0.61065228\n",
      "Iteration 34, loss = 0.60968302\n",
      "Iteration 35, loss = 0.60875104\n",
      "Iteration 36, loss = 0.60780043\n",
      "Iteration 37, loss = 0.60689408\n",
      "Iteration 38, loss = 0.60596033\n",
      "Iteration 39, loss = 0.60501165\n",
      "Iteration 40, loss = 0.60406967\n",
      "Iteration 41, loss = 0.60318293\n",
      "Iteration 42, loss = 0.60228841\n",
      "Iteration 43, loss = 0.60143072\n",
      "Iteration 44, loss = 0.60048709\n",
      "Iteration 45, loss = 0.59964272\n",
      "Iteration 46, loss = 0.59875692\n",
      "Iteration 47, loss = 0.59782812\n",
      "Iteration 48, loss = 0.59697638\n",
      "Iteration 49, loss = 0.59608058\n",
      "Iteration 50, loss = 0.59523581\n",
      "Iteration 51, loss = 0.59434283\n",
      "Iteration 52, loss = 0.59356958\n",
      "Iteration 53, loss = 0.59266454\n",
      "Iteration 54, loss = 0.59181707\n",
      "Iteration 55, loss = 0.59096712\n",
      "Iteration 56, loss = 0.59015600\n",
      "Iteration 57, loss = 0.58928283\n",
      "Iteration 58, loss = 0.58849581\n",
      "Iteration 59, loss = 0.58766539\n",
      "Iteration 60, loss = 0.58680094\n",
      "Iteration 61, loss = 0.58600236\n",
      "Iteration 62, loss = 0.58516324\n",
      "Iteration 63, loss = 0.58441632\n",
      "Iteration 64, loss = 0.58357280\n",
      "Iteration 65, loss = 0.58276919\n",
      "Iteration 66, loss = 0.58198964\n",
      "Iteration 67, loss = 0.58121976\n",
      "Iteration 68, loss = 0.58041418\n",
      "Iteration 69, loss = 0.57964540\n",
      "Iteration 70, loss = 0.57885753\n",
      "Iteration 71, loss = 0.57814214\n",
      "Iteration 72, loss = 0.57742396\n",
      "Iteration 73, loss = 0.57659770\n",
      "Iteration 74, loss = 0.57586656\n",
      "Iteration 75, loss = 0.57516322\n",
      "Iteration 76, loss = 0.57440362\n",
      "Iteration 77, loss = 0.57367377\n",
      "Iteration 78, loss = 0.57298059\n",
      "Iteration 79, loss = 0.57225202\n",
      "Iteration 80, loss = 0.57170016\n",
      "Iteration 81, loss = 0.57082213\n",
      "Iteration 82, loss = 0.57014249\n",
      "Iteration 83, loss = 0.56938653\n",
      "Iteration 84, loss = 0.56874773\n",
      "Iteration 85, loss = 0.56810548\n",
      "Iteration 86, loss = 0.56739374\n",
      "Iteration 87, loss = 0.56677460\n",
      "Iteration 88, loss = 0.56605204\n",
      "Iteration 89, loss = 0.56535690\n",
      "Iteration 90, loss = 0.56475930\n",
      "Iteration 91, loss = 0.56411489\n",
      "Iteration 92, loss = 0.56342585\n",
      "Iteration 93, loss = 0.56282703\n",
      "Iteration 94, loss = 0.56217794\n",
      "Iteration 95, loss = 0.56154988\n",
      "Iteration 96, loss = 0.56089144\n",
      "Iteration 97, loss = 0.56027615\n",
      "Iteration 98, loss = 0.55968642\n",
      "Iteration 99, loss = 0.55909197\n",
      "Iteration 100, loss = 0.55847018\n",
      "Iteration 101, loss = 0.55784779\n",
      "Iteration 102, loss = 0.55723686\n",
      "Iteration 103, loss = 0.55664261\n",
      "Iteration 104, loss = 0.55613118\n",
      "Iteration 105, loss = 0.55561629\n",
      "Iteration 106, loss = 0.55496225\n",
      "Iteration 107, loss = 0.55442763\n",
      "Iteration 108, loss = 0.55378451\n",
      "Iteration 109, loss = 0.55326267\n",
      "Iteration 110, loss = 0.55264626\n",
      "Iteration 111, loss = 0.55215921\n",
      "Iteration 112, loss = 0.55157701\n",
      "Iteration 113, loss = 0.55102703\n",
      "Iteration 114, loss = 0.55050710\n",
      "Iteration 115, loss = 0.54995326\n",
      "Iteration 116, loss = 0.54936100\n",
      "Iteration 117, loss = 0.54882986\n",
      "Iteration 118, loss = 0.54829561\n",
      "Iteration 119, loss = 0.54773976\n",
      "Iteration 120, loss = 0.54720348\n",
      "Iteration 121, loss = 0.54667154\n",
      "Iteration 122, loss = 0.54613406\n",
      "Iteration 123, loss = 0.54561465\n",
      "Iteration 124, loss = 0.54515120\n",
      "Iteration 125, loss = 0.54457357\n",
      "Iteration 126, loss = 0.54403174\n",
      "Iteration 127, loss = 0.54346478\n",
      "Iteration 128, loss = 0.54303811\n",
      "Iteration 129, loss = 0.54259959\n",
      "Iteration 130, loss = 0.54191583\n",
      "Iteration 131, loss = 0.54149458\n",
      "Iteration 132, loss = 0.54096156\n",
      "Iteration 133, loss = 0.54033388\n",
      "Iteration 134, loss = 0.53993759\n",
      "Iteration 135, loss = 0.53937192\n",
      "Iteration 136, loss = 0.53885034\n",
      "Iteration 137, loss = 0.53834931\n",
      "Iteration 138, loss = 0.53779721\n",
      "Iteration 139, loss = 0.53732573\n",
      "Iteration 140, loss = 0.53680142\n",
      "Iteration 141, loss = 0.53626923\n",
      "Iteration 142, loss = 0.53574847\n",
      "Iteration 143, loss = 0.53523328\n",
      "Iteration 144, loss = 0.53476857\n",
      "Iteration 145, loss = 0.53430712\n",
      "Iteration 146, loss = 0.53382707\n",
      "Iteration 147, loss = 0.53330494\n",
      "Iteration 148, loss = 0.53281550\n",
      "Iteration 149, loss = 0.53234221\n",
      "Iteration 150, loss = 0.53185840\n",
      "Iteration 151, loss = 0.53138709\n",
      "Iteration 152, loss = 0.53096841\n",
      "Iteration 153, loss = 0.53037196\n",
      "Iteration 154, loss = 0.52991183\n",
      "Iteration 155, loss = 0.52932646\n",
      "Iteration 156, loss = 0.52883258\n",
      "Iteration 157, loss = 0.52836314\n",
      "Iteration 158, loss = 0.52791348\n",
      "Iteration 159, loss = 0.52742019\n",
      "Iteration 160, loss = 0.52684027\n",
      "Iteration 161, loss = 0.52636420\n",
      "Iteration 162, loss = 0.52586709\n",
      "Iteration 163, loss = 0.52538136\n",
      "Iteration 164, loss = 0.52498353\n",
      "Iteration 165, loss = 0.52444446\n",
      "Iteration 166, loss = 0.52397418\n",
      "Iteration 167, loss = 0.52350186\n",
      "Iteration 168, loss = 0.52296721\n",
      "Iteration 169, loss = 0.52243963\n",
      "Iteration 170, loss = 0.52200843\n",
      "Iteration 171, loss = 0.52146089\n",
      "Iteration 172, loss = 0.52103216\n",
      "Iteration 173, loss = 0.52047457\n",
      "Iteration 174, loss = 0.52002492\n",
      "Iteration 175, loss = 0.51970932\n",
      "Iteration 176, loss = 0.51909364\n",
      "Iteration 177, loss = 0.51859303\n",
      "Iteration 178, loss = 0.51804965\n",
      "Iteration 179, loss = 0.51759099\n",
      "Iteration 180, loss = 0.51721914\n",
      "Iteration 181, loss = 0.51662179\n",
      "Iteration 182, loss = 0.51617267\n",
      "Iteration 183, loss = 0.51571585\n",
      "Iteration 184, loss = 0.51518989\n",
      "Iteration 185, loss = 0.51471476\n",
      "Iteration 186, loss = 0.51423593\n",
      "Iteration 187, loss = 0.51366796\n",
      "Iteration 188, loss = 0.51319089\n",
      "Iteration 189, loss = 0.51273149\n",
      "Iteration 190, loss = 0.51230467\n",
      "Iteration 191, loss = 0.51178295\n",
      "Iteration 192, loss = 0.51139386\n",
      "Iteration 193, loss = 0.51083166\n",
      "Iteration 194, loss = 0.51059961\n",
      "Iteration 195, loss = 0.50987571\n",
      "Iteration 196, loss = 0.50946647\n",
      "Iteration 197, loss = 0.50882684\n",
      "Iteration 198, loss = 0.50845513\n",
      "Iteration 199, loss = 0.50794787\n",
      "Iteration 200, loss = 0.50743343\n",
      "Iteration 201, loss = 0.50700631\n",
      "Iteration 202, loss = 0.50649699\n",
      "Iteration 203, loss = 0.50597860\n",
      "Iteration 204, loss = 0.50544489\n",
      "Iteration 205, loss = 0.50510459\n",
      "Iteration 206, loss = 0.50456177\n",
      "Iteration 207, loss = 0.50400250\n",
      "Iteration 208, loss = 0.50355289\n",
      "Iteration 209, loss = 0.50303776\n",
      "Iteration 210, loss = 0.50265719\n",
      "Iteration 211, loss = 0.50204506\n",
      "Iteration 212, loss = 0.50158926\n",
      "Iteration 213, loss = 0.50108487\n",
      "Iteration 214, loss = 0.50064818\n",
      "Iteration 215, loss = 0.50010246\n",
      "Iteration 216, loss = 0.49962812\n",
      "Iteration 217, loss = 0.49907983\n",
      "Iteration 218, loss = 0.49858508\n",
      "Iteration 219, loss = 0.49815044\n",
      "Iteration 220, loss = 0.49767812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 221, loss = 0.49709485\n",
      "Iteration 222, loss = 0.49669166\n",
      "Iteration 223, loss = 0.49615130\n",
      "Iteration 224, loss = 0.49568555\n",
      "Iteration 225, loss = 0.49515898\n",
      "Iteration 226, loss = 0.49478638\n",
      "Iteration 227, loss = 0.49430731\n",
      "Iteration 228, loss = 0.49368869\n",
      "Iteration 229, loss = 0.49334034\n",
      "Iteration 230, loss = 0.49264561\n",
      "Iteration 231, loss = 0.49214190\n",
      "Iteration 232, loss = 0.49162176\n",
      "Iteration 233, loss = 0.49119566\n",
      "Iteration 234, loss = 0.49071810\n",
      "Iteration 235, loss = 0.49021729\n",
      "Iteration 236, loss = 0.48969232\n",
      "Iteration 237, loss = 0.48925012\n",
      "Iteration 238, loss = 0.48869139\n",
      "Iteration 239, loss = 0.48825505\n",
      "Iteration 240, loss = 0.48765115\n",
      "Iteration 241, loss = 0.48712999\n",
      "Iteration 242, loss = 0.48668773\n",
      "Iteration 243, loss = 0.48623573\n",
      "Iteration 244, loss = 0.48565668\n",
      "Iteration 245, loss = 0.48506713\n",
      "Iteration 246, loss = 0.48466149\n",
      "Iteration 247, loss = 0.48416659\n",
      "Iteration 248, loss = 0.48360919\n",
      "Iteration 249, loss = 0.48299516\n",
      "Iteration 250, loss = 0.48243485\n",
      "Iteration 251, loss = 0.48201720\n",
      "Iteration 252, loss = 0.48145114\n",
      "Iteration 253, loss = 0.48085034\n",
      "Iteration 254, loss = 0.48046671\n",
      "Iteration 255, loss = 0.47984497\n",
      "Iteration 256, loss = 0.47927970\n",
      "Iteration 257, loss = 0.47873947\n",
      "Iteration 258, loss = 0.47821754\n",
      "Iteration 259, loss = 0.47769019\n",
      "Iteration 260, loss = 0.47740775\n",
      "Iteration 261, loss = 0.47653812\n",
      "Iteration 262, loss = 0.47608671\n",
      "Iteration 263, loss = 0.47550314\n",
      "Iteration 264, loss = 0.47517943\n",
      "Iteration 265, loss = 0.47452135\n",
      "Iteration 266, loss = 0.47386553\n",
      "Iteration 267, loss = 0.47325793\n",
      "Iteration 268, loss = 0.47281830\n",
      "Iteration 269, loss = 0.47219733\n",
      "Iteration 270, loss = 0.47174765\n",
      "Iteration 271, loss = 0.47137245\n",
      "Iteration 272, loss = 0.47071733\n",
      "Iteration 273, loss = 0.47022676\n",
      "Iteration 274, loss = 0.46961785\n",
      "Iteration 275, loss = 0.46905875\n",
      "Iteration 276, loss = 0.46839924\n",
      "Iteration 277, loss = 0.46809029\n",
      "Iteration 278, loss = 0.46752687\n",
      "Iteration 279, loss = 0.46693328\n",
      "Iteration 280, loss = 0.46625810\n",
      "Iteration 281, loss = 0.46577889\n",
      "Iteration 282, loss = 0.46527741\n",
      "Iteration 283, loss = 0.46467148\n",
      "Iteration 284, loss = 0.46407459\n",
      "Iteration 285, loss = 0.46366034\n",
      "Iteration 286, loss = 0.46302226\n",
      "Iteration 287, loss = 0.46259139\n",
      "Iteration 288, loss = 0.46186292\n",
      "Iteration 289, loss = 0.46135679\n",
      "Iteration 290, loss = 0.46093290\n",
      "Iteration 291, loss = 0.46039550\n",
      "Iteration 292, loss = 0.45997841\n",
      "Iteration 293, loss = 0.45950034\n",
      "Iteration 294, loss = 0.45874947\n",
      "Iteration 295, loss = 0.45810079\n",
      "Iteration 296, loss = 0.45762534\n",
      "Iteration 297, loss = 0.45720486\n",
      "Iteration 298, loss = 0.45650215\n",
      "Iteration 299, loss = 0.45610964\n",
      "Iteration 300, loss = 0.45549608\n",
      "Iteration 301, loss = 0.45506245\n",
      "Iteration 302, loss = 0.45465148\n",
      "Iteration 303, loss = 0.45399278\n",
      "Iteration 304, loss = 0.45354790\n",
      "Iteration 305, loss = 0.45279594\n",
      "Iteration 306, loss = 0.45253111\n",
      "Iteration 307, loss = 0.45183555\n",
      "Iteration 308, loss = 0.45143242\n",
      "Iteration 309, loss = 0.45076342\n",
      "Iteration 310, loss = 0.45049753\n",
      "Iteration 311, loss = 0.44984282\n",
      "Iteration 312, loss = 0.44923430\n",
      "Iteration 313, loss = 0.44903635\n",
      "Iteration 314, loss = 0.44823983\n",
      "Iteration 315, loss = 0.44777850\n",
      "Iteration 316, loss = 0.44723044\n",
      "Iteration 317, loss = 0.44672357\n",
      "Iteration 318, loss = 0.44615788\n",
      "Iteration 319, loss = 0.44563288\n",
      "Iteration 320, loss = 0.44531003\n",
      "Iteration 321, loss = 0.44451490\n",
      "Iteration 322, loss = 0.44417641\n",
      "Iteration 323, loss = 0.44352250\n",
      "Iteration 324, loss = 0.44338190\n",
      "Iteration 325, loss = 0.44276669\n",
      "Iteration 326, loss = 0.44210523\n",
      "Iteration 327, loss = 0.44152171\n",
      "Iteration 328, loss = 0.44125754\n",
      "Iteration 329, loss = 0.44055644\n",
      "Iteration 330, loss = 0.44009734\n",
      "Iteration 331, loss = 0.43951378\n",
      "Iteration 332, loss = 0.43903675\n",
      "Iteration 333, loss = 0.43855008\n",
      "Iteration 334, loss = 0.43798871\n",
      "Iteration 335, loss = 0.43757162\n",
      "Iteration 336, loss = 0.43696261\n",
      "Iteration 337, loss = 0.43642107\n",
      "Iteration 338, loss = 0.43593380\n",
      "Iteration 339, loss = 0.43545670\n",
      "Iteration 340, loss = 0.43493729\n",
      "Iteration 341, loss = 0.43453482\n",
      "Iteration 342, loss = 0.43407671\n",
      "Iteration 343, loss = 0.43355748\n",
      "Iteration 344, loss = 0.43293262\n",
      "Iteration 345, loss = 0.43247941\n",
      "Iteration 346, loss = 0.43188959\n",
      "Iteration 347, loss = 0.43118820\n",
      "Iteration 348, loss = 0.43102365\n",
      "Iteration 349, loss = 0.43020557\n",
      "Iteration 350, loss = 0.42972400\n",
      "Iteration 351, loss = 0.42926925\n",
      "Iteration 352, loss = 0.42893538\n",
      "Iteration 353, loss = 0.42831209\n",
      "Iteration 354, loss = 0.42766001\n",
      "Iteration 355, loss = 0.42718441\n",
      "Iteration 356, loss = 0.42693017\n",
      "Iteration 357, loss = 0.42613160\n",
      "Iteration 358, loss = 0.42561081\n",
      "Iteration 359, loss = 0.42525297\n",
      "Iteration 360, loss = 0.42456937\n",
      "Iteration 361, loss = 0.42423228\n",
      "Iteration 362, loss = 0.42353888\n",
      "Iteration 363, loss = 0.42300379\n",
      "Iteration 364, loss = 0.42265124\n",
      "Iteration 365, loss = 0.42204107\n",
      "Iteration 366, loss = 0.42162118\n",
      "Iteration 367, loss = 0.42119870\n",
      "Iteration 368, loss = 0.42053149\n",
      "Iteration 369, loss = 0.42008584\n",
      "Iteration 370, loss = 0.41974127\n",
      "Iteration 371, loss = 0.41932818\n",
      "Iteration 372, loss = 0.41856018\n",
      "Iteration 373, loss = 0.41825594\n",
      "Iteration 374, loss = 0.41772401\n",
      "Iteration 375, loss = 0.41705269\n",
      "Iteration 376, loss = 0.41655213\n",
      "Iteration 377, loss = 0.41606933\n",
      "Iteration 378, loss = 0.41563678\n",
      "Iteration 379, loss = 0.41545648\n",
      "Iteration 380, loss = 0.41483376\n",
      "Iteration 381, loss = 0.41408542\n",
      "Iteration 382, loss = 0.41398681\n",
      "Iteration 383, loss = 0.41338199\n",
      "Iteration 384, loss = 0.41283703\n",
      "Iteration 385, loss = 0.41207283\n",
      "Iteration 386, loss = 0.41161778\n",
      "Iteration 387, loss = 0.41111031\n",
      "Iteration 388, loss = 0.41098729\n",
      "Iteration 389, loss = 0.41009725\n",
      "Iteration 390, loss = 0.40972201\n",
      "Iteration 391, loss = 0.40936535\n",
      "Iteration 392, loss = 0.40909763\n",
      "Iteration 393, loss = 0.40820789\n",
      "Iteration 394, loss = 0.40772402\n",
      "Iteration 395, loss = 0.40733613\n",
      "Iteration 396, loss = 0.40680393\n",
      "Iteration 397, loss = 0.40639548\n",
      "Iteration 398, loss = 0.40578746\n",
      "Iteration 399, loss = 0.40551476\n",
      "Iteration 400, loss = 0.40511331\n",
      "Iteration 401, loss = 0.40439424\n",
      "Iteration 402, loss = 0.40404868\n",
      "Iteration 403, loss = 0.40367181\n",
      "Iteration 404, loss = 0.40281918\n",
      "Iteration 405, loss = 0.40282112\n",
      "Iteration 406, loss = 0.40201451\n",
      "Iteration 407, loss = 0.40177120\n",
      "Iteration 408, loss = 0.40133548\n",
      "Iteration 409, loss = 0.40063861\n",
      "Iteration 410, loss = 0.40002325\n",
      "Iteration 411, loss = 0.39972082\n",
      "Iteration 412, loss = 0.39918534\n",
      "Iteration 413, loss = 0.39869283\n",
      "Iteration 414, loss = 0.39817241\n",
      "Iteration 415, loss = 0.39770049\n",
      "Iteration 416, loss = 0.39733415\n",
      "Iteration 417, loss = 0.39692481\n",
      "Iteration 418, loss = 0.39646657\n",
      "Iteration 419, loss = 0.39588501\n",
      "Iteration 420, loss = 0.39581684\n",
      "Iteration 421, loss = 0.39514410\n",
      "Iteration 422, loss = 0.39456993\n",
      "Iteration 423, loss = 0.39412043\n",
      "Iteration 424, loss = 0.39377973\n",
      "Iteration 425, loss = 0.39327723\n",
      "Iteration 426, loss = 0.39268587\n",
      "Iteration 427, loss = 0.39222972\n",
      "Iteration 428, loss = 0.39196547\n",
      "Iteration 429, loss = 0.39124786\n",
      "Iteration 430, loss = 0.39114408\n",
      "Iteration 431, loss = 0.39063515\n",
      "Iteration 432, loss = 0.39025373\n",
      "Iteration 433, loss = 0.38948922\n",
      "Iteration 434, loss = 0.38899880\n",
      "Iteration 435, loss = 0.38881473\n",
      "Iteration 436, loss = 0.38826262\n",
      "Iteration 437, loss = 0.38831768\n",
      "Iteration 438, loss = 0.38750288\n",
      "Iteration 439, loss = 0.38695306\n",
      "Iteration 440, loss = 0.38669586\n",
      "Iteration 441, loss = 0.38608028\n",
      "Iteration 442, loss = 0.38568269\n",
      "Iteration 443, loss = 0.38536001\n",
      "Iteration 444, loss = 0.38473770\n",
      "Iteration 445, loss = 0.38434703\n",
      "Iteration 446, loss = 0.38383803\n",
      "Iteration 447, loss = 0.38363765\n",
      "Iteration 448, loss = 0.38319235\n",
      "Iteration 449, loss = 0.38280146\n",
      "Iteration 450, loss = 0.38241467\n",
      "Iteration 451, loss = 0.38206529\n",
      "Iteration 452, loss = 0.38125631\n",
      "Iteration 453, loss = 0.38087053\n",
      "Iteration 454, loss = 0.38046057\n",
      "Iteration 455, loss = 0.38012805\n",
      "Iteration 456, loss = 0.37959949\n",
      "Iteration 457, loss = 0.37918048\n",
      "Iteration 458, loss = 0.37896518\n",
      "Iteration 459, loss = 0.37805315\n",
      "Iteration 460, loss = 0.37833768\n",
      "Iteration 461, loss = 0.37781200\n",
      "Iteration 462, loss = 0.37736037\n",
      "Iteration 463, loss = 0.37651469\n",
      "Iteration 464, loss = 0.37619810\n",
      "Iteration 465, loss = 0.37619477\n",
      "Iteration 466, loss = 0.37521880\n",
      "Iteration 467, loss = 0.37481922\n",
      "Iteration 468, loss = 0.37442593\n",
      "Iteration 469, loss = 0.37430099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 470, loss = 0.37399995\n",
      "Iteration 471, loss = 0.37347247\n",
      "Iteration 472, loss = 0.37301883\n",
      "Iteration 473, loss = 0.37263190\n",
      "Iteration 474, loss = 0.37197596\n",
      "Iteration 475, loss = 0.37177863\n",
      "Iteration 476, loss = 0.37137400\n",
      "Iteration 477, loss = 0.37104225\n",
      "Iteration 478, loss = 0.37052257\n",
      "Iteration 479, loss = 0.37003301\n",
      "Iteration 480, loss = 0.37013935\n",
      "Iteration 481, loss = 0.36960944\n",
      "Iteration 482, loss = 0.36905336\n",
      "Iteration 483, loss = 0.36859684\n",
      "Iteration 484, loss = 0.36797542\n",
      "Iteration 485, loss = 0.36786111\n",
      "Iteration 486, loss = 0.36730413\n",
      "Iteration 487, loss = 0.36700252\n",
      "Iteration 488, loss = 0.36630826\n",
      "Iteration 489, loss = 0.36641557\n",
      "Iteration 490, loss = 0.36593001\n",
      "Iteration 491, loss = 0.36492483\n",
      "Iteration 492, loss = 0.36510481\n",
      "Iteration 493, loss = 0.36445481\n",
      "Iteration 494, loss = 0.36423478\n",
      "Iteration 495, loss = 0.36377051\n",
      "Iteration 496, loss = 0.36348876\n",
      "Iteration 497, loss = 0.36305678\n",
      "Iteration 498, loss = 0.36278724\n",
      "Iteration 499, loss = 0.36225278\n",
      "Iteration 500, loss = 0.36205152\n",
      "Iteration 501, loss = 0.36160103\n",
      "Iteration 502, loss = 0.36125410\n",
      "Iteration 503, loss = 0.36060550\n",
      "Iteration 504, loss = 0.36066900\n",
      "Iteration 505, loss = 0.35976778\n",
      "Iteration 506, loss = 0.35971651\n",
      "Iteration 507, loss = 0.35924375\n",
      "Iteration 508, loss = 0.35916738\n",
      "Iteration 509, loss = 0.35862706\n",
      "Iteration 510, loss = 0.35813689\n",
      "Iteration 511, loss = 0.35824153\n",
      "Iteration 512, loss = 0.35728475\n",
      "Iteration 513, loss = 0.35723263\n",
      "Iteration 514, loss = 0.35671853\n",
      "Iteration 515, loss = 0.35621315\n",
      "Iteration 516, loss = 0.35588962\n",
      "Iteration 517, loss = 0.35554431\n",
      "Iteration 518, loss = 0.35562053\n",
      "Iteration 519, loss = 0.35510072\n",
      "Iteration 520, loss = 0.35454882\n",
      "Iteration 521, loss = 0.35401294\n",
      "Iteration 522, loss = 0.35382556\n",
      "Iteration 523, loss = 0.35315257\n",
      "Iteration 524, loss = 0.35310036\n",
      "Iteration 525, loss = 0.35276949\n",
      "Iteration 526, loss = 0.35245950\n",
      "Iteration 527, loss = 0.35222377\n",
      "Iteration 528, loss = 0.35209959\n",
      "Iteration 529, loss = 0.35190068\n",
      "Iteration 530, loss = 0.35106265\n",
      "Iteration 531, loss = 0.35050351\n",
      "Iteration 532, loss = 0.35032286\n",
      "Iteration 533, loss = 0.34981331\n",
      "Iteration 534, loss = 0.34974559\n",
      "Iteration 535, loss = 0.34908046\n",
      "Iteration 536, loss = 0.34897601\n",
      "Iteration 537, loss = 0.34846838\n",
      "Iteration 538, loss = 0.34801811\n",
      "Iteration 539, loss = 0.34804387\n",
      "Iteration 540, loss = 0.34736963\n",
      "Iteration 541, loss = 0.34787742\n",
      "Iteration 542, loss = 0.34661916\n",
      "Iteration 543, loss = 0.34656961\n",
      "Iteration 544, loss = 0.34590138\n",
      "Iteration 545, loss = 0.34579040\n",
      "Iteration 546, loss = 0.34537974\n",
      "Iteration 547, loss = 0.34514383\n",
      "Iteration 548, loss = 0.34505785\n",
      "Iteration 549, loss = 0.34514433\n",
      "Iteration 550, loss = 0.34453102\n",
      "Iteration 551, loss = 0.34352617\n",
      "Iteration 552, loss = 0.34392381\n",
      "Iteration 553, loss = 0.34323102\n",
      "Iteration 554, loss = 0.34291840\n",
      "Iteration 555, loss = 0.34228313\n",
      "Iteration 556, loss = 0.34243752\n",
      "Iteration 557, loss = 0.34172033\n",
      "Iteration 558, loss = 0.34150971\n",
      "Iteration 559, loss = 0.34105889\n",
      "Iteration 560, loss = 0.34089320\n",
      "Iteration 561, loss = 0.34066933\n",
      "Iteration 562, loss = 0.34041430\n",
      "Iteration 563, loss = 0.33989125\n",
      "Iteration 564, loss = 0.33944935\n",
      "Iteration 565, loss = 0.33906837\n",
      "Iteration 566, loss = 0.33880500\n",
      "Iteration 567, loss = 0.33866987\n",
      "Iteration 568, loss = 0.33832606\n",
      "Iteration 569, loss = 0.33802259\n",
      "Iteration 570, loss = 0.33777330\n",
      "Iteration 571, loss = 0.33741106\n",
      "Iteration 572, loss = 0.33702188\n",
      "Iteration 573, loss = 0.33690840\n",
      "Iteration 574, loss = 0.33639025\n",
      "Iteration 575, loss = 0.33624919\n",
      "Iteration 576, loss = 0.33544304\n",
      "Iteration 577, loss = 0.33569819\n",
      "Iteration 578, loss = 0.33525428\n",
      "Iteration 579, loss = 0.33450930\n",
      "Iteration 580, loss = 0.33447932\n",
      "Iteration 581, loss = 0.33397399\n",
      "Iteration 582, loss = 0.33428603\n",
      "Iteration 583, loss = 0.33349597\n",
      "Iteration 584, loss = 0.33317979\n",
      "Iteration 585, loss = 0.33272243\n",
      "Iteration 586, loss = 0.33323502\n",
      "Iteration 587, loss = 0.33230632\n",
      "Iteration 588, loss = 0.33206808\n",
      "Iteration 589, loss = 0.33186179\n",
      "Iteration 590, loss = 0.33127947\n",
      "Iteration 591, loss = 0.33124531\n",
      "Iteration 592, loss = 0.33124875\n",
      "Iteration 593, loss = 0.33072926\n",
      "Iteration 594, loss = 0.33028393\n",
      "Iteration 595, loss = 0.32997118\n",
      "Iteration 596, loss = 0.33000005\n",
      "Iteration 597, loss = 0.32914720\n",
      "Iteration 598, loss = 0.32910243\n",
      "Iteration 599, loss = 0.32881471\n",
      "Iteration 600, loss = 0.32891607\n",
      "Iteration 601, loss = 0.32844616\n",
      "Iteration 602, loss = 0.32770938\n",
      "Iteration 603, loss = 0.32754559\n",
      "Iteration 604, loss = 0.32726633\n",
      "Iteration 605, loss = 0.32699529\n",
      "Iteration 606, loss = 0.32658052\n",
      "Iteration 607, loss = 0.32641238\n",
      "Iteration 608, loss = 0.32594228\n",
      "Iteration 609, loss = 0.32640788\n",
      "Iteration 610, loss = 0.32571691\n",
      "Iteration 611, loss = 0.32515103\n",
      "Iteration 612, loss = 0.32533830\n",
      "Iteration 613, loss = 0.32422416\n",
      "Iteration 614, loss = 0.32457419\n",
      "Iteration 615, loss = 0.32406066\n",
      "Iteration 616, loss = 0.32370939\n",
      "Iteration 617, loss = 0.32316817\n",
      "Iteration 618, loss = 0.32307330\n",
      "Iteration 619, loss = 0.32267855\n",
      "Iteration 620, loss = 0.32249165\n",
      "Iteration 621, loss = 0.32212943\n",
      "Iteration 622, loss = 0.32177387\n",
      "Iteration 623, loss = 0.32149207\n",
      "Iteration 624, loss = 0.32135548\n",
      "Iteration 625, loss = 0.32092593\n",
      "Iteration 626, loss = 0.32052287\n",
      "Iteration 627, loss = 0.32089317\n",
      "Iteration 628, loss = 0.31986434\n",
      "Iteration 629, loss = 0.31982386\n",
      "Iteration 630, loss = 0.31949857\n",
      "Iteration 631, loss = 0.31962882\n",
      "Iteration 632, loss = 0.31874367\n",
      "Iteration 633, loss = 0.31867707\n",
      "Iteration 634, loss = 0.31828004\n",
      "Iteration 635, loss = 0.31802947\n",
      "Iteration 636, loss = 0.31820560\n",
      "Iteration 637, loss = 0.31750485\n",
      "Iteration 638, loss = 0.31713612\n",
      "Iteration 639, loss = 0.31655298\n",
      "Iteration 640, loss = 0.31633784\n",
      "Iteration 641, loss = 0.31614834\n",
      "Iteration 642, loss = 0.31622547\n",
      "Iteration 643, loss = 0.31634855\n",
      "Iteration 644, loss = 0.31599469\n",
      "Iteration 645, loss = 0.31524827\n",
      "Iteration 646, loss = 0.31545622\n",
      "Iteration 647, loss = 0.31416999\n",
      "Iteration 648, loss = 0.31441060\n",
      "Iteration 649, loss = 0.31401396\n",
      "Iteration 650, loss = 0.31394363\n",
      "Iteration 651, loss = 0.31358150\n",
      "Iteration 652, loss = 0.31376072\n",
      "Iteration 653, loss = 0.31253559\n",
      "Iteration 654, loss = 0.31286207\n",
      "Iteration 655, loss = 0.31228206\n",
      "Iteration 656, loss = 0.31242087\n",
      "Iteration 657, loss = 0.31166666\n",
      "Iteration 658, loss = 0.31154065\n",
      "Iteration 659, loss = 0.31130612\n",
      "Iteration 660, loss = 0.31058741\n",
      "Iteration 661, loss = 0.31114460\n",
      "Iteration 662, loss = 0.31015102\n",
      "Iteration 663, loss = 0.31023523\n",
      "Iteration 664, loss = 0.30965244\n",
      "Iteration 665, loss = 0.30962934\n",
      "Iteration 666, loss = 0.30908015\n",
      "Iteration 667, loss = 0.30935842\n",
      "Iteration 668, loss = 0.30881915\n",
      "Iteration 669, loss = 0.30912830\n",
      "Iteration 670, loss = 0.30826513\n",
      "Iteration 671, loss = 0.30781777\n",
      "Iteration 672, loss = 0.30777028\n",
      "Iteration 673, loss = 0.30688942\n",
      "Iteration 674, loss = 0.30753127\n",
      "Iteration 675, loss = 0.30681017\n",
      "Iteration 676, loss = 0.30649415\n",
      "Iteration 677, loss = 0.30660129\n",
      "Iteration 678, loss = 0.30604687\n",
      "Iteration 679, loss = 0.30581228\n",
      "Iteration 680, loss = 0.30513874\n",
      "Iteration 681, loss = 0.30581661\n",
      "Iteration 682, loss = 0.30481168\n",
      "Iteration 683, loss = 0.30507887\n",
      "Iteration 684, loss = 0.30439329\n",
      "Iteration 685, loss = 0.30397542\n",
      "Iteration 686, loss = 0.30426768\n",
      "Iteration 687, loss = 0.30426408\n",
      "Iteration 688, loss = 0.30375653\n",
      "Iteration 689, loss = 0.30320289\n",
      "Iteration 690, loss = 0.30307420\n",
      "Iteration 691, loss = 0.30269376\n",
      "Iteration 692, loss = 0.30250886\n",
      "Iteration 693, loss = 0.30233347\n",
      "Iteration 694, loss = 0.30162972\n",
      "Iteration 695, loss = 0.30141827\n",
      "Iteration 696, loss = 0.30120252\n",
      "Iteration 697, loss = 0.30107259\n",
      "Iteration 698, loss = 0.30076161\n",
      "Iteration 699, loss = 0.30102828\n",
      "Iteration 700, loss = 0.30036326\n",
      "Iteration 701, loss = 0.30002098\n",
      "Iteration 702, loss = 0.29986199\n",
      "Iteration 703, loss = 0.30027047\n",
      "Iteration 704, loss = 0.29909834\n",
      "Iteration 705, loss = 0.29891806\n",
      "Iteration 706, loss = 0.29887348\n",
      "Iteration 707, loss = 0.29812837\n",
      "Iteration 708, loss = 0.29824241\n",
      "Iteration 709, loss = 0.29834638\n",
      "Iteration 710, loss = 0.29848735\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69681869\n",
      "Iteration 2, loss = 0.68897145\n",
      "Iteration 3, loss = 0.68163262\n",
      "Iteration 4, loss = 0.67499980\n",
      "Iteration 5, loss = 0.67029895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.66506709\n",
      "Iteration 7, loss = 0.66074214\n",
      "Iteration 8, loss = 0.65646344\n",
      "Iteration 9, loss = 0.65239093\n",
      "Iteration 10, loss = 0.64924551\n",
      "Iteration 11, loss = 0.64586161\n",
      "Iteration 12, loss = 0.64255034\n",
      "Iteration 13, loss = 0.63986740\n",
      "Iteration 14, loss = 0.63724827\n",
      "Iteration 15, loss = 0.63558937\n",
      "Iteration 16, loss = 0.63379673\n",
      "Iteration 17, loss = 0.63219058\n",
      "Iteration 18, loss = 0.63027602\n",
      "Iteration 19, loss = 0.62837097\n",
      "Iteration 20, loss = 0.62695425\n",
      "Iteration 21, loss = 0.62537546\n",
      "Iteration 22, loss = 0.62406410\n",
      "Iteration 23, loss = 0.62230731\n",
      "Iteration 24, loss = 0.62136869\n",
      "Iteration 25, loss = 0.62031594\n",
      "Iteration 26, loss = 0.61889142\n",
      "Iteration 27, loss = 0.61810809\n",
      "Iteration 28, loss = 0.61711853\n",
      "Iteration 29, loss = 0.61604037\n",
      "Iteration 30, loss = 0.61515862\n",
      "Iteration 31, loss = 0.61451556\n",
      "Iteration 32, loss = 0.61333636\n",
      "Iteration 33, loss = 0.61231678\n",
      "Iteration 34, loss = 0.61121978\n",
      "Iteration 35, loss = 0.61025757\n",
      "Iteration 36, loss = 0.60943563\n",
      "Iteration 37, loss = 0.60861105\n",
      "Iteration 38, loss = 0.60830126\n",
      "Iteration 39, loss = 0.60665359\n",
      "Iteration 40, loss = 0.60582986\n",
      "Iteration 41, loss = 0.60571294\n",
      "Iteration 42, loss = 0.60414568\n",
      "Iteration 43, loss = 0.60347376\n",
      "Iteration 44, loss = 0.60248127\n",
      "Iteration 45, loss = 0.60175122\n",
      "Iteration 46, loss = 0.60107537\n",
      "Iteration 47, loss = 0.60033010\n",
      "Iteration 48, loss = 0.59996348\n",
      "Iteration 49, loss = 0.59884040\n",
      "Iteration 50, loss = 0.59794331\n",
      "Iteration 51, loss = 0.59746105\n",
      "Iteration 52, loss = 0.59649064\n",
      "Iteration 53, loss = 0.59553024\n",
      "Iteration 54, loss = 0.59449051\n",
      "Iteration 55, loss = 0.59363026\n",
      "Iteration 56, loss = 0.59318454\n",
      "Iteration 57, loss = 0.59198034\n",
      "Iteration 58, loss = 0.59127265\n",
      "Iteration 59, loss = 0.59047778\n",
      "Iteration 60, loss = 0.59003764\n",
      "Iteration 61, loss = 0.58977450\n",
      "Iteration 62, loss = 0.58863085\n",
      "Iteration 63, loss = 0.58761593\n",
      "Iteration 64, loss = 0.58834791\n",
      "Iteration 65, loss = 0.58657384\n",
      "Iteration 66, loss = 0.58554441\n",
      "Iteration 67, loss = 0.58467734\n",
      "Iteration 68, loss = 0.58391833\n",
      "Iteration 69, loss = 0.58299301\n",
      "Iteration 70, loss = 0.58254300\n",
      "Iteration 71, loss = 0.58202412\n",
      "Iteration 72, loss = 0.58070935\n",
      "Iteration 73, loss = 0.58054576\n",
      "Iteration 74, loss = 0.57946678\n",
      "Iteration 75, loss = 0.57867845\n",
      "Iteration 76, loss = 0.57830959\n",
      "Iteration 77, loss = 0.57705878\n",
      "Iteration 78, loss = 0.57627780\n",
      "Iteration 79, loss = 0.57566164\n",
      "Iteration 80, loss = 0.57485987\n",
      "Iteration 81, loss = 0.57481091\n",
      "Iteration 82, loss = 0.57379273\n",
      "Iteration 83, loss = 0.57289211\n",
      "Iteration 84, loss = 0.57225311\n",
      "Iteration 85, loss = 0.57139255\n",
      "Iteration 86, loss = 0.57063625\n",
      "Iteration 87, loss = 0.57010083\n",
      "Iteration 88, loss = 0.56922298\n",
      "Iteration 89, loss = 0.56848132\n",
      "Iteration 90, loss = 0.56795423\n",
      "Iteration 91, loss = 0.56717434\n",
      "Iteration 92, loss = 0.56662419\n",
      "Iteration 93, loss = 0.56670268\n",
      "Iteration 94, loss = 0.56587159\n",
      "Iteration 95, loss = 0.56574264\n",
      "Iteration 96, loss = 0.56553182\n",
      "Iteration 97, loss = 0.56450283\n",
      "Iteration 98, loss = 0.56382239\n",
      "Iteration 99, loss = 0.56276305\n",
      "Iteration 100, loss = 0.56251285\n",
      "Iteration 101, loss = 0.56163428\n",
      "Iteration 102, loss = 0.56088602\n",
      "Iteration 103, loss = 0.56002410\n",
      "Iteration 104, loss = 0.55997002\n",
      "Iteration 105, loss = 0.55912860\n",
      "Iteration 106, loss = 0.55922339\n",
      "Iteration 107, loss = 0.55828933\n",
      "Iteration 108, loss = 0.55722538\n",
      "Iteration 109, loss = 0.55674952\n",
      "Iteration 110, loss = 0.55665891\n",
      "Iteration 111, loss = 0.55623121\n",
      "Iteration 112, loss = 0.55522637\n",
      "Iteration 113, loss = 0.55557912\n",
      "Iteration 114, loss = 0.55441064\n",
      "Iteration 115, loss = 0.55364508\n",
      "Iteration 116, loss = 0.55287759\n",
      "Iteration 117, loss = 0.55234560\n",
      "Iteration 118, loss = 0.55220648\n",
      "Iteration 119, loss = 0.55136330\n",
      "Iteration 120, loss = 0.55070252\n",
      "Iteration 121, loss = 0.55074888\n",
      "Iteration 122, loss = 0.54941492\n",
      "Iteration 123, loss = 0.54941282\n",
      "Iteration 124, loss = 0.54886248\n",
      "Iteration 125, loss = 0.54860737\n",
      "Iteration 126, loss = 0.54762329\n",
      "Iteration 127, loss = 0.54678845\n",
      "Iteration 128, loss = 0.54601508\n",
      "Iteration 129, loss = 0.54522971\n",
      "Iteration 130, loss = 0.54687572\n",
      "Iteration 131, loss = 0.54524668\n",
      "Iteration 132, loss = 0.54441749\n",
      "Iteration 133, loss = 0.54401486\n",
      "Iteration 134, loss = 0.54360418\n",
      "Iteration 135, loss = 0.54229149\n",
      "Iteration 136, loss = 0.54217221\n",
      "Iteration 137, loss = 0.54124857\n",
      "Iteration 138, loss = 0.54283087\n",
      "Iteration 139, loss = 0.54097202\n",
      "Iteration 140, loss = 0.54076643\n",
      "Iteration 141, loss = 0.54057247\n",
      "Iteration 142, loss = 0.54176854\n",
      "Iteration 143, loss = 0.53943206\n",
      "Iteration 144, loss = 0.53815112\n",
      "Iteration 145, loss = 0.53761384\n",
      "Iteration 146, loss = 0.53736733\n",
      "Iteration 147, loss = 0.53675913\n",
      "Iteration 148, loss = 0.53653506\n",
      "Iteration 149, loss = 0.53542062\n",
      "Iteration 150, loss = 0.53555255\n",
      "Iteration 151, loss = 0.53498072\n",
      "Iteration 152, loss = 0.53386799\n",
      "Iteration 153, loss = 0.53402965\n",
      "Iteration 154, loss = 0.53285576\n",
      "Iteration 155, loss = 0.53282878\n",
      "Iteration 156, loss = 0.53183795\n",
      "Iteration 157, loss = 0.53123767\n",
      "Iteration 158, loss = 0.53068890\n",
      "Iteration 159, loss = 0.53093948\n",
      "Iteration 160, loss = 0.52970130\n",
      "Iteration 161, loss = 0.52901985\n",
      "Iteration 162, loss = 0.52894729\n",
      "Iteration 163, loss = 0.52894820\n",
      "Iteration 164, loss = 0.52847510\n",
      "Iteration 165, loss = 0.52744527\n",
      "Iteration 166, loss = 0.52673046\n",
      "Iteration 167, loss = 0.52668195\n",
      "Iteration 168, loss = 0.52642220\n",
      "Iteration 169, loss = 0.52820170\n",
      "Iteration 170, loss = 0.52597596\n",
      "Iteration 171, loss = 0.52628714\n",
      "Iteration 172, loss = 0.52427739\n",
      "Iteration 173, loss = 0.52358031\n",
      "Iteration 174, loss = 0.52287959\n",
      "Iteration 175, loss = 0.52270168\n",
      "Iteration 176, loss = 0.52194160\n",
      "Iteration 177, loss = 0.52152242\n",
      "Iteration 178, loss = 0.52081374\n",
      "Iteration 179, loss = 0.52061725\n",
      "Iteration 180, loss = 0.52257343\n",
      "Iteration 181, loss = 0.52014661\n",
      "Iteration 182, loss = 0.51906372\n",
      "Iteration 183, loss = 0.51831193\n",
      "Iteration 184, loss = 0.51815638\n",
      "Iteration 185, loss = 0.51791950\n",
      "Iteration 186, loss = 0.51716065\n",
      "Iteration 187, loss = 0.51755777\n",
      "Iteration 188, loss = 0.51631823\n",
      "Iteration 189, loss = 0.51523500\n",
      "Iteration 190, loss = 0.51497446\n",
      "Iteration 191, loss = 0.51491860\n",
      "Iteration 192, loss = 0.51411847\n",
      "Iteration 193, loss = 0.51422692\n",
      "Iteration 194, loss = 0.51358152\n",
      "Iteration 195, loss = 0.51225168\n",
      "Iteration 196, loss = 0.51193205\n",
      "Iteration 197, loss = 0.51224089\n",
      "Iteration 198, loss = 0.51385009\n",
      "Iteration 199, loss = 0.51183097\n",
      "Iteration 200, loss = 0.51036359\n",
      "Iteration 201, loss = 0.50973926\n",
      "Iteration 202, loss = 0.50894992\n",
      "Iteration 203, loss = 0.50860697\n",
      "Iteration 204, loss = 0.51429121\n",
      "Iteration 205, loss = 0.50878579\n",
      "Iteration 206, loss = 0.50791324\n",
      "Iteration 207, loss = 0.50619422\n",
      "Iteration 208, loss = 0.50602592\n",
      "Iteration 209, loss = 0.50502848\n",
      "Iteration 210, loss = 0.50541716\n",
      "Iteration 211, loss = 0.50394784\n",
      "Iteration 212, loss = 0.50376593\n",
      "Iteration 213, loss = 0.50307686\n",
      "Iteration 214, loss = 0.50226934\n",
      "Iteration 215, loss = 0.50271835\n",
      "Iteration 216, loss = 0.50199495\n",
      "Iteration 217, loss = 0.50089577\n",
      "Iteration 218, loss = 0.50046151\n",
      "Iteration 219, loss = 0.50200550\n",
      "Iteration 220, loss = 0.50011470\n",
      "Iteration 221, loss = 0.49939845\n",
      "Iteration 222, loss = 0.49906324\n",
      "Iteration 223, loss = 0.49792147\n",
      "Iteration 224, loss = 0.49773303\n",
      "Iteration 225, loss = 0.49816819\n",
      "Iteration 226, loss = 0.49749084\n",
      "Iteration 227, loss = 0.49890229\n",
      "Iteration 228, loss = 0.49621210\n",
      "Iteration 229, loss = 0.49557733\n",
      "Iteration 230, loss = 0.49464992\n",
      "Iteration 231, loss = 0.49378633\n",
      "Iteration 232, loss = 0.49643666\n",
      "Iteration 233, loss = 0.49366665\n",
      "Iteration 234, loss = 0.49372975\n",
      "Iteration 235, loss = 0.49799723\n",
      "Iteration 236, loss = 0.49744776\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[0.82358821 0.8385     0.8275     0.8375     0.76788394]\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn MLP training \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# preprocessing data \n",
    "\n",
    "X_scale = preprocessing.scale(datasets)\n",
    "# print(X_scale)\n",
    "\n",
    "# sgd optimizer \n",
    "mlp = MLPClassifier(solver='sgd', activation='relu',alpha=1e-4, hidden_layer_sizes=(128, 128),\n",
    "                    random_state=1, max_iter=1000,verbose=True)\n",
    "\n",
    "# lbfgs - very slow \n",
    "# mlp = MLPClassifier(solver='lbfgs', activation='relu',alpha=1e-4,hidden_layer_sizes=(100,100),\n",
    "#                     random_state=1,max_iter=50,verbose=10,learning_rate_init=.1)\n",
    "\n",
    "# adam \n",
    "# mlp = MLPClassifier(solver='adam', activation='relu',alpha=1e-4,hidden_layer_sizes=(100,100),\n",
    "#                     random_state=1,max_iter=50,verbose=10,learning_rate_init=.1)\n",
    "\n",
    "# mlp.fit(X_scale, output_scikit) \n",
    "# print(mlp.classes_)\n",
    "# print(mlp.n_layers_)\n",
    "# print(mlp.n_iter_)\n",
    "# print(mlp.loss_)\n",
    "# print(mlp.out_activation_)\n",
    "# print(mlp.n_outputs_)\n",
    "\n",
    "score_mlp = cross_val_score(mlp, X_scale, output_scikit,cv=5)  \n",
    "print(score_mlp)\n",
    "\n",
    "# 100000 data ~ 93%\n",
    "# 10000 data - 85%\n",
    "# 100000 data using 256, 256 - 91%\n",
    "# 500000 data (128,128) - 96%   0.96623034 0.96747    0.96845    0.96848    0.96720967\n",
    "# 50000 (128,128) [0.89191081 0.8818     0.8794     0.8831     0.86838684]\n",
    "# 10000[0.82358821 0.8385     0.8275     0.8375     0.76788394]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82358821 0.8385     0.8275     0.8375     0.76788394]\n"
     ]
    }
   ],
   "source": [
    "print(score_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.49885042\n",
      "Iteration 2, loss = 0.46490056\n",
      "Iteration 3, loss = 0.44266073\n",
      "Iteration 4, loss = 0.42757042\n",
      "Iteration 5, loss = 0.41712887\n",
      "Iteration 6, loss = 0.40949324\n",
      "Iteration 7, loss = 0.40346630\n",
      "Iteration 8, loss = 0.39843189\n",
      "Iteration 9, loss = 0.39413972\n",
      "Iteration 10, loss = 0.39024454\n",
      "Iteration 11, loss = 0.38660906\n",
      "Iteration 12, loss = 0.38314647\n",
      "Iteration 13, loss = 0.37989620\n",
      "Iteration 14, loss = 0.37675355\n",
      "Iteration 15, loss = 0.37372386\n",
      "Iteration 16, loss = 0.37086982\n",
      "Iteration 17, loss = 0.36812615\n",
      "Iteration 18, loss = 0.36525163\n",
      "Iteration 19, loss = 0.36254164\n",
      "Iteration 20, loss = 0.36007618\n",
      "Iteration 21, loss = 0.35748215\n",
      "Iteration 22, loss = 0.35513170\n",
      "Iteration 23, loss = 0.35312384\n",
      "Iteration 24, loss = 0.35058065\n",
      "Iteration 25, loss = 0.34837448\n",
      "Iteration 26, loss = 0.34640089\n",
      "Iteration 27, loss = 0.34419876\n",
      "Iteration 28, loss = 0.34224586\n",
      "Iteration 29, loss = 0.34035850\n",
      "Iteration 30, loss = 0.33859535\n",
      "Iteration 31, loss = 0.33664119\n",
      "Iteration 32, loss = 0.33508599\n",
      "Iteration 33, loss = 0.33335070\n",
      "Iteration 34, loss = 0.33162573\n",
      "Iteration 35, loss = 0.33007857\n",
      "Iteration 36, loss = 0.32855159\n",
      "Iteration 37, loss = 0.32713561\n",
      "Iteration 38, loss = 0.32585313\n",
      "Iteration 39, loss = 0.32435224\n",
      "Iteration 40, loss = 0.32300683\n",
      "Iteration 41, loss = 0.32165939\n",
      "Iteration 42, loss = 0.32041200\n",
      "Iteration 43, loss = 0.31918646\n",
      "Iteration 44, loss = 0.31803849\n",
      "Iteration 45, loss = 0.31687176\n",
      "Iteration 46, loss = 0.31571399\n",
      "Iteration 47, loss = 0.31454637\n",
      "Iteration 48, loss = 0.31355167\n",
      "Iteration 49, loss = 0.31235593\n",
      "Iteration 50, loss = 0.31152776\n",
      "Iteration 51, loss = 0.31035317\n",
      "Iteration 52, loss = 0.30939908\n",
      "Iteration 53, loss = 0.30852058\n",
      "Iteration 54, loss = 0.30738314\n",
      "Iteration 55, loss = 0.30664416\n",
      "Iteration 56, loss = 0.30544297\n",
      "Iteration 57, loss = 0.30446891\n",
      "Iteration 58, loss = 0.30350524\n",
      "Iteration 59, loss = 0.30266545\n",
      "Iteration 60, loss = 0.30166454\n",
      "Iteration 61, loss = 0.30079130\n",
      "Iteration 62, loss = 0.29978665\n",
      "Iteration 63, loss = 0.29894928\n",
      "Iteration 64, loss = 0.29806970\n",
      "Iteration 65, loss = 0.29711286\n",
      "Iteration 66, loss = 0.29613665\n",
      "Iteration 67, loss = 0.29542958\n",
      "Iteration 68, loss = 0.29444184\n",
      "Iteration 69, loss = 0.29357299\n",
      "Iteration 70, loss = 0.29277350\n",
      "Iteration 71, loss = 0.29184308\n",
      "Iteration 72, loss = 0.29082861\n",
      "Iteration 73, loss = 0.29025164\n",
      "Iteration 74, loss = 0.28908519\n",
      "Iteration 75, loss = 0.28821296\n",
      "Iteration 76, loss = 0.28723250\n",
      "Iteration 77, loss = 0.28661782\n",
      "Iteration 78, loss = 0.28559891\n",
      "Iteration 79, loss = 0.28479144\n",
      "Iteration 80, loss = 0.28382400\n",
      "Iteration 81, loss = 0.28293780\n",
      "Iteration 82, loss = 0.28197756\n",
      "Iteration 83, loss = 0.28110821\n",
      "Iteration 84, loss = 0.28024005\n",
      "Iteration 85, loss = 0.27930359\n",
      "Iteration 86, loss = 0.27844135\n",
      "Iteration 87, loss = 0.27759658\n",
      "Iteration 88, loss = 0.27656031\n",
      "Iteration 89, loss = 0.27587877\n",
      "Iteration 90, loss = 0.27488131\n",
      "Iteration 91, loss = 0.27407667\n",
      "Iteration 92, loss = 0.27318962\n",
      "Iteration 93, loss = 0.27230858\n",
      "Iteration 94, loss = 0.27123362\n",
      "Iteration 95, loss = 0.27037731\n",
      "Iteration 96, loss = 0.26946373\n",
      "Iteration 97, loss = 0.26855457\n",
      "Iteration 98, loss = 0.26768181\n",
      "Iteration 99, loss = 0.26679764\n",
      "Iteration 100, loss = 0.26587572\n",
      "Iteration 101, loss = 0.26493492\n",
      "Iteration 102, loss = 0.26406159\n",
      "Iteration 103, loss = 0.26319776\n",
      "Iteration 104, loss = 0.26218191\n",
      "Iteration 105, loss = 0.26131324\n",
      "Iteration 106, loss = 0.26045287\n",
      "Iteration 107, loss = 0.25950784\n",
      "Iteration 108, loss = 0.25872608\n",
      "Iteration 109, loss = 0.25772665\n",
      "Iteration 110, loss = 0.25683196\n",
      "Iteration 111, loss = 0.25612290\n",
      "Iteration 112, loss = 0.25513035\n",
      "Iteration 113, loss = 0.25438459\n",
      "Iteration 114, loss = 0.25343594\n",
      "Iteration 115, loss = 0.25251445\n",
      "Iteration 116, loss = 0.25164465\n",
      "Iteration 117, loss = 0.25068101\n",
      "Iteration 118, loss = 0.24986651\n",
      "Iteration 119, loss = 0.24890816\n",
      "Iteration 120, loss = 0.24814540\n",
      "Iteration 121, loss = 0.24720766\n",
      "Iteration 122, loss = 0.24647715\n",
      "Iteration 123, loss = 0.24551312\n",
      "Iteration 124, loss = 0.24463891\n",
      "Iteration 125, loss = 0.24386158\n",
      "Iteration 126, loss = 0.24303745\n",
      "Iteration 127, loss = 0.24210090\n",
      "Iteration 128, loss = 0.24128446\n",
      "Iteration 129, loss = 0.24042081\n",
      "Iteration 130, loss = 0.23969020\n",
      "Iteration 131, loss = 0.23877036\n",
      "Iteration 132, loss = 0.23792236\n",
      "Iteration 133, loss = 0.23709729\n",
      "Iteration 134, loss = 0.23659661\n",
      "Iteration 135, loss = 0.23550621\n",
      "Iteration 136, loss = 0.23480329\n",
      "Iteration 137, loss = 0.23403130\n",
      "Iteration 138, loss = 0.23312106\n",
      "Iteration 139, loss = 0.23250586\n",
      "Iteration 140, loss = 0.23168319\n",
      "Iteration 141, loss = 0.23073621\n",
      "Iteration 142, loss = 0.23004135\n",
      "Iteration 143, loss = 0.22932016\n",
      "Iteration 144, loss = 0.22841081\n",
      "Iteration 145, loss = 0.22767313\n",
      "Iteration 146, loss = 0.22696246\n",
      "Iteration 147, loss = 0.22621649\n",
      "Iteration 148, loss = 0.22536311\n",
      "Iteration 149, loss = 0.22486117\n",
      "Iteration 150, loss = 0.22384516\n",
      "Iteration 151, loss = 0.22295788\n",
      "Iteration 152, loss = 0.22253213\n",
      "Iteration 153, loss = 0.22159556\n",
      "Iteration 154, loss = 0.22077676\n",
      "Iteration 155, loss = 0.22024593\n",
      "Iteration 156, loss = 0.21931838\n",
      "Iteration 157, loss = 0.21869390\n",
      "Iteration 158, loss = 0.21804920\n",
      "Iteration 159, loss = 0.21725292\n",
      "Iteration 160, loss = 0.21654432\n",
      "Iteration 161, loss = 0.21592015\n",
      "Iteration 162, loss = 0.21511983\n",
      "Iteration 163, loss = 0.21422887\n",
      "Iteration 164, loss = 0.21370652\n",
      "Iteration 165, loss = 0.21300506\n",
      "Iteration 166, loss = 0.21235216\n",
      "Iteration 167, loss = 0.21142762\n",
      "Iteration 168, loss = 0.21082586\n",
      "Iteration 169, loss = 0.21010790\n",
      "Iteration 170, loss = 0.20962309\n",
      "Iteration 171, loss = 0.20889285\n",
      "Iteration 172, loss = 0.20809796\n",
      "Iteration 173, loss = 0.20748845\n",
      "Iteration 174, loss = 0.20690928\n",
      "Iteration 175, loss = 0.20618227\n",
      "Iteration 176, loss = 0.20563254\n",
      "Iteration 177, loss = 0.20487882\n",
      "Iteration 178, loss = 0.20414292\n",
      "Iteration 179, loss = 0.20365984\n",
      "Iteration 180, loss = 0.20286009\n",
      "Iteration 181, loss = 0.20242101\n",
      "Iteration 182, loss = 0.20170997\n",
      "Iteration 183, loss = 0.20090109\n",
      "Iteration 184, loss = 0.20034575\n",
      "Iteration 185, loss = 0.19974499\n",
      "Iteration 186, loss = 0.19914707\n",
      "Iteration 187, loss = 0.19845661\n",
      "Iteration 188, loss = 0.19776874\n",
      "Iteration 189, loss = 0.19741137\n",
      "Iteration 190, loss = 0.19674397\n",
      "Iteration 191, loss = 0.19608117\n",
      "Iteration 192, loss = 0.19565096\n",
      "Iteration 193, loss = 0.19480845\n",
      "Iteration 194, loss = 0.19432785\n",
      "Iteration 195, loss = 0.19368689\n",
      "Iteration 196, loss = 0.19326398\n",
      "Iteration 197, loss = 0.19255224\n",
      "Iteration 198, loss = 0.19192632\n",
      "Iteration 199, loss = 0.19134530\n",
      "Iteration 200, loss = 0.19077972\n",
      "Iteration 201, loss = 0.19030988\n",
      "Iteration 202, loss = 0.18945960\n",
      "Iteration 203, loss = 0.18913858\n",
      "Iteration 204, loss = 0.18850856\n",
      "Iteration 205, loss = 0.18800456\n",
      "Iteration 206, loss = 0.18758294\n",
      "Iteration 207, loss = 0.18681863\n",
      "Iteration 208, loss = 0.18650401\n",
      "Iteration 209, loss = 0.18592453\n",
      "Iteration 210, loss = 0.18515235\n",
      "Iteration 211, loss = 0.18487025\n",
      "Iteration 212, loss = 0.18402302\n",
      "Iteration 213, loss = 0.18352311\n",
      "Iteration 214, loss = 0.18292133\n",
      "Iteration 215, loss = 0.18233269\n",
      "Iteration 216, loss = 0.18198074\n",
      "Iteration 217, loss = 0.18140783\n",
      "Iteration 218, loss = 0.18101804\n",
      "Iteration 219, loss = 0.18030437\n",
      "Iteration 220, loss = 0.17999678\n",
      "Iteration 221, loss = 0.17932347\n",
      "Iteration 222, loss = 0.17866778\n",
      "Iteration 223, loss = 0.17840289\n",
      "Iteration 224, loss = 0.17759511\n",
      "Iteration 225, loss = 0.17729003\n",
      "Iteration 226, loss = 0.17651558\n",
      "Iteration 227, loss = 0.17617602\n",
      "Iteration 228, loss = 0.17565121\n",
      "Iteration 229, loss = 0.17509843\n",
      "Iteration 230, loss = 0.17469235\n",
      "Iteration 231, loss = 0.17406430\n",
      "Iteration 232, loss = 0.17355187\n",
      "Iteration 233, loss = 0.17317802\n",
      "Iteration 234, loss = 0.17270348\n",
      "Iteration 235, loss = 0.17202613\n",
      "Iteration 236, loss = 0.17197983\n",
      "Iteration 237, loss = 0.17119949\n",
      "Iteration 238, loss = 0.17092480\n",
      "Iteration 239, loss = 0.17041765\n",
      "Iteration 240, loss = 0.16988658\n",
      "Iteration 241, loss = 0.16947510\n",
      "Iteration 242, loss = 0.16896685\n",
      "Iteration 243, loss = 0.16839818\n",
      "Iteration 244, loss = 0.16809174\n",
      "Iteration 245, loss = 0.16741801\n",
      "Iteration 246, loss = 0.16713682\n",
      "Iteration 247, loss = 0.16674280\n",
      "Iteration 248, loss = 0.16634488\n",
      "Iteration 249, loss = 0.16582164\n",
      "Iteration 250, loss = 0.16533832\n",
      "Iteration 251, loss = 0.16501629\n",
      "Iteration 252, loss = 0.16446198\n",
      "Iteration 253, loss = 0.16398299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.16358071\n",
      "Iteration 255, loss = 0.16328588\n",
      "Iteration 256, loss = 0.16281157\n",
      "Iteration 257, loss = 0.16259166\n",
      "Iteration 258, loss = 0.16191650\n",
      "Iteration 259, loss = 0.16161546\n",
      "Iteration 260, loss = 0.16112561\n",
      "Iteration 261, loss = 0.16059631\n",
      "Iteration 262, loss = 0.16036556\n",
      "Iteration 263, loss = 0.15994603\n",
      "Iteration 264, loss = 0.15958787\n",
      "Iteration 265, loss = 0.15923456\n",
      "Iteration 266, loss = 0.15857806\n",
      "Iteration 267, loss = 0.15821155\n",
      "Iteration 268, loss = 0.15807956\n",
      "Iteration 269, loss = 0.15768585\n",
      "Iteration 270, loss = 0.15696123\n",
      "Iteration 271, loss = 0.15677103\n",
      "Iteration 272, loss = 0.15644811\n",
      "Iteration 273, loss = 0.15584270\n",
      "Iteration 274, loss = 0.15582960\n",
      "Iteration 275, loss = 0.15520261\n",
      "Iteration 276, loss = 0.15480750\n",
      "Iteration 277, loss = 0.15440831\n",
      "Iteration 278, loss = 0.15402954\n",
      "Iteration 279, loss = 0.15379567\n",
      "Iteration 280, loss = 0.15341437\n",
      "Iteration 281, loss = 0.15286690\n",
      "Iteration 282, loss = 0.15260247\n",
      "Iteration 283, loss = 0.15211120\n",
      "Iteration 284, loss = 0.15205082\n",
      "Iteration 285, loss = 0.15147033\n",
      "Iteration 286, loss = 0.15126211\n",
      "Iteration 287, loss = 0.15090924\n",
      "Iteration 288, loss = 0.15023042\n",
      "Iteration 289, loss = 0.15019814\n",
      "Iteration 290, loss = 0.14962902\n",
      "Iteration 291, loss = 0.14930080\n",
      "Iteration 292, loss = 0.14903159\n",
      "Iteration 293, loss = 0.14862079\n",
      "Iteration 294, loss = 0.14840933\n",
      "Iteration 295, loss = 0.14803473\n",
      "Iteration 296, loss = 0.14777208\n",
      "Iteration 297, loss = 0.14704464\n",
      "Iteration 298, loss = 0.14695696\n",
      "Iteration 299, loss = 0.14665650\n",
      "Iteration 300, loss = 0.14642742\n",
      "Iteration 301, loss = 0.14586367\n",
      "Iteration 302, loss = 0.14566745\n",
      "Iteration 303, loss = 0.14542526\n",
      "Iteration 304, loss = 0.14480426\n",
      "Iteration 305, loss = 0.14473495\n",
      "Iteration 306, loss = 0.14449861\n",
      "Iteration 307, loss = 0.14393183\n",
      "Iteration 308, loss = 0.14393250\n",
      "Iteration 309, loss = 0.14344038\n",
      "Iteration 310, loss = 0.14293526\n",
      "Iteration 311, loss = 0.14259751\n",
      "Iteration 312, loss = 0.14255180\n",
      "Iteration 313, loss = 0.14213445\n",
      "Iteration 314, loss = 0.14178441\n",
      "Iteration 315, loss = 0.14171034\n",
      "Iteration 316, loss = 0.14134776\n",
      "Iteration 317, loss = 0.14101328\n",
      "Iteration 318, loss = 0.14061158\n",
      "Iteration 319, loss = 0.14019027\n",
      "Iteration 320, loss = 0.14006715\n",
      "Iteration 321, loss = 0.13971158\n",
      "Iteration 322, loss = 0.13933074\n",
      "Iteration 323, loss = 0.13912077\n",
      "Iteration 324, loss = 0.13875157\n",
      "Iteration 325, loss = 0.13879535\n",
      "Iteration 326, loss = 0.13830239\n",
      "Iteration 327, loss = 0.13797952\n",
      "Iteration 328, loss = 0.13805669\n",
      "Iteration 329, loss = 0.13759826\n",
      "Iteration 330, loss = 0.13729987\n",
      "Iteration 331, loss = 0.13721167\n",
      "Iteration 332, loss = 0.13670309\n",
      "Iteration 333, loss = 0.13659471\n",
      "Iteration 334, loss = 0.13626055\n",
      "Iteration 335, loss = 0.13598727\n",
      "Iteration 336, loss = 0.13558282\n",
      "Iteration 337, loss = 0.13564396\n",
      "Iteration 338, loss = 0.13519265\n",
      "Iteration 339, loss = 0.13491365\n",
      "Iteration 340, loss = 0.13478611\n",
      "Iteration 341, loss = 0.13423622\n",
      "Iteration 342, loss = 0.13400583\n",
      "Iteration 343, loss = 0.13418035\n",
      "Iteration 344, loss = 0.13368246\n",
      "Iteration 345, loss = 0.13356498\n",
      "Iteration 346, loss = 0.13312877\n",
      "Iteration 347, loss = 0.13289094\n",
      "Iteration 348, loss = 0.13262190\n",
      "Iteration 349, loss = 0.13252544\n",
      "Iteration 350, loss = 0.13216602\n",
      "Iteration 351, loss = 0.13199701\n",
      "Iteration 352, loss = 0.13170789\n",
      "Iteration 353, loss = 0.13159377\n",
      "Iteration 354, loss = 0.13122792\n",
      "Iteration 355, loss = 0.13095181\n",
      "Iteration 356, loss = 0.13076155\n",
      "Iteration 357, loss = 0.13065826\n",
      "Iteration 358, loss = 0.13032620\n",
      "Iteration 359, loss = 0.13044337\n",
      "Iteration 360, loss = 0.12982957\n",
      "Iteration 361, loss = 0.12955164\n",
      "Iteration 362, loss = 0.12932300\n",
      "Iteration 363, loss = 0.12910716\n",
      "Iteration 364, loss = 0.12900159\n",
      "Iteration 365, loss = 0.12886331\n",
      "Iteration 366, loss = 0.12869762\n",
      "Iteration 367, loss = 0.12849752\n",
      "Iteration 368, loss = 0.12822010\n",
      "Iteration 369, loss = 0.12795619\n",
      "Iteration 370, loss = 0.12774805\n",
      "Iteration 371, loss = 0.12740927\n",
      "Iteration 372, loss = 0.12735264\n",
      "Iteration 373, loss = 0.12691910\n",
      "Iteration 374, loss = 0.12681934\n",
      "Iteration 375, loss = 0.12674586\n",
      "Iteration 376, loss = 0.12650764\n",
      "Iteration 377, loss = 0.12613837\n",
      "Iteration 378, loss = 0.12623596\n",
      "Iteration 379, loss = 0.12573123\n",
      "Iteration 380, loss = 0.12570154\n",
      "Iteration 381, loss = 0.12573064\n",
      "Iteration 382, loss = 0.12522799\n",
      "Iteration 383, loss = 0.12491137\n",
      "Iteration 384, loss = 0.12491471\n",
      "Iteration 385, loss = 0.12455478\n",
      "Iteration 386, loss = 0.12476841\n",
      "Iteration 387, loss = 0.12412945\n",
      "Iteration 388, loss = 0.12415878\n",
      "Iteration 389, loss = 0.12392481\n",
      "Iteration 390, loss = 0.12372900\n",
      "Iteration 391, loss = 0.12370193\n",
      "Iteration 392, loss = 0.12338089\n",
      "Iteration 393, loss = 0.12322544\n",
      "Iteration 394, loss = 0.12301332\n",
      "Iteration 395, loss = 0.12292282\n",
      "Iteration 396, loss = 0.12260378\n",
      "Iteration 397, loss = 0.12230241\n",
      "Iteration 398, loss = 0.12217562\n",
      "Iteration 399, loss = 0.12195870\n",
      "Iteration 400, loss = 0.12192068\n",
      "Iteration 401, loss = 0.12193764\n",
      "Iteration 402, loss = 0.12148012\n",
      "Iteration 403, loss = 0.12136333\n",
      "Iteration 404, loss = 0.12118655\n",
      "Iteration 405, loss = 0.12111776\n",
      "Iteration 406, loss = 0.12099149\n",
      "Iteration 407, loss = 0.12099458\n",
      "Iteration 408, loss = 0.12043087\n",
      "Iteration 409, loss = 0.12042562\n",
      "Iteration 410, loss = 0.12032659\n",
      "Iteration 411, loss = 0.11996181\n",
      "Iteration 412, loss = 0.11981265\n",
      "Iteration 413, loss = 0.11969100\n",
      "Iteration 414, loss = 0.11951823\n",
      "Iteration 415, loss = 0.11945041\n",
      "Iteration 416, loss = 0.11918893\n",
      "Iteration 417, loss = 0.11901429\n",
      "Iteration 418, loss = 0.11887326\n",
      "Iteration 419, loss = 0.11873009\n",
      "Iteration 420, loss = 0.11857964\n",
      "Iteration 421, loss = 0.11844273\n",
      "Iteration 422, loss = 0.11830305\n",
      "Iteration 423, loss = 0.11800752\n",
      "Iteration 424, loss = 0.11797868\n",
      "Iteration 425, loss = 0.11804880\n",
      "Iteration 426, loss = 0.11759962\n",
      "Iteration 427, loss = 0.11737926\n",
      "Iteration 428, loss = 0.11739312\n",
      "Iteration 429, loss = 0.11707240\n",
      "Iteration 430, loss = 0.11702525\n",
      "Iteration 431, loss = 0.11685497\n",
      "Iteration 432, loss = 0.11698237\n",
      "Iteration 433, loss = 0.11676597\n",
      "Iteration 434, loss = 0.11631569\n",
      "Iteration 435, loss = 0.11613187\n",
      "Iteration 436, loss = 0.11628599\n",
      "Iteration 437, loss = 0.11604813\n",
      "Iteration 438, loss = 0.11609220\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50393918\n",
      "Iteration 2, loss = 0.47018204\n",
      "Iteration 3, loss = 0.44815260\n",
      "Iteration 4, loss = 0.43315644\n",
      "Iteration 5, loss = 0.42290123\n",
      "Iteration 6, loss = 0.41526579\n",
      "Iteration 7, loss = 0.40922319\n",
      "Iteration 8, loss = 0.40427057\n",
      "Iteration 9, loss = 0.39987946\n",
      "Iteration 10, loss = 0.39602033\n",
      "Iteration 11, loss = 0.39229537\n",
      "Iteration 12, loss = 0.38875731\n",
      "Iteration 13, loss = 0.38536713\n",
      "Iteration 14, loss = 0.38219605\n",
      "Iteration 15, loss = 0.37906002\n",
      "Iteration 16, loss = 0.37615553\n",
      "Iteration 17, loss = 0.37341575\n",
      "Iteration 18, loss = 0.37054353\n",
      "Iteration 19, loss = 0.36790520\n",
      "Iteration 20, loss = 0.36523721\n",
      "Iteration 21, loss = 0.36266384\n",
      "Iteration 22, loss = 0.36028538\n",
      "Iteration 23, loss = 0.35810188\n",
      "Iteration 24, loss = 0.35564464\n",
      "Iteration 25, loss = 0.35352868\n",
      "Iteration 26, loss = 0.35143625\n",
      "Iteration 27, loss = 0.34926813\n",
      "Iteration 28, loss = 0.34732051\n",
      "Iteration 29, loss = 0.34537094\n",
      "Iteration 30, loss = 0.34350228\n",
      "Iteration 31, loss = 0.34173498\n",
      "Iteration 32, loss = 0.34006433\n",
      "Iteration 33, loss = 0.33834598\n",
      "Iteration 34, loss = 0.33667046\n",
      "Iteration 35, loss = 0.33495330\n",
      "Iteration 36, loss = 0.33336450\n",
      "Iteration 37, loss = 0.33183711\n",
      "Iteration 38, loss = 0.33053171\n",
      "Iteration 39, loss = 0.32895655\n",
      "Iteration 40, loss = 0.32763462\n",
      "Iteration 41, loss = 0.32625280\n",
      "Iteration 42, loss = 0.32509483\n",
      "Iteration 43, loss = 0.32368457\n",
      "Iteration 44, loss = 0.32248144\n",
      "Iteration 45, loss = 0.32122031\n",
      "Iteration 46, loss = 0.31997080\n",
      "Iteration 47, loss = 0.31870074\n",
      "Iteration 48, loss = 0.31763670\n",
      "Iteration 49, loss = 0.31639683\n",
      "Iteration 50, loss = 0.31544533\n",
      "Iteration 51, loss = 0.31418808\n",
      "Iteration 52, loss = 0.31326450\n",
      "Iteration 53, loss = 0.31220289\n",
      "Iteration 54, loss = 0.31099972\n",
      "Iteration 55, loss = 0.31007863\n",
      "Iteration 56, loss = 0.30896055\n",
      "Iteration 57, loss = 0.30785307\n",
      "Iteration 58, loss = 0.30697560\n",
      "Iteration 59, loss = 0.30587483\n",
      "Iteration 60, loss = 0.30485989\n",
      "Iteration 61, loss = 0.30391190\n",
      "Iteration 62, loss = 0.30283641\n",
      "Iteration 63, loss = 0.30190824\n",
      "Iteration 64, loss = 0.30109047\n",
      "Iteration 65, loss = 0.30000902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 66, loss = 0.29896643\n",
      "Iteration 67, loss = 0.29817845\n",
      "Iteration 68, loss = 0.29720931\n",
      "Iteration 69, loss = 0.29609313\n",
      "Iteration 70, loss = 0.29522775\n",
      "Iteration 71, loss = 0.29413212\n",
      "Iteration 72, loss = 0.29325873\n",
      "Iteration 73, loss = 0.29257525\n",
      "Iteration 74, loss = 0.29134183\n",
      "Iteration 75, loss = 0.29029777\n",
      "Iteration 76, loss = 0.28942318\n",
      "Iteration 77, loss = 0.28864609\n",
      "Iteration 78, loss = 0.28755955\n",
      "Iteration 79, loss = 0.28649860\n",
      "Iteration 80, loss = 0.28599904\n",
      "Iteration 81, loss = 0.28480029\n",
      "Iteration 82, loss = 0.28377999\n",
      "Iteration 83, loss = 0.28290814\n",
      "Iteration 84, loss = 0.28201404\n",
      "Iteration 85, loss = 0.28121646\n",
      "Iteration 86, loss = 0.28016037\n",
      "Iteration 87, loss = 0.27934492\n",
      "Iteration 88, loss = 0.27822828\n",
      "Iteration 89, loss = 0.27737942\n",
      "Iteration 90, loss = 0.27640410\n",
      "Iteration 91, loss = 0.27570372\n",
      "Iteration 92, loss = 0.27458727\n",
      "Iteration 93, loss = 0.27376877\n",
      "Iteration 94, loss = 0.27265781\n",
      "Iteration 95, loss = 0.27166749\n",
      "Iteration 96, loss = 0.27079594\n",
      "Iteration 97, loss = 0.26992890\n",
      "Iteration 98, loss = 0.26909531\n",
      "Iteration 99, loss = 0.26819842\n",
      "Iteration 100, loss = 0.26730702\n",
      "Iteration 101, loss = 0.26630497\n",
      "Iteration 102, loss = 0.26533717\n",
      "Iteration 103, loss = 0.26450928\n",
      "Iteration 104, loss = 0.26358612\n",
      "Iteration 105, loss = 0.26261849\n",
      "Iteration 106, loss = 0.26175078\n",
      "Iteration 107, loss = 0.26086605\n",
      "Iteration 108, loss = 0.25994952\n",
      "Iteration 109, loss = 0.25891707\n",
      "Iteration 110, loss = 0.25805795\n",
      "Iteration 111, loss = 0.25713192\n",
      "Iteration 112, loss = 0.25624879\n",
      "Iteration 113, loss = 0.25561255\n",
      "Iteration 114, loss = 0.25462560\n",
      "Iteration 115, loss = 0.25354898\n",
      "Iteration 116, loss = 0.25261977\n",
      "Iteration 117, loss = 0.25173210\n",
      "Iteration 118, loss = 0.25092526\n",
      "Iteration 119, loss = 0.24994761\n",
      "Iteration 120, loss = 0.24927209\n",
      "Iteration 121, loss = 0.24829534\n",
      "Iteration 122, loss = 0.24729378\n",
      "Iteration 123, loss = 0.24649297\n",
      "Iteration 124, loss = 0.24573406\n",
      "Iteration 125, loss = 0.24483559\n",
      "Iteration 126, loss = 0.24389700\n",
      "Iteration 127, loss = 0.24310194\n",
      "Iteration 128, loss = 0.24225229\n",
      "Iteration 129, loss = 0.24132027\n",
      "Iteration 130, loss = 0.24056107\n",
      "Iteration 131, loss = 0.23965957\n",
      "Iteration 132, loss = 0.23872835\n",
      "Iteration 133, loss = 0.23783705\n",
      "Iteration 134, loss = 0.23708196\n",
      "Iteration 135, loss = 0.23616536\n",
      "Iteration 136, loss = 0.23528376\n",
      "Iteration 137, loss = 0.23446600\n",
      "Iteration 138, loss = 0.23356400\n",
      "Iteration 139, loss = 0.23288258\n",
      "Iteration 140, loss = 0.23206315\n",
      "Iteration 141, loss = 0.23119340\n",
      "Iteration 142, loss = 0.23065585\n",
      "Iteration 143, loss = 0.22953940\n",
      "Iteration 144, loss = 0.22868549\n",
      "Iteration 145, loss = 0.22792456\n",
      "Iteration 146, loss = 0.22715012\n",
      "Iteration 147, loss = 0.22633825\n",
      "Iteration 148, loss = 0.22547341\n",
      "Iteration 149, loss = 0.22490255\n",
      "Iteration 150, loss = 0.22389912\n",
      "Iteration 151, loss = 0.22307236\n",
      "Iteration 152, loss = 0.22252290\n",
      "Iteration 153, loss = 0.22172416\n",
      "Iteration 154, loss = 0.22085578\n",
      "Iteration 155, loss = 0.22014823\n",
      "Iteration 156, loss = 0.21931842\n",
      "Iteration 157, loss = 0.21864943\n",
      "Iteration 158, loss = 0.21781668\n",
      "Iteration 159, loss = 0.21728705\n",
      "Iteration 160, loss = 0.21645551\n",
      "Iteration 161, loss = 0.21588690\n",
      "Iteration 162, loss = 0.21483255\n",
      "Iteration 163, loss = 0.21409296\n",
      "Iteration 164, loss = 0.21348236\n",
      "Iteration 165, loss = 0.21289087\n",
      "Iteration 166, loss = 0.21232375\n",
      "Iteration 167, loss = 0.21130948\n",
      "Iteration 168, loss = 0.21061218\n",
      "Iteration 169, loss = 0.20986022\n",
      "Iteration 170, loss = 0.20926907\n",
      "Iteration 171, loss = 0.20845781\n",
      "Iteration 172, loss = 0.20778252\n",
      "Iteration 173, loss = 0.20698851\n",
      "Iteration 174, loss = 0.20643389\n",
      "Iteration 175, loss = 0.20575389\n",
      "Iteration 176, loss = 0.20522897\n",
      "Iteration 177, loss = 0.20440915\n",
      "Iteration 178, loss = 0.20369586\n",
      "Iteration 179, loss = 0.20301654\n",
      "Iteration 180, loss = 0.20225708\n",
      "Iteration 181, loss = 0.20162798\n",
      "Iteration 182, loss = 0.20106815\n",
      "Iteration 183, loss = 0.20033562\n",
      "Iteration 184, loss = 0.19955168\n",
      "Iteration 185, loss = 0.19885476\n",
      "Iteration 186, loss = 0.19831272\n",
      "Iteration 187, loss = 0.19764706\n",
      "Iteration 188, loss = 0.19693616\n",
      "Iteration 189, loss = 0.19631746\n",
      "Iteration 190, loss = 0.19555943\n",
      "Iteration 191, loss = 0.19496183\n",
      "Iteration 192, loss = 0.19475527\n",
      "Iteration 193, loss = 0.19353662\n",
      "Iteration 194, loss = 0.19310918\n",
      "Iteration 195, loss = 0.19246880\n",
      "Iteration 196, loss = 0.19194197\n",
      "Iteration 197, loss = 0.19119667\n",
      "Iteration 198, loss = 0.19060792\n",
      "Iteration 199, loss = 0.18974023\n",
      "Iteration 200, loss = 0.18897477\n",
      "Iteration 201, loss = 0.18861602\n",
      "Iteration 202, loss = 0.18777607\n",
      "Iteration 203, loss = 0.18746860\n",
      "Iteration 204, loss = 0.18658464\n",
      "Iteration 205, loss = 0.18606679\n",
      "Iteration 206, loss = 0.18543982\n",
      "Iteration 207, loss = 0.18482082\n",
      "Iteration 208, loss = 0.18446652\n",
      "Iteration 209, loss = 0.18376033\n",
      "Iteration 210, loss = 0.18291200\n",
      "Iteration 211, loss = 0.18256692\n",
      "Iteration 212, loss = 0.18185498\n",
      "Iteration 213, loss = 0.18148791\n",
      "Iteration 214, loss = 0.18073894\n",
      "Iteration 215, loss = 0.18017195\n",
      "Iteration 216, loss = 0.17966887\n",
      "Iteration 217, loss = 0.17912252\n",
      "Iteration 218, loss = 0.17853176\n",
      "Iteration 219, loss = 0.17804699\n",
      "Iteration 220, loss = 0.17754216\n",
      "Iteration 221, loss = 0.17687491\n",
      "Iteration 222, loss = 0.17638294\n",
      "Iteration 223, loss = 0.17589124\n",
      "Iteration 224, loss = 0.17523521\n",
      "Iteration 225, loss = 0.17476965\n",
      "Iteration 226, loss = 0.17413497\n",
      "Iteration 227, loss = 0.17378311\n",
      "Iteration 228, loss = 0.17308040\n",
      "Iteration 229, loss = 0.17263731\n",
      "Iteration 230, loss = 0.17227923\n",
      "Iteration 231, loss = 0.17182969\n",
      "Iteration 232, loss = 0.17104054\n",
      "Iteration 233, loss = 0.17061864\n",
      "Iteration 234, loss = 0.17018374\n",
      "Iteration 235, loss = 0.16965690\n",
      "Iteration 236, loss = 0.16911785\n",
      "Iteration 237, loss = 0.16851681\n",
      "Iteration 238, loss = 0.16819558\n",
      "Iteration 239, loss = 0.16777525\n",
      "Iteration 240, loss = 0.16694442\n",
      "Iteration 241, loss = 0.16674462\n",
      "Iteration 242, loss = 0.16615755\n",
      "Iteration 243, loss = 0.16560762\n",
      "Iteration 244, loss = 0.16521969\n",
      "Iteration 245, loss = 0.16459633\n",
      "Iteration 246, loss = 0.16423053\n",
      "Iteration 247, loss = 0.16374036\n",
      "Iteration 248, loss = 0.16335091\n",
      "Iteration 249, loss = 0.16278647\n",
      "Iteration 250, loss = 0.16231883\n",
      "Iteration 251, loss = 0.16210342\n",
      "Iteration 252, loss = 0.16138249\n",
      "Iteration 253, loss = 0.16097368\n",
      "Iteration 254, loss = 0.16047712\n",
      "Iteration 255, loss = 0.16022005\n",
      "Iteration 256, loss = 0.15984163\n",
      "Iteration 257, loss = 0.15919225\n",
      "Iteration 258, loss = 0.15891530\n",
      "Iteration 259, loss = 0.15828828\n",
      "Iteration 260, loss = 0.15814626\n",
      "Iteration 261, loss = 0.15765590\n",
      "Iteration 262, loss = 0.15725030\n",
      "Iteration 263, loss = 0.15668498\n",
      "Iteration 264, loss = 0.15627769\n",
      "Iteration 265, loss = 0.15599498\n",
      "Iteration 266, loss = 0.15550348\n",
      "Iteration 267, loss = 0.15510539\n",
      "Iteration 268, loss = 0.15478674\n",
      "Iteration 269, loss = 0.15441332\n",
      "Iteration 270, loss = 0.15370779\n",
      "Iteration 271, loss = 0.15350670\n",
      "Iteration 272, loss = 0.15306319\n",
      "Iteration 273, loss = 0.15255357\n",
      "Iteration 274, loss = 0.15251780\n",
      "Iteration 275, loss = 0.15187360\n",
      "Iteration 276, loss = 0.15157637\n",
      "Iteration 277, loss = 0.15124761\n",
      "Iteration 278, loss = 0.15060873\n",
      "Iteration 279, loss = 0.15039475\n",
      "Iteration 280, loss = 0.15030131\n",
      "Iteration 281, loss = 0.14965180\n",
      "Iteration 282, loss = 0.14933500\n",
      "Iteration 283, loss = 0.14882200\n",
      "Iteration 284, loss = 0.14857304\n",
      "Iteration 285, loss = 0.14809808\n",
      "Iteration 286, loss = 0.14790803\n",
      "Iteration 287, loss = 0.14754998\n",
      "Iteration 288, loss = 0.14699585\n",
      "Iteration 289, loss = 0.14682058\n",
      "Iteration 290, loss = 0.14638365\n",
      "Iteration 291, loss = 0.14620757\n",
      "Iteration 292, loss = 0.14580258\n",
      "Iteration 293, loss = 0.14536018\n",
      "Iteration 294, loss = 0.14507597\n",
      "Iteration 295, loss = 0.14484028\n",
      "Iteration 296, loss = 0.14450958\n",
      "Iteration 297, loss = 0.14390877\n",
      "Iteration 298, loss = 0.14388012\n",
      "Iteration 299, loss = 0.14356049\n",
      "Iteration 300, loss = 0.14334679\n",
      "Iteration 301, loss = 0.14282784\n",
      "Iteration 302, loss = 0.14269653\n",
      "Iteration 303, loss = 0.14240439\n",
      "Iteration 304, loss = 0.14187121\n",
      "Iteration 305, loss = 0.14161404\n",
      "Iteration 306, loss = 0.14162429\n",
      "Iteration 307, loss = 0.14102574\n",
      "Iteration 308, loss = 0.14090764\n",
      "Iteration 309, loss = 0.14054870\n",
      "Iteration 310, loss = 0.14008751\n",
      "Iteration 311, loss = 0.13988463\n",
      "Iteration 312, loss = 0.13965123\n",
      "Iteration 313, loss = 0.13940314\n",
      "Iteration 314, loss = 0.13911540\n",
      "Iteration 315, loss = 0.13894184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 316, loss = 0.13858003\n",
      "Iteration 317, loss = 0.13842773\n",
      "Iteration 318, loss = 0.13802827\n",
      "Iteration 319, loss = 0.13758792\n",
      "Iteration 320, loss = 0.13725908\n",
      "Iteration 321, loss = 0.13701464\n",
      "Iteration 322, loss = 0.13679000\n",
      "Iteration 323, loss = 0.13648167\n",
      "Iteration 324, loss = 0.13633519\n",
      "Iteration 325, loss = 0.13597731\n",
      "Iteration 326, loss = 0.13587169\n",
      "Iteration 327, loss = 0.13532706\n",
      "Iteration 328, loss = 0.13523449\n",
      "Iteration 329, loss = 0.13497024\n",
      "Iteration 330, loss = 0.13493723\n",
      "Iteration 331, loss = 0.13462209\n",
      "Iteration 332, loss = 0.13392710\n",
      "Iteration 333, loss = 0.13401991\n",
      "Iteration 334, loss = 0.13366616\n",
      "Iteration 335, loss = 0.13328129\n",
      "Iteration 336, loss = 0.13332652\n",
      "Iteration 337, loss = 0.13303080\n",
      "Iteration 338, loss = 0.13261990\n",
      "Iteration 339, loss = 0.13251751\n",
      "Iteration 340, loss = 0.13226341\n",
      "Iteration 341, loss = 0.13205289\n",
      "Iteration 342, loss = 0.13178630\n",
      "Iteration 343, loss = 0.13176795\n",
      "Iteration 344, loss = 0.13134212\n",
      "Iteration 345, loss = 0.13139778\n",
      "Iteration 346, loss = 0.13104656\n",
      "Iteration 347, loss = 0.13067253\n",
      "Iteration 348, loss = 0.13041125\n",
      "Iteration 349, loss = 0.13028485\n",
      "Iteration 350, loss = 0.12997242\n",
      "Iteration 351, loss = 0.12985106\n",
      "Iteration 352, loss = 0.12969979\n",
      "Iteration 353, loss = 0.12943598\n",
      "Iteration 354, loss = 0.12901057\n",
      "Iteration 355, loss = 0.12881553\n",
      "Iteration 356, loss = 0.12890104\n",
      "Iteration 357, loss = 0.12877710\n",
      "Iteration 358, loss = 0.12830970\n",
      "Iteration 359, loss = 0.12850451\n",
      "Iteration 360, loss = 0.12780890\n",
      "Iteration 361, loss = 0.12747743\n",
      "Iteration 362, loss = 0.12749646\n",
      "Iteration 363, loss = 0.12727779\n",
      "Iteration 364, loss = 0.12701437\n",
      "Iteration 365, loss = 0.12683447\n",
      "Iteration 366, loss = 0.12672243\n",
      "Iteration 367, loss = 0.12648255\n",
      "Iteration 368, loss = 0.12634807\n",
      "Iteration 369, loss = 0.12604308\n",
      "Iteration 370, loss = 0.12600107\n",
      "Iteration 371, loss = 0.12554833\n",
      "Iteration 372, loss = 0.12560018\n",
      "Iteration 373, loss = 0.12516725\n",
      "Iteration 374, loss = 0.12509122\n",
      "Iteration 375, loss = 0.12500379\n",
      "Iteration 376, loss = 0.12471124\n",
      "Iteration 377, loss = 0.12455159\n",
      "Iteration 378, loss = 0.12441060\n",
      "Iteration 379, loss = 0.12409488\n",
      "Iteration 380, loss = 0.12417931\n",
      "Iteration 381, loss = 0.12412540\n",
      "Iteration 382, loss = 0.12363160\n",
      "Iteration 383, loss = 0.12334334\n",
      "Iteration 384, loss = 0.12336370\n",
      "Iteration 385, loss = 0.12320861\n",
      "Iteration 386, loss = 0.12341715\n",
      "Iteration 387, loss = 0.12268154\n",
      "Iteration 388, loss = 0.12260820\n",
      "Iteration 389, loss = 0.12257983\n",
      "Iteration 390, loss = 0.12255833\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50224028\n",
      "Iteration 2, loss = 0.46805592\n",
      "Iteration 3, loss = 0.44564341\n",
      "Iteration 4, loss = 0.43073389\n",
      "Iteration 5, loss = 0.42023875\n",
      "Iteration 6, loss = 0.41259766\n",
      "Iteration 7, loss = 0.40655157\n",
      "Iteration 8, loss = 0.40164010\n",
      "Iteration 9, loss = 0.39731117\n",
      "Iteration 10, loss = 0.39340123\n",
      "Iteration 11, loss = 0.38979468\n",
      "Iteration 12, loss = 0.38629251\n",
      "Iteration 13, loss = 0.38294411\n",
      "Iteration 14, loss = 0.37982690\n",
      "Iteration 15, loss = 0.37675395\n",
      "Iteration 16, loss = 0.37383707\n",
      "Iteration 17, loss = 0.37126002\n",
      "Iteration 18, loss = 0.36842316\n",
      "Iteration 19, loss = 0.36569749\n",
      "Iteration 20, loss = 0.36306191\n",
      "Iteration 21, loss = 0.36056315\n",
      "Iteration 22, loss = 0.35818664\n",
      "Iteration 23, loss = 0.35585073\n",
      "Iteration 24, loss = 0.35359356\n",
      "Iteration 25, loss = 0.35151025\n",
      "Iteration 26, loss = 0.34929130\n",
      "Iteration 27, loss = 0.34730267\n",
      "Iteration 28, loss = 0.34536126\n",
      "Iteration 29, loss = 0.34333803\n",
      "Iteration 30, loss = 0.34149309\n",
      "Iteration 31, loss = 0.33978552\n",
      "Iteration 32, loss = 0.33799491\n",
      "Iteration 33, loss = 0.33641173\n",
      "Iteration 34, loss = 0.33474052\n",
      "Iteration 35, loss = 0.33314160\n",
      "Iteration 36, loss = 0.33156474\n",
      "Iteration 37, loss = 0.33014165\n",
      "Iteration 38, loss = 0.32866247\n",
      "Iteration 39, loss = 0.32725468\n",
      "Iteration 40, loss = 0.32601727\n",
      "Iteration 41, loss = 0.32455564\n",
      "Iteration 42, loss = 0.32354470\n",
      "Iteration 43, loss = 0.32215516\n",
      "Iteration 44, loss = 0.32090296\n",
      "Iteration 45, loss = 0.31973179\n",
      "Iteration 46, loss = 0.31843290\n",
      "Iteration 47, loss = 0.31734317\n",
      "Iteration 48, loss = 0.31616140\n",
      "Iteration 49, loss = 0.31503678\n",
      "Iteration 50, loss = 0.31406504\n",
      "Iteration 51, loss = 0.31288758\n",
      "Iteration 52, loss = 0.31209173\n",
      "Iteration 53, loss = 0.31100457\n",
      "Iteration 54, loss = 0.30978392\n",
      "Iteration 55, loss = 0.30886085\n",
      "Iteration 56, loss = 0.30781505\n",
      "Iteration 57, loss = 0.30689023\n",
      "Iteration 58, loss = 0.30586624\n",
      "Iteration 59, loss = 0.30479147\n",
      "Iteration 60, loss = 0.30384200\n",
      "Iteration 61, loss = 0.30306545\n",
      "Iteration 62, loss = 0.30193427\n",
      "Iteration 63, loss = 0.30105951\n",
      "Iteration 64, loss = 0.30026358\n",
      "Iteration 65, loss = 0.29921340\n",
      "Iteration 66, loss = 0.29825153\n",
      "Iteration 67, loss = 0.29749417\n",
      "Iteration 68, loss = 0.29665593\n",
      "Iteration 69, loss = 0.29562287\n",
      "Iteration 70, loss = 0.29480772\n",
      "Iteration 71, loss = 0.29384676\n",
      "Iteration 72, loss = 0.29317917\n",
      "Iteration 73, loss = 0.29222156\n",
      "Iteration 74, loss = 0.29131495\n",
      "Iteration 75, loss = 0.29031679\n",
      "Iteration 76, loss = 0.28945354\n",
      "Iteration 77, loss = 0.28877439\n",
      "Iteration 78, loss = 0.28785306\n",
      "Iteration 79, loss = 0.28681921\n",
      "Iteration 80, loss = 0.28605985\n",
      "Iteration 81, loss = 0.28532943\n",
      "Iteration 82, loss = 0.28428355\n",
      "Iteration 83, loss = 0.28330079\n",
      "Iteration 84, loss = 0.28234050\n",
      "Iteration 85, loss = 0.28167724\n",
      "Iteration 86, loss = 0.28078573\n",
      "Iteration 87, loss = 0.27992518\n",
      "Iteration 88, loss = 0.27891018\n",
      "Iteration 89, loss = 0.27794558\n",
      "Iteration 90, loss = 0.27704545\n",
      "Iteration 91, loss = 0.27643048\n",
      "Iteration 92, loss = 0.27547735\n",
      "Iteration 93, loss = 0.27447612\n",
      "Iteration 94, loss = 0.27363688\n",
      "Iteration 95, loss = 0.27266530\n",
      "Iteration 96, loss = 0.27170818\n",
      "Iteration 97, loss = 0.27072364\n",
      "Iteration 98, loss = 0.26994649\n",
      "Iteration 99, loss = 0.26909967\n",
      "Iteration 100, loss = 0.26814886\n",
      "Iteration 101, loss = 0.26724131\n",
      "Iteration 102, loss = 0.26632971\n",
      "Iteration 103, loss = 0.26542984\n",
      "Iteration 104, loss = 0.26459878\n",
      "Iteration 105, loss = 0.26372854\n",
      "Iteration 106, loss = 0.26284329\n",
      "Iteration 107, loss = 0.26184064\n",
      "Iteration 108, loss = 0.26084928\n",
      "Iteration 109, loss = 0.26002535\n",
      "Iteration 110, loss = 0.25912966\n",
      "Iteration 111, loss = 0.25824058\n",
      "Iteration 112, loss = 0.25735707\n",
      "Iteration 113, loss = 0.25642818\n",
      "Iteration 114, loss = 0.25547011\n",
      "Iteration 115, loss = 0.25471885\n",
      "Iteration 116, loss = 0.25370790\n",
      "Iteration 117, loss = 0.25274404\n",
      "Iteration 118, loss = 0.25199308\n",
      "Iteration 119, loss = 0.25113629\n",
      "Iteration 120, loss = 0.25028849\n",
      "Iteration 121, loss = 0.24928218\n",
      "Iteration 122, loss = 0.24841914\n",
      "Iteration 123, loss = 0.24767040\n",
      "Iteration 124, loss = 0.24682048\n",
      "Iteration 125, loss = 0.24582570\n",
      "Iteration 126, loss = 0.24482881\n",
      "Iteration 127, loss = 0.24410156\n",
      "Iteration 128, loss = 0.24340442\n",
      "Iteration 129, loss = 0.24255595\n",
      "Iteration 130, loss = 0.24156634\n",
      "Iteration 131, loss = 0.24087522\n",
      "Iteration 132, loss = 0.23986593\n",
      "Iteration 133, loss = 0.23903048\n",
      "Iteration 134, loss = 0.23828518\n",
      "Iteration 135, loss = 0.23743763\n",
      "Iteration 136, loss = 0.23656436\n",
      "Iteration 137, loss = 0.23575219\n",
      "Iteration 138, loss = 0.23479406\n",
      "Iteration 139, loss = 0.23416517\n",
      "Iteration 140, loss = 0.23318336\n",
      "Iteration 141, loss = 0.23235521\n",
      "Iteration 142, loss = 0.23188221\n",
      "Iteration 143, loss = 0.23084227\n",
      "Iteration 144, loss = 0.23002721\n",
      "Iteration 145, loss = 0.22924025\n",
      "Iteration 146, loss = 0.22862159\n",
      "Iteration 147, loss = 0.22767685\n",
      "Iteration 148, loss = 0.22687586\n",
      "Iteration 149, loss = 0.22618925\n",
      "Iteration 150, loss = 0.22528010\n",
      "Iteration 151, loss = 0.22460405\n",
      "Iteration 152, loss = 0.22375857\n",
      "Iteration 153, loss = 0.22297041\n",
      "Iteration 154, loss = 0.22225615\n",
      "Iteration 155, loss = 0.22153006\n",
      "Iteration 156, loss = 0.22070929\n",
      "Iteration 157, loss = 0.22011280\n",
      "Iteration 158, loss = 0.21929670\n",
      "Iteration 159, loss = 0.21871555\n",
      "Iteration 160, loss = 0.21787422\n",
      "Iteration 161, loss = 0.21723009\n",
      "Iteration 162, loss = 0.21630835\n",
      "Iteration 163, loss = 0.21571942\n",
      "Iteration 164, loss = 0.21510880\n",
      "Iteration 165, loss = 0.21430724\n",
      "Iteration 166, loss = 0.21383107\n",
      "Iteration 167, loss = 0.21281270\n",
      "Iteration 168, loss = 0.21210386\n",
      "Iteration 169, loss = 0.21143510\n",
      "Iteration 170, loss = 0.21082019\n",
      "Iteration 171, loss = 0.21004984\n",
      "Iteration 172, loss = 0.20957630\n",
      "Iteration 173, loss = 0.20861357\n",
      "Iteration 174, loss = 0.20803419\n",
      "Iteration 175, loss = 0.20742052\n",
      "Iteration 176, loss = 0.20681343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 177, loss = 0.20608279\n",
      "Iteration 178, loss = 0.20559522\n",
      "Iteration 179, loss = 0.20471579\n",
      "Iteration 180, loss = 0.20409656\n",
      "Iteration 181, loss = 0.20332293\n",
      "Iteration 182, loss = 0.20286221\n",
      "Iteration 183, loss = 0.20234132\n",
      "Iteration 184, loss = 0.20149406\n",
      "Iteration 185, loss = 0.20081258\n",
      "Iteration 186, loss = 0.20031139\n",
      "Iteration 187, loss = 0.19947721\n",
      "Iteration 188, loss = 0.19884354\n",
      "Iteration 189, loss = 0.19822151\n",
      "Iteration 190, loss = 0.19759604\n",
      "Iteration 191, loss = 0.19695968\n",
      "Iteration 192, loss = 0.19654208\n",
      "Iteration 193, loss = 0.19563547\n",
      "Iteration 194, loss = 0.19528863\n",
      "Iteration 195, loss = 0.19451739\n",
      "Iteration 196, loss = 0.19418865\n",
      "Iteration 197, loss = 0.19338300\n",
      "Iteration 198, loss = 0.19283717\n",
      "Iteration 199, loss = 0.19195512\n",
      "Iteration 200, loss = 0.19129743\n",
      "Iteration 201, loss = 0.19097870\n",
      "Iteration 202, loss = 0.19020238\n",
      "Iteration 203, loss = 0.18981013\n",
      "Iteration 204, loss = 0.18894834\n",
      "Iteration 205, loss = 0.18847456\n",
      "Iteration 206, loss = 0.18783407\n",
      "Iteration 207, loss = 0.18720518\n",
      "Iteration 208, loss = 0.18684990\n",
      "Iteration 209, loss = 0.18610164\n",
      "Iteration 210, loss = 0.18554317\n",
      "Iteration 211, loss = 0.18494226\n",
      "Iteration 212, loss = 0.18435727\n",
      "Iteration 213, loss = 0.18394828\n",
      "Iteration 214, loss = 0.18368386\n",
      "Iteration 215, loss = 0.18281136\n",
      "Iteration 216, loss = 0.18213511\n",
      "Iteration 217, loss = 0.18169000\n",
      "Iteration 218, loss = 0.18095217\n",
      "Iteration 219, loss = 0.18035361\n",
      "Iteration 220, loss = 0.17991172\n",
      "Iteration 221, loss = 0.17927648\n",
      "Iteration 222, loss = 0.17878382\n",
      "Iteration 223, loss = 0.17822575\n",
      "Iteration 224, loss = 0.17788468\n",
      "Iteration 225, loss = 0.17729833\n",
      "Iteration 226, loss = 0.17664944\n",
      "Iteration 227, loss = 0.17628070\n",
      "Iteration 228, loss = 0.17564614\n",
      "Iteration 229, loss = 0.17515208\n",
      "Iteration 230, loss = 0.17451992\n",
      "Iteration 231, loss = 0.17424327\n",
      "Iteration 232, loss = 0.17339006\n",
      "Iteration 233, loss = 0.17305947\n",
      "Iteration 234, loss = 0.17245097\n",
      "Iteration 235, loss = 0.17226725\n",
      "Iteration 236, loss = 0.17162031\n",
      "Iteration 237, loss = 0.17102521\n",
      "Iteration 238, loss = 0.17054108\n",
      "Iteration 239, loss = 0.17020168\n",
      "Iteration 240, loss = 0.16946184\n",
      "Iteration 241, loss = 0.16909789\n",
      "Iteration 242, loss = 0.16859207\n",
      "Iteration 243, loss = 0.16799516\n",
      "Iteration 244, loss = 0.16746645\n",
      "Iteration 245, loss = 0.16686627\n",
      "Iteration 246, loss = 0.16645054\n",
      "Iteration 247, loss = 0.16623899\n",
      "Iteration 248, loss = 0.16585705\n",
      "Iteration 249, loss = 0.16513342\n",
      "Iteration 250, loss = 0.16458121\n",
      "Iteration 251, loss = 0.16450620\n",
      "Iteration 252, loss = 0.16372718\n",
      "Iteration 253, loss = 0.16326700\n",
      "Iteration 254, loss = 0.16281339\n",
      "Iteration 255, loss = 0.16243426\n",
      "Iteration 256, loss = 0.16185709\n",
      "Iteration 257, loss = 0.16149847\n",
      "Iteration 258, loss = 0.16093383\n",
      "Iteration 259, loss = 0.16051924\n",
      "Iteration 260, loss = 0.16023279\n",
      "Iteration 261, loss = 0.15971292\n",
      "Iteration 262, loss = 0.15913964\n",
      "Iteration 263, loss = 0.15875861\n",
      "Iteration 264, loss = 0.15835314\n",
      "Iteration 265, loss = 0.15807835\n",
      "Iteration 266, loss = 0.15758369\n",
      "Iteration 267, loss = 0.15714292\n",
      "Iteration 268, loss = 0.15678582\n",
      "Iteration 269, loss = 0.15631478\n",
      "Iteration 270, loss = 0.15585105\n",
      "Iteration 271, loss = 0.15547923\n",
      "Iteration 272, loss = 0.15515335\n",
      "Iteration 273, loss = 0.15463618\n",
      "Iteration 274, loss = 0.15456537\n",
      "Iteration 275, loss = 0.15383712\n",
      "Iteration 276, loss = 0.15344570\n",
      "Iteration 277, loss = 0.15326864\n",
      "Iteration 278, loss = 0.15253352\n",
      "Iteration 279, loss = 0.15250640\n",
      "Iteration 280, loss = 0.15228886\n",
      "Iteration 281, loss = 0.15160549\n",
      "Iteration 282, loss = 0.15121554\n",
      "Iteration 283, loss = 0.15064340\n",
      "Iteration 284, loss = 0.15038646\n",
      "Iteration 285, loss = 0.14984225\n",
      "Iteration 286, loss = 0.14975733\n",
      "Iteration 287, loss = 0.14930474\n",
      "Iteration 288, loss = 0.14892244\n",
      "Iteration 289, loss = 0.14855994\n",
      "Iteration 290, loss = 0.14801627\n",
      "Iteration 291, loss = 0.14795689\n",
      "Iteration 292, loss = 0.14757680\n",
      "Iteration 293, loss = 0.14705703\n",
      "Iteration 294, loss = 0.14680696\n",
      "Iteration 295, loss = 0.14631927\n",
      "Iteration 296, loss = 0.14600801\n",
      "Iteration 297, loss = 0.14576766\n",
      "Iteration 298, loss = 0.14523983\n",
      "Iteration 299, loss = 0.14497126\n",
      "Iteration 300, loss = 0.14467500\n",
      "Iteration 301, loss = 0.14438161\n",
      "Iteration 302, loss = 0.14397676\n",
      "Iteration 303, loss = 0.14356615\n",
      "Iteration 304, loss = 0.14314059\n",
      "Iteration 305, loss = 0.14294832\n",
      "Iteration 306, loss = 0.14278981\n",
      "Iteration 307, loss = 0.14225588\n",
      "Iteration 308, loss = 0.14226385\n",
      "Iteration 309, loss = 0.14161909\n",
      "Iteration 310, loss = 0.14124046\n",
      "Iteration 311, loss = 0.14132057\n",
      "Iteration 312, loss = 0.14074079\n",
      "Iteration 313, loss = 0.14054616\n",
      "Iteration 314, loss = 0.14010545\n",
      "Iteration 315, loss = 0.14014082\n",
      "Iteration 316, loss = 0.13974420\n",
      "Iteration 317, loss = 0.13945006\n",
      "Iteration 318, loss = 0.13902016\n",
      "Iteration 319, loss = 0.13859838\n",
      "Iteration 320, loss = 0.13822031\n",
      "Iteration 321, loss = 0.13807404\n",
      "Iteration 322, loss = 0.13780544\n",
      "Iteration 323, loss = 0.13763263\n",
      "Iteration 324, loss = 0.13709876\n",
      "Iteration 325, loss = 0.13711156\n",
      "Iteration 326, loss = 0.13658676\n",
      "Iteration 327, loss = 0.13635391\n",
      "Iteration 328, loss = 0.13616328\n",
      "Iteration 329, loss = 0.13594917\n",
      "Iteration 330, loss = 0.13570568\n",
      "Iteration 331, loss = 0.13563833\n",
      "Iteration 332, loss = 0.13486321\n",
      "Iteration 333, loss = 0.13482347\n",
      "Iteration 334, loss = 0.13455744\n",
      "Iteration 335, loss = 0.13433177\n",
      "Iteration 336, loss = 0.13406697\n",
      "Iteration 337, loss = 0.13371538\n",
      "Iteration 338, loss = 0.13331449\n",
      "Iteration 339, loss = 0.13328741\n",
      "Iteration 340, loss = 0.13309502\n",
      "Iteration 341, loss = 0.13275968\n",
      "Iteration 342, loss = 0.13247145\n",
      "Iteration 343, loss = 0.13237106\n",
      "Iteration 344, loss = 0.13192941\n",
      "Iteration 345, loss = 0.13173486\n",
      "Iteration 346, loss = 0.13159654\n",
      "Iteration 347, loss = 0.13121749\n",
      "Iteration 348, loss = 0.13108300\n",
      "Iteration 349, loss = 0.13069944\n",
      "Iteration 350, loss = 0.13029616\n",
      "Iteration 351, loss = 0.13033810\n",
      "Iteration 352, loss = 0.13017194\n",
      "Iteration 353, loss = 0.12984162\n",
      "Iteration 354, loss = 0.12949718\n",
      "Iteration 355, loss = 0.12911972\n",
      "Iteration 356, loss = 0.12920335\n",
      "Iteration 357, loss = 0.12953381\n",
      "Iteration 358, loss = 0.12868550\n",
      "Iteration 359, loss = 0.12856752\n",
      "Iteration 360, loss = 0.12804942\n",
      "Iteration 361, loss = 0.12795259\n",
      "Iteration 362, loss = 0.12773792\n",
      "Iteration 363, loss = 0.12749744\n",
      "Iteration 364, loss = 0.12726794\n",
      "Iteration 365, loss = 0.12696624\n",
      "Iteration 366, loss = 0.12689096\n",
      "Iteration 367, loss = 0.12663930\n",
      "Iteration 368, loss = 0.12649829\n",
      "Iteration 369, loss = 0.12627200\n",
      "Iteration 370, loss = 0.12636867\n",
      "Iteration 371, loss = 0.12579195\n",
      "Iteration 372, loss = 0.12589846\n",
      "Iteration 373, loss = 0.12565377\n",
      "Iteration 374, loss = 0.12536197\n",
      "Iteration 375, loss = 0.12506794\n",
      "Iteration 376, loss = 0.12465556\n",
      "Iteration 377, loss = 0.12486086\n",
      "Iteration 378, loss = 0.12483882\n",
      "Iteration 379, loss = 0.12429828\n",
      "Iteration 380, loss = 0.12414315\n",
      "Iteration 381, loss = 0.12395980\n",
      "Iteration 382, loss = 0.12373873\n",
      "Iteration 383, loss = 0.12348393\n",
      "Iteration 384, loss = 0.12346753\n",
      "Iteration 385, loss = 0.12306994\n",
      "Iteration 386, loss = 0.12341689\n",
      "Iteration 387, loss = 0.12263531\n",
      "Iteration 388, loss = 0.12263056\n",
      "Iteration 389, loss = 0.12230605\n",
      "Iteration 390, loss = 0.12216282\n",
      "Iteration 391, loss = 0.12188543\n",
      "Iteration 392, loss = 0.12190300\n",
      "Iteration 393, loss = 0.12141616\n",
      "Iteration 394, loss = 0.12135804\n",
      "Iteration 395, loss = 0.12131522\n",
      "Iteration 396, loss = 0.12098826\n",
      "Iteration 397, loss = 0.12069205\n",
      "Iteration 398, loss = 0.12052841\n",
      "Iteration 399, loss = 0.12038922\n",
      "Iteration 400, loss = 0.12016574\n",
      "Iteration 401, loss = 0.12017842\n",
      "Iteration 402, loss = 0.12010757\n",
      "Iteration 403, loss = 0.11979551\n",
      "Iteration 404, loss = 0.11954650\n",
      "Iteration 405, loss = 0.11978076\n",
      "Iteration 406, loss = 0.11900287\n",
      "Iteration 407, loss = 0.11923363\n",
      "Iteration 408, loss = 0.11888314\n",
      "Iteration 409, loss = 0.11871015\n",
      "Iteration 410, loss = 0.11867496\n",
      "Iteration 411, loss = 0.11835809\n",
      "Iteration 412, loss = 0.11839098\n",
      "Iteration 413, loss = 0.11810371\n",
      "Iteration 414, loss = 0.11790732\n",
      "Iteration 415, loss = 0.11786481\n",
      "Iteration 416, loss = 0.11776321\n",
      "Iteration 417, loss = 0.11781276\n",
      "Iteration 418, loss = 0.11748008\n",
      "Iteration 419, loss = 0.11722189\n",
      "Iteration 420, loss = 0.11711455\n",
      "Iteration 421, loss = 0.11692771\n",
      "Iteration 422, loss = 0.11661951\n",
      "Iteration 423, loss = 0.11632016\n",
      "Iteration 424, loss = 0.11655362\n",
      "Iteration 425, loss = 0.11640903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 426, loss = 0.11609981\n",
      "Iteration 427, loss = 0.11592327\n",
      "Iteration 428, loss = 0.11587718\n",
      "Iteration 429, loss = 0.11555652\n",
      "Iteration 430, loss = 0.11546013\n",
      "Iteration 431, loss = 0.11543568\n",
      "Iteration 432, loss = 0.11545279\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50191428\n",
      "Iteration 2, loss = 0.47004226\n",
      "Iteration 3, loss = 0.44858670\n",
      "Iteration 4, loss = 0.43398704\n",
      "Iteration 5, loss = 0.42352555\n",
      "Iteration 6, loss = 0.41577953\n",
      "Iteration 7, loss = 0.40951785\n",
      "Iteration 8, loss = 0.40433915\n",
      "Iteration 9, loss = 0.39986627\n",
      "Iteration 10, loss = 0.39579073\n",
      "Iteration 11, loss = 0.39189325\n",
      "Iteration 12, loss = 0.38831620\n",
      "Iteration 13, loss = 0.38477661\n",
      "Iteration 14, loss = 0.38149372\n",
      "Iteration 15, loss = 0.37829962\n",
      "Iteration 16, loss = 0.37517863\n",
      "Iteration 17, loss = 0.37225967\n",
      "Iteration 18, loss = 0.36941018\n",
      "Iteration 19, loss = 0.36652002\n",
      "Iteration 20, loss = 0.36381255\n",
      "Iteration 21, loss = 0.36115206\n",
      "Iteration 22, loss = 0.35864211\n",
      "Iteration 23, loss = 0.35613865\n",
      "Iteration 24, loss = 0.35373740\n",
      "Iteration 25, loss = 0.35154146\n",
      "Iteration 26, loss = 0.34933682\n",
      "Iteration 27, loss = 0.34717964\n",
      "Iteration 28, loss = 0.34504460\n",
      "Iteration 29, loss = 0.34305292\n",
      "Iteration 30, loss = 0.34118894\n",
      "Iteration 31, loss = 0.33935224\n",
      "Iteration 32, loss = 0.33753914\n",
      "Iteration 33, loss = 0.33578019\n",
      "Iteration 34, loss = 0.33420819\n",
      "Iteration 35, loss = 0.33251314\n",
      "Iteration 36, loss = 0.33103732\n",
      "Iteration 37, loss = 0.32955474\n",
      "Iteration 38, loss = 0.32806907\n",
      "Iteration 39, loss = 0.32651957\n",
      "Iteration 40, loss = 0.32525549\n",
      "Iteration 41, loss = 0.32379910\n",
      "Iteration 42, loss = 0.32260708\n",
      "Iteration 43, loss = 0.32124248\n",
      "Iteration 44, loss = 0.32007123\n",
      "Iteration 45, loss = 0.31868661\n",
      "Iteration 46, loss = 0.31754493\n",
      "Iteration 47, loss = 0.31649190\n",
      "Iteration 48, loss = 0.31543775\n",
      "Iteration 49, loss = 0.31422533\n",
      "Iteration 50, loss = 0.31314712\n",
      "Iteration 51, loss = 0.31196656\n",
      "Iteration 52, loss = 0.31120254\n",
      "Iteration 53, loss = 0.31009161\n",
      "Iteration 54, loss = 0.30902377\n",
      "Iteration 55, loss = 0.30794555\n",
      "Iteration 56, loss = 0.30698252\n",
      "Iteration 57, loss = 0.30613871\n",
      "Iteration 58, loss = 0.30507600\n",
      "Iteration 59, loss = 0.30397643\n",
      "Iteration 60, loss = 0.30319122\n",
      "Iteration 61, loss = 0.30228875\n",
      "Iteration 62, loss = 0.30118916\n",
      "Iteration 63, loss = 0.30035029\n",
      "Iteration 64, loss = 0.29954439\n",
      "Iteration 65, loss = 0.29850322\n",
      "Iteration 66, loss = 0.29759352\n",
      "Iteration 67, loss = 0.29676085\n",
      "Iteration 68, loss = 0.29591655\n",
      "Iteration 69, loss = 0.29492730\n",
      "Iteration 70, loss = 0.29402984\n",
      "Iteration 71, loss = 0.29312425\n",
      "Iteration 72, loss = 0.29235012\n",
      "Iteration 73, loss = 0.29129417\n",
      "Iteration 74, loss = 0.29039469\n",
      "Iteration 75, loss = 0.28953293\n",
      "Iteration 76, loss = 0.28875576\n",
      "Iteration 77, loss = 0.28780455\n",
      "Iteration 78, loss = 0.28702607\n",
      "Iteration 79, loss = 0.28593174\n",
      "Iteration 80, loss = 0.28505489\n",
      "Iteration 81, loss = 0.28441969\n",
      "Iteration 82, loss = 0.28340373\n",
      "Iteration 83, loss = 0.28229856\n",
      "Iteration 84, loss = 0.28145927\n",
      "Iteration 85, loss = 0.28081606\n",
      "Iteration 86, loss = 0.27991546\n",
      "Iteration 87, loss = 0.27899744\n",
      "Iteration 88, loss = 0.27791469\n",
      "Iteration 89, loss = 0.27716506\n",
      "Iteration 90, loss = 0.27614226\n",
      "Iteration 91, loss = 0.27538406\n",
      "Iteration 92, loss = 0.27443522\n",
      "Iteration 93, loss = 0.27352231\n",
      "Iteration 94, loss = 0.27260476\n",
      "Iteration 95, loss = 0.27171097\n",
      "Iteration 96, loss = 0.27082070\n",
      "Iteration 97, loss = 0.26991506\n",
      "Iteration 98, loss = 0.26912343\n",
      "Iteration 99, loss = 0.26829747\n",
      "Iteration 100, loss = 0.26722490\n",
      "Iteration 101, loss = 0.26623673\n",
      "Iteration 102, loss = 0.26544494\n",
      "Iteration 103, loss = 0.26449478\n",
      "Iteration 104, loss = 0.26354013\n",
      "Iteration 105, loss = 0.26276540\n",
      "Iteration 106, loss = 0.26196770\n",
      "Iteration 107, loss = 0.26089636\n",
      "Iteration 108, loss = 0.25989915\n",
      "Iteration 109, loss = 0.25898508\n",
      "Iteration 110, loss = 0.25807683\n",
      "Iteration 111, loss = 0.25709344\n",
      "Iteration 112, loss = 0.25626943\n",
      "Iteration 113, loss = 0.25526658\n",
      "Iteration 114, loss = 0.25429472\n",
      "Iteration 115, loss = 0.25348727\n",
      "Iteration 116, loss = 0.25245294\n",
      "Iteration 117, loss = 0.25164044\n",
      "Iteration 118, loss = 0.25055054\n",
      "Iteration 119, loss = 0.24976082\n",
      "Iteration 120, loss = 0.24890927\n",
      "Iteration 121, loss = 0.24790689\n",
      "Iteration 122, loss = 0.24701038\n",
      "Iteration 123, loss = 0.24620474\n",
      "Iteration 124, loss = 0.24543216\n",
      "Iteration 125, loss = 0.24428432\n",
      "Iteration 126, loss = 0.24335125\n",
      "Iteration 127, loss = 0.24242757\n",
      "Iteration 128, loss = 0.24163747\n",
      "Iteration 129, loss = 0.24100816\n",
      "Iteration 130, loss = 0.23972761\n",
      "Iteration 131, loss = 0.23899987\n",
      "Iteration 132, loss = 0.23803242\n",
      "Iteration 133, loss = 0.23722978\n",
      "Iteration 134, loss = 0.23631046\n",
      "Iteration 135, loss = 0.23533881\n",
      "Iteration 136, loss = 0.23437251\n",
      "Iteration 137, loss = 0.23372234\n",
      "Iteration 138, loss = 0.23275091\n",
      "Iteration 139, loss = 0.23186421\n",
      "Iteration 140, loss = 0.23086476\n",
      "Iteration 141, loss = 0.22995477\n",
      "Iteration 142, loss = 0.22936098\n",
      "Iteration 143, loss = 0.22833670\n",
      "Iteration 144, loss = 0.22758157\n",
      "Iteration 145, loss = 0.22671770\n",
      "Iteration 146, loss = 0.22603646\n",
      "Iteration 147, loss = 0.22490485\n",
      "Iteration 148, loss = 0.22409365\n",
      "Iteration 149, loss = 0.22327163\n",
      "Iteration 150, loss = 0.22244332\n",
      "Iteration 151, loss = 0.22166739\n",
      "Iteration 152, loss = 0.22076073\n",
      "Iteration 153, loss = 0.22007328\n",
      "Iteration 154, loss = 0.21922308\n",
      "Iteration 155, loss = 0.21836845\n",
      "Iteration 156, loss = 0.21754596\n",
      "Iteration 157, loss = 0.21689094\n",
      "Iteration 158, loss = 0.21601035\n",
      "Iteration 159, loss = 0.21518514\n",
      "Iteration 160, loss = 0.21431748\n",
      "Iteration 161, loss = 0.21360074\n",
      "Iteration 162, loss = 0.21275756\n",
      "Iteration 163, loss = 0.21208526\n",
      "Iteration 164, loss = 0.21141081\n",
      "Iteration 165, loss = 0.21043780\n",
      "Iteration 166, loss = 0.20996224\n",
      "Iteration 167, loss = 0.20872502\n",
      "Iteration 168, loss = 0.20830044\n",
      "Iteration 169, loss = 0.20758655\n",
      "Iteration 170, loss = 0.20682900\n",
      "Iteration 171, loss = 0.20607765\n",
      "Iteration 172, loss = 0.20551062\n",
      "Iteration 173, loss = 0.20447952\n",
      "Iteration 174, loss = 0.20387197\n",
      "Iteration 175, loss = 0.20333120\n",
      "Iteration 176, loss = 0.20265553\n",
      "Iteration 177, loss = 0.20192609\n",
      "Iteration 178, loss = 0.20127755\n",
      "Iteration 179, loss = 0.20043006\n",
      "Iteration 180, loss = 0.19973000\n",
      "Iteration 181, loss = 0.19911484\n",
      "Iteration 182, loss = 0.19846072\n",
      "Iteration 183, loss = 0.19790519\n",
      "Iteration 184, loss = 0.19716995\n",
      "Iteration 185, loss = 0.19645005\n",
      "Iteration 186, loss = 0.19602109\n",
      "Iteration 187, loss = 0.19537720\n",
      "Iteration 188, loss = 0.19461006\n",
      "Iteration 189, loss = 0.19396868\n",
      "Iteration 190, loss = 0.19342332\n",
      "Iteration 191, loss = 0.19273370\n",
      "Iteration 192, loss = 0.19212802\n",
      "Iteration 193, loss = 0.19159175\n",
      "Iteration 194, loss = 0.19102486\n",
      "Iteration 195, loss = 0.19031090\n",
      "Iteration 196, loss = 0.19011869\n",
      "Iteration 197, loss = 0.18914372\n",
      "Iteration 198, loss = 0.18870736\n",
      "Iteration 199, loss = 0.18801980\n",
      "Iteration 200, loss = 0.18729236\n",
      "Iteration 201, loss = 0.18704558\n",
      "Iteration 202, loss = 0.18621052\n",
      "Iteration 203, loss = 0.18575156\n",
      "Iteration 204, loss = 0.18498342\n",
      "Iteration 205, loss = 0.18445173\n",
      "Iteration 206, loss = 0.18387741\n",
      "Iteration 207, loss = 0.18326028\n",
      "Iteration 208, loss = 0.18282850\n",
      "Iteration 209, loss = 0.18235987\n",
      "Iteration 210, loss = 0.18159020\n",
      "Iteration 211, loss = 0.18104831\n",
      "Iteration 212, loss = 0.18035599\n",
      "Iteration 213, loss = 0.18000135\n",
      "Iteration 214, loss = 0.17938589\n",
      "Iteration 215, loss = 0.17891715\n",
      "Iteration 216, loss = 0.17823421\n",
      "Iteration 217, loss = 0.17768978\n",
      "Iteration 218, loss = 0.17713578\n",
      "Iteration 219, loss = 0.17636045\n",
      "Iteration 220, loss = 0.17605698\n",
      "Iteration 221, loss = 0.17536518\n",
      "Iteration 222, loss = 0.17475607\n",
      "Iteration 223, loss = 0.17433084\n",
      "Iteration 224, loss = 0.17389982\n",
      "Iteration 225, loss = 0.17324653\n",
      "Iteration 226, loss = 0.17280700\n",
      "Iteration 227, loss = 0.17223907\n",
      "Iteration 228, loss = 0.17180207\n",
      "Iteration 229, loss = 0.17131523\n",
      "Iteration 230, loss = 0.17074317\n",
      "Iteration 231, loss = 0.17015458\n",
      "Iteration 232, loss = 0.16959843\n",
      "Iteration 233, loss = 0.16921422\n",
      "Iteration 234, loss = 0.16878438\n",
      "Iteration 235, loss = 0.16834000\n",
      "Iteration 236, loss = 0.16762610\n",
      "Iteration 237, loss = 0.16714503\n",
      "Iteration 238, loss = 0.16684233\n",
      "Iteration 239, loss = 0.16628178\n",
      "Iteration 240, loss = 0.16580204\n",
      "Iteration 241, loss = 0.16526467\n",
      "Iteration 242, loss = 0.16475118\n",
      "Iteration 243, loss = 0.16441124\n",
      "Iteration 244, loss = 0.16391662\n",
      "Iteration 245, loss = 0.16342854\n",
      "Iteration 246, loss = 0.16302602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 247, loss = 0.16263409\n",
      "Iteration 248, loss = 0.16222307\n",
      "Iteration 249, loss = 0.16172039\n",
      "Iteration 250, loss = 0.16139577\n",
      "Iteration 251, loss = 0.16111591\n",
      "Iteration 252, loss = 0.16031617\n",
      "Iteration 253, loss = 0.16008683\n",
      "Iteration 254, loss = 0.15970721\n",
      "Iteration 255, loss = 0.15925923\n",
      "Iteration 256, loss = 0.15856781\n",
      "Iteration 257, loss = 0.15820792\n",
      "Iteration 258, loss = 0.15769435\n",
      "Iteration 259, loss = 0.15743256\n",
      "Iteration 260, loss = 0.15701308\n",
      "Iteration 261, loss = 0.15657352\n",
      "Iteration 262, loss = 0.15622484\n",
      "Iteration 263, loss = 0.15560591\n",
      "Iteration 264, loss = 0.15535199\n",
      "Iteration 265, loss = 0.15510555\n",
      "Iteration 266, loss = 0.15462030\n",
      "Iteration 267, loss = 0.15439686\n",
      "Iteration 268, loss = 0.15396423\n",
      "Iteration 269, loss = 0.15342389\n",
      "Iteration 270, loss = 0.15317348\n",
      "Iteration 271, loss = 0.15269690\n",
      "Iteration 272, loss = 0.15257866\n",
      "Iteration 273, loss = 0.15210066\n",
      "Iteration 274, loss = 0.15171187\n",
      "Iteration 275, loss = 0.15120671\n",
      "Iteration 276, loss = 0.15079383\n",
      "Iteration 277, loss = 0.15074093\n",
      "Iteration 278, loss = 0.15015530\n",
      "Iteration 279, loss = 0.15018018\n",
      "Iteration 280, loss = 0.14952303\n",
      "Iteration 281, loss = 0.14922613\n",
      "Iteration 282, loss = 0.14887247\n",
      "Iteration 283, loss = 0.14847836\n",
      "Iteration 284, loss = 0.14816906\n",
      "Iteration 285, loss = 0.14783225\n",
      "Iteration 286, loss = 0.14756653\n",
      "Iteration 287, loss = 0.14706963\n",
      "Iteration 288, loss = 0.14680018\n",
      "Iteration 289, loss = 0.14643383\n",
      "Iteration 290, loss = 0.14602532\n",
      "Iteration 291, loss = 0.14597705\n",
      "Iteration 292, loss = 0.14583091\n",
      "Iteration 293, loss = 0.14517471\n",
      "Iteration 294, loss = 0.14513764\n",
      "Iteration 295, loss = 0.14465125\n",
      "Iteration 296, loss = 0.14418958\n",
      "Iteration 297, loss = 0.14400933\n",
      "Iteration 298, loss = 0.14356047\n",
      "Iteration 299, loss = 0.14339912\n",
      "Iteration 300, loss = 0.14302345\n",
      "Iteration 301, loss = 0.14287917\n",
      "Iteration 302, loss = 0.14237741\n",
      "Iteration 303, loss = 0.14207012\n",
      "Iteration 304, loss = 0.14173928\n",
      "Iteration 305, loss = 0.14150524\n",
      "Iteration 306, loss = 0.14125567\n",
      "Iteration 307, loss = 0.14085321\n",
      "Iteration 308, loss = 0.14058946\n",
      "Iteration 309, loss = 0.14033600\n",
      "Iteration 310, loss = 0.13989590\n",
      "Iteration 311, loss = 0.13984387\n",
      "Iteration 312, loss = 0.13942592\n",
      "Iteration 313, loss = 0.13917072\n",
      "Iteration 314, loss = 0.13870073\n",
      "Iteration 315, loss = 0.13893662\n",
      "Iteration 316, loss = 0.13822110\n",
      "Iteration 317, loss = 0.13806522\n",
      "Iteration 318, loss = 0.13763848\n",
      "Iteration 319, loss = 0.13748119\n",
      "Iteration 320, loss = 0.13706041\n",
      "Iteration 321, loss = 0.13705625\n",
      "Iteration 322, loss = 0.13661930\n",
      "Iteration 323, loss = 0.13645053\n",
      "Iteration 324, loss = 0.13593861\n",
      "Iteration 325, loss = 0.13568590\n",
      "Iteration 326, loss = 0.13546202\n",
      "Iteration 327, loss = 0.13510285\n",
      "Iteration 328, loss = 0.13529650\n",
      "Iteration 329, loss = 0.13479758\n",
      "Iteration 330, loss = 0.13435965\n",
      "Iteration 331, loss = 0.13432535\n",
      "Iteration 332, loss = 0.13387761\n",
      "Iteration 333, loss = 0.13366971\n",
      "Iteration 334, loss = 0.13350534\n",
      "Iteration 335, loss = 0.13312018\n",
      "Iteration 336, loss = 0.13291471\n",
      "Iteration 337, loss = 0.13263010\n",
      "Iteration 338, loss = 0.13225520\n",
      "Iteration 339, loss = 0.13222403\n",
      "Iteration 340, loss = 0.13189835\n",
      "Iteration 341, loss = 0.13192638\n",
      "Iteration 342, loss = 0.13142796\n",
      "Iteration 343, loss = 0.13141514\n",
      "Iteration 344, loss = 0.13081510\n",
      "Iteration 345, loss = 0.13068575\n",
      "Iteration 346, loss = 0.13059841\n",
      "Iteration 347, loss = 0.13016103\n",
      "Iteration 348, loss = 0.13023177\n",
      "Iteration 349, loss = 0.12981653\n",
      "Iteration 350, loss = 0.12939544\n",
      "Iteration 351, loss = 0.12939026\n",
      "Iteration 352, loss = 0.12940079\n",
      "Iteration 353, loss = 0.12918866\n",
      "Iteration 354, loss = 0.12868653\n",
      "Iteration 355, loss = 0.12833382\n",
      "Iteration 356, loss = 0.12826486\n",
      "Iteration 357, loss = 0.12811805\n",
      "Iteration 358, loss = 0.12766514\n",
      "Iteration 359, loss = 0.12751778\n",
      "Iteration 360, loss = 0.12717316\n",
      "Iteration 361, loss = 0.12714428\n",
      "Iteration 362, loss = 0.12690530\n",
      "Iteration 363, loss = 0.12691768\n",
      "Iteration 364, loss = 0.12638343\n",
      "Iteration 365, loss = 0.12618351\n",
      "Iteration 366, loss = 0.12599297\n",
      "Iteration 367, loss = 0.12583440\n",
      "Iteration 368, loss = 0.12558682\n",
      "Iteration 369, loss = 0.12529275\n",
      "Iteration 370, loss = 0.12551787\n",
      "Iteration 371, loss = 0.12497245\n",
      "Iteration 372, loss = 0.12485931\n",
      "Iteration 373, loss = 0.12467525\n",
      "Iteration 374, loss = 0.12454597\n",
      "Iteration 375, loss = 0.12407814\n",
      "Iteration 376, loss = 0.12379856\n",
      "Iteration 377, loss = 0.12382033\n",
      "Iteration 378, loss = 0.12364665\n",
      "Iteration 379, loss = 0.12311326\n",
      "Iteration 380, loss = 0.12317720\n",
      "Iteration 381, loss = 0.12297157\n",
      "Iteration 382, loss = 0.12266438\n",
      "Iteration 383, loss = 0.12241316\n",
      "Iteration 384, loss = 0.12226187\n",
      "Iteration 385, loss = 0.12188136\n",
      "Iteration 386, loss = 0.12214071\n",
      "Iteration 387, loss = 0.12165914\n",
      "Iteration 388, loss = 0.12130152\n",
      "Iteration 389, loss = 0.12129992\n",
      "Iteration 390, loss = 0.12096333\n",
      "Iteration 391, loss = 0.12094814\n",
      "Iteration 392, loss = 0.12093845\n",
      "Iteration 393, loss = 0.12022993\n",
      "Iteration 394, loss = 0.12048748\n",
      "Iteration 395, loss = 0.12029611\n",
      "Iteration 396, loss = 0.11994825\n",
      "Iteration 397, loss = 0.11984352\n",
      "Iteration 398, loss = 0.11942019\n",
      "Iteration 399, loss = 0.11941343\n",
      "Iteration 400, loss = 0.11908838\n",
      "Iteration 401, loss = 0.11910325\n",
      "Iteration 402, loss = 0.11898131\n",
      "Iteration 403, loss = 0.11871936\n",
      "Iteration 404, loss = 0.11855710\n",
      "Iteration 405, loss = 0.11861324\n",
      "Iteration 406, loss = 0.11788546\n",
      "Iteration 407, loss = 0.11815207\n",
      "Iteration 408, loss = 0.11787579\n",
      "Iteration 409, loss = 0.11749532\n",
      "Iteration 410, loss = 0.11767339\n",
      "Iteration 411, loss = 0.11726277\n",
      "Iteration 412, loss = 0.11715990\n",
      "Iteration 413, loss = 0.11682247\n",
      "Iteration 414, loss = 0.11663836\n",
      "Iteration 415, loss = 0.11662270\n",
      "Iteration 416, loss = 0.11667590\n",
      "Iteration 417, loss = 0.11663382\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49885881\n",
      "Iteration 2, loss = 0.46558774\n",
      "Iteration 3, loss = 0.44377242\n",
      "Iteration 4, loss = 0.42949474\n",
      "Iteration 5, loss = 0.41953647\n",
      "Iteration 6, loss = 0.41224865\n",
      "Iteration 7, loss = 0.40659977\n",
      "Iteration 8, loss = 0.40175696\n",
      "Iteration 9, loss = 0.39753643\n",
      "Iteration 10, loss = 0.39383515\n",
      "Iteration 11, loss = 0.39022063\n",
      "Iteration 12, loss = 0.38692225\n",
      "Iteration 13, loss = 0.38366689\n",
      "Iteration 14, loss = 0.38056038\n",
      "Iteration 15, loss = 0.37769611\n",
      "Iteration 16, loss = 0.37478714\n",
      "Iteration 17, loss = 0.37202952\n",
      "Iteration 18, loss = 0.36932793\n",
      "Iteration 19, loss = 0.36675361\n",
      "Iteration 20, loss = 0.36420333\n",
      "Iteration 21, loss = 0.36179265\n",
      "Iteration 22, loss = 0.35942982\n",
      "Iteration 23, loss = 0.35716262\n",
      "Iteration 24, loss = 0.35491441\n",
      "Iteration 25, loss = 0.35274253\n",
      "Iteration 26, loss = 0.35072412\n",
      "Iteration 27, loss = 0.34880578\n",
      "Iteration 28, loss = 0.34669265\n",
      "Iteration 29, loss = 0.34490324\n",
      "Iteration 30, loss = 0.34299982\n",
      "Iteration 31, loss = 0.34124755\n",
      "Iteration 32, loss = 0.33945339\n",
      "Iteration 33, loss = 0.33777669\n",
      "Iteration 34, loss = 0.33618990\n",
      "Iteration 35, loss = 0.33453824\n",
      "Iteration 36, loss = 0.33309112\n",
      "Iteration 37, loss = 0.33157946\n",
      "Iteration 38, loss = 0.33012728\n",
      "Iteration 39, loss = 0.32870560\n",
      "Iteration 40, loss = 0.32735703\n",
      "Iteration 41, loss = 0.32593537\n",
      "Iteration 42, loss = 0.32486175\n",
      "Iteration 43, loss = 0.32350912\n",
      "Iteration 44, loss = 0.32215283\n",
      "Iteration 45, loss = 0.32084740\n",
      "Iteration 46, loss = 0.31972526\n",
      "Iteration 47, loss = 0.31860744\n",
      "Iteration 48, loss = 0.31740871\n",
      "Iteration 49, loss = 0.31622725\n",
      "Iteration 50, loss = 0.31516928\n",
      "Iteration 51, loss = 0.31390885\n",
      "Iteration 52, loss = 0.31298570\n",
      "Iteration 53, loss = 0.31193280\n",
      "Iteration 54, loss = 0.31087594\n",
      "Iteration 55, loss = 0.30966157\n",
      "Iteration 56, loss = 0.30865260\n",
      "Iteration 57, loss = 0.30780832\n",
      "Iteration 58, loss = 0.30674065\n",
      "Iteration 59, loss = 0.30577032\n",
      "Iteration 60, loss = 0.30473898\n",
      "Iteration 61, loss = 0.30382440\n",
      "Iteration 62, loss = 0.30273937\n",
      "Iteration 63, loss = 0.30180955\n",
      "Iteration 64, loss = 0.30097854\n",
      "Iteration 65, loss = 0.29992279\n",
      "Iteration 66, loss = 0.29885872\n",
      "Iteration 67, loss = 0.29816610\n",
      "Iteration 68, loss = 0.29706032\n",
      "Iteration 69, loss = 0.29616385\n",
      "Iteration 70, loss = 0.29530319\n",
      "Iteration 71, loss = 0.29429759\n",
      "Iteration 72, loss = 0.29351599\n",
      "Iteration 73, loss = 0.29249526\n",
      "Iteration 74, loss = 0.29154418\n",
      "Iteration 75, loss = 0.29070389\n",
      "Iteration 76, loss = 0.28997970\n",
      "Iteration 77, loss = 0.28885230\n",
      "Iteration 78, loss = 0.28814321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79, loss = 0.28697091\n",
      "Iteration 80, loss = 0.28604057\n",
      "Iteration 81, loss = 0.28529350\n",
      "Iteration 82, loss = 0.28428370\n",
      "Iteration 83, loss = 0.28331902\n",
      "Iteration 84, loss = 0.28242872\n",
      "Iteration 85, loss = 0.28149765\n",
      "Iteration 86, loss = 0.28073114\n",
      "Iteration 87, loss = 0.27983332\n",
      "Iteration 88, loss = 0.27872335\n",
      "Iteration 89, loss = 0.27785021\n",
      "Iteration 90, loss = 0.27679866\n",
      "Iteration 91, loss = 0.27604854\n",
      "Iteration 92, loss = 0.27507165\n",
      "Iteration 93, loss = 0.27420604\n",
      "Iteration 94, loss = 0.27323133\n",
      "Iteration 95, loss = 0.27233272\n",
      "Iteration 96, loss = 0.27137413\n",
      "Iteration 97, loss = 0.27043993\n",
      "Iteration 98, loss = 0.26964630\n",
      "Iteration 99, loss = 0.26873250\n",
      "Iteration 100, loss = 0.26772687\n",
      "Iteration 101, loss = 0.26678587\n",
      "Iteration 102, loss = 0.26597300\n",
      "Iteration 103, loss = 0.26500450\n",
      "Iteration 104, loss = 0.26408033\n",
      "Iteration 105, loss = 0.26324890\n",
      "Iteration 106, loss = 0.26234988\n",
      "Iteration 107, loss = 0.26138609\n",
      "Iteration 108, loss = 0.26047024\n",
      "Iteration 109, loss = 0.25959312\n",
      "Iteration 110, loss = 0.25878238\n",
      "Iteration 111, loss = 0.25786924\n",
      "Iteration 112, loss = 0.25695526\n",
      "Iteration 113, loss = 0.25606001\n",
      "Iteration 114, loss = 0.25514543\n",
      "Iteration 115, loss = 0.25437362\n",
      "Iteration 116, loss = 0.25353096\n",
      "Iteration 117, loss = 0.25267909\n",
      "Iteration 118, loss = 0.25165387\n",
      "Iteration 119, loss = 0.25093389\n",
      "Iteration 120, loss = 0.25006071\n",
      "Iteration 121, loss = 0.24919635\n",
      "Iteration 122, loss = 0.24827057\n",
      "Iteration 123, loss = 0.24755183\n",
      "Iteration 124, loss = 0.24676984\n",
      "Iteration 125, loss = 0.24584603\n",
      "Iteration 126, loss = 0.24497017\n",
      "Iteration 127, loss = 0.24415294\n",
      "Iteration 128, loss = 0.24347355\n",
      "Iteration 129, loss = 0.24286024\n",
      "Iteration 130, loss = 0.24166980\n",
      "Iteration 131, loss = 0.24096215\n",
      "Iteration 132, loss = 0.24005285\n",
      "Iteration 133, loss = 0.23930441\n",
      "Iteration 134, loss = 0.23841869\n",
      "Iteration 135, loss = 0.23771380\n",
      "Iteration 136, loss = 0.23695098\n",
      "Iteration 137, loss = 0.23628655\n",
      "Iteration 138, loss = 0.23529621\n",
      "Iteration 139, loss = 0.23461020\n",
      "Iteration 140, loss = 0.23363494\n",
      "Iteration 141, loss = 0.23290379\n",
      "Iteration 142, loss = 0.23221376\n",
      "Iteration 143, loss = 0.23146776\n",
      "Iteration 144, loss = 0.23091314\n",
      "Iteration 145, loss = 0.23006446\n",
      "Iteration 146, loss = 0.22932883\n",
      "Iteration 147, loss = 0.22826605\n",
      "Iteration 148, loss = 0.22767884\n",
      "Iteration 149, loss = 0.22695957\n",
      "Iteration 150, loss = 0.22633034\n",
      "Iteration 151, loss = 0.22558295\n",
      "Iteration 152, loss = 0.22467151\n",
      "Iteration 153, loss = 0.22396912\n",
      "Iteration 154, loss = 0.22333806\n",
      "Iteration 155, loss = 0.22274591\n",
      "Iteration 156, loss = 0.22190913\n",
      "Iteration 157, loss = 0.22108203\n",
      "Iteration 158, loss = 0.22025665\n",
      "Iteration 159, loss = 0.21960334\n",
      "Iteration 160, loss = 0.21884278\n",
      "Iteration 161, loss = 0.21814861\n",
      "Iteration 162, loss = 0.21742287\n",
      "Iteration 163, loss = 0.21671944\n",
      "Iteration 164, loss = 0.21598220\n",
      "Iteration 165, loss = 0.21517598\n",
      "Iteration 166, loss = 0.21486989\n",
      "Iteration 167, loss = 0.21367475\n",
      "Iteration 168, loss = 0.21310084\n",
      "Iteration 169, loss = 0.21235333\n",
      "Iteration 170, loss = 0.21192395\n",
      "Iteration 171, loss = 0.21106371\n",
      "Iteration 172, loss = 0.21049928\n",
      "Iteration 173, loss = 0.20966937\n",
      "Iteration 174, loss = 0.20902023\n",
      "Iteration 175, loss = 0.20832315\n",
      "Iteration 176, loss = 0.20748897\n",
      "Iteration 177, loss = 0.20707530\n",
      "Iteration 178, loss = 0.20617452\n",
      "Iteration 179, loss = 0.20553790\n",
      "Iteration 180, loss = 0.20482291\n",
      "Iteration 181, loss = 0.20426347\n",
      "Iteration 182, loss = 0.20360713\n",
      "Iteration 183, loss = 0.20309493\n",
      "Iteration 184, loss = 0.20240743\n",
      "Iteration 185, loss = 0.20164004\n",
      "Iteration 186, loss = 0.20134429\n",
      "Iteration 187, loss = 0.20051168\n",
      "Iteration 188, loss = 0.19966122\n",
      "Iteration 189, loss = 0.19908776\n",
      "Iteration 190, loss = 0.19841784\n",
      "Iteration 191, loss = 0.19774964\n",
      "Iteration 192, loss = 0.19707801\n",
      "Iteration 193, loss = 0.19665795\n",
      "Iteration 194, loss = 0.19579862\n",
      "Iteration 195, loss = 0.19504565\n",
      "Iteration 196, loss = 0.19481313\n",
      "Iteration 197, loss = 0.19408327\n",
      "Iteration 198, loss = 0.19331848\n",
      "Iteration 199, loss = 0.19284562\n",
      "Iteration 200, loss = 0.19231906\n",
      "Iteration 201, loss = 0.19145476\n",
      "Iteration 202, loss = 0.19090811\n",
      "Iteration 203, loss = 0.19044953\n",
      "Iteration 204, loss = 0.18972457\n",
      "Iteration 205, loss = 0.18909381\n",
      "Iteration 206, loss = 0.18861312\n",
      "Iteration 207, loss = 0.18797072\n",
      "Iteration 208, loss = 0.18737168\n",
      "Iteration 209, loss = 0.18683632\n",
      "Iteration 210, loss = 0.18630402\n",
      "Iteration 211, loss = 0.18570168\n",
      "Iteration 212, loss = 0.18511963\n",
      "Iteration 213, loss = 0.18471097\n",
      "Iteration 214, loss = 0.18397615\n",
      "Iteration 215, loss = 0.18354011\n",
      "Iteration 216, loss = 0.18284721\n",
      "Iteration 217, loss = 0.18253973\n",
      "Iteration 218, loss = 0.18181104\n",
      "Iteration 219, loss = 0.18100853\n",
      "Iteration 220, loss = 0.18069512\n",
      "Iteration 221, loss = 0.18021962\n",
      "Iteration 222, loss = 0.17940784\n",
      "Iteration 223, loss = 0.17901369\n",
      "Iteration 224, loss = 0.17859597\n",
      "Iteration 225, loss = 0.17797046\n",
      "Iteration 226, loss = 0.17737919\n",
      "Iteration 227, loss = 0.17687131\n",
      "Iteration 228, loss = 0.17650989\n",
      "Iteration 229, loss = 0.17592037\n",
      "Iteration 230, loss = 0.17537144\n",
      "Iteration 231, loss = 0.17494676\n",
      "Iteration 232, loss = 0.17416116\n",
      "Iteration 233, loss = 0.17368769\n",
      "Iteration 234, loss = 0.17323190\n",
      "Iteration 235, loss = 0.17280353\n",
      "Iteration 236, loss = 0.17220560\n",
      "Iteration 237, loss = 0.17169743\n",
      "Iteration 238, loss = 0.17137602\n",
      "Iteration 239, loss = 0.17086969\n",
      "Iteration 240, loss = 0.17058837\n",
      "Iteration 241, loss = 0.16988982\n",
      "Iteration 242, loss = 0.16949327\n",
      "Iteration 243, loss = 0.16894421\n",
      "Iteration 244, loss = 0.16849362\n",
      "Iteration 245, loss = 0.16791678\n",
      "Iteration 246, loss = 0.16761598\n",
      "Iteration 247, loss = 0.16697196\n",
      "Iteration 248, loss = 0.16652819\n",
      "Iteration 249, loss = 0.16605393\n",
      "Iteration 250, loss = 0.16577309\n",
      "Iteration 251, loss = 0.16540586\n",
      "Iteration 252, loss = 0.16474513\n",
      "Iteration 253, loss = 0.16447474\n",
      "Iteration 254, loss = 0.16390410\n",
      "Iteration 255, loss = 0.16353823\n",
      "Iteration 256, loss = 0.16295711\n",
      "Iteration 257, loss = 0.16242885\n",
      "Iteration 258, loss = 0.16198575\n",
      "Iteration 259, loss = 0.16169280\n",
      "Iteration 260, loss = 0.16112878\n",
      "Iteration 261, loss = 0.16067045\n",
      "Iteration 262, loss = 0.16053547\n",
      "Iteration 263, loss = 0.15978598\n",
      "Iteration 264, loss = 0.15948988\n",
      "Iteration 265, loss = 0.15915585\n",
      "Iteration 266, loss = 0.15879519\n",
      "Iteration 267, loss = 0.15835762\n",
      "Iteration 268, loss = 0.15797425\n",
      "Iteration 269, loss = 0.15748998\n",
      "Iteration 270, loss = 0.15715986\n",
      "Iteration 271, loss = 0.15674165\n",
      "Iteration 272, loss = 0.15651758\n",
      "Iteration 273, loss = 0.15619889\n",
      "Iteration 274, loss = 0.15566481\n",
      "Iteration 275, loss = 0.15527067\n",
      "Iteration 276, loss = 0.15460395\n",
      "Iteration 277, loss = 0.15440693\n",
      "Iteration 278, loss = 0.15404612\n",
      "Iteration 279, loss = 0.15392067\n",
      "Iteration 280, loss = 0.15345973\n",
      "Iteration 281, loss = 0.15297212\n",
      "Iteration 282, loss = 0.15251618\n",
      "Iteration 283, loss = 0.15221701\n",
      "Iteration 284, loss = 0.15182775\n",
      "Iteration 285, loss = 0.15155948\n",
      "Iteration 286, loss = 0.15127956\n",
      "Iteration 287, loss = 0.15074068\n",
      "Iteration 288, loss = 0.15048879\n",
      "Iteration 289, loss = 0.15005922\n",
      "Iteration 290, loss = 0.14976525\n",
      "Iteration 291, loss = 0.14939030\n",
      "Iteration 292, loss = 0.14913404\n",
      "Iteration 293, loss = 0.14888154\n",
      "Iteration 294, loss = 0.14864096\n",
      "Iteration 295, loss = 0.14790753\n",
      "Iteration 296, loss = 0.14784473\n",
      "Iteration 297, loss = 0.14761424\n",
      "Iteration 298, loss = 0.14703098\n",
      "Iteration 299, loss = 0.14683521\n",
      "Iteration 300, loss = 0.14639991\n",
      "Iteration 301, loss = 0.14631177\n",
      "Iteration 302, loss = 0.14581693\n",
      "Iteration 303, loss = 0.14551081\n",
      "Iteration 304, loss = 0.14511637\n",
      "Iteration 305, loss = 0.14505405\n",
      "Iteration 306, loss = 0.14468079\n",
      "Iteration 307, loss = 0.14416402\n",
      "Iteration 308, loss = 0.14405042\n",
      "Iteration 309, loss = 0.14361223\n",
      "Iteration 310, loss = 0.14316570\n",
      "Iteration 311, loss = 0.14304254\n",
      "Iteration 312, loss = 0.14279251\n",
      "Iteration 313, loss = 0.14232808\n",
      "Iteration 314, loss = 0.14218179\n",
      "Iteration 315, loss = 0.14186109\n",
      "Iteration 316, loss = 0.14151750\n",
      "Iteration 317, loss = 0.14122011\n",
      "Iteration 318, loss = 0.14099829\n",
      "Iteration 319, loss = 0.14059299\n",
      "Iteration 320, loss = 0.14066377\n",
      "Iteration 321, loss = 0.14015485\n",
      "Iteration 322, loss = 0.13991637\n",
      "Iteration 323, loss = 0.13945015\n",
      "Iteration 324, loss = 0.13933668\n",
      "Iteration 325, loss = 0.13891653\n",
      "Iteration 326, loss = 0.13856685\n",
      "Iteration 327, loss = 0.13818378\n",
      "Iteration 328, loss = 0.13879855\n",
      "Iteration 329, loss = 0.13792146\n",
      "Iteration 330, loss = 0.13764527\n",
      "Iteration 331, loss = 0.13759146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 332, loss = 0.13697854\n",
      "Iteration 333, loss = 0.13694013\n",
      "Iteration 334, loss = 0.13649618\n",
      "Iteration 335, loss = 0.13622628\n",
      "Iteration 336, loss = 0.13599644\n",
      "Iteration 337, loss = 0.13575759\n",
      "Iteration 338, loss = 0.13539308\n",
      "Iteration 339, loss = 0.13533028\n",
      "Iteration 340, loss = 0.13506638\n",
      "Iteration 341, loss = 0.13486567\n",
      "Iteration 342, loss = 0.13455576\n",
      "Iteration 343, loss = 0.13427724\n",
      "Iteration 344, loss = 0.13379626\n",
      "Iteration 345, loss = 0.13387466\n",
      "Iteration 346, loss = 0.13364902\n",
      "Iteration 347, loss = 0.13317868\n",
      "Iteration 348, loss = 0.13327351\n",
      "Iteration 349, loss = 0.13300120\n",
      "Iteration 350, loss = 0.13250078\n",
      "Iteration 351, loss = 0.13228998\n",
      "Iteration 352, loss = 0.13220987\n",
      "Iteration 353, loss = 0.13218584\n",
      "Iteration 354, loss = 0.13173188\n",
      "Iteration 355, loss = 0.13146952\n",
      "Iteration 356, loss = 0.13118952\n",
      "Iteration 357, loss = 0.13103187\n",
      "Iteration 358, loss = 0.13079713\n",
      "Iteration 359, loss = 0.13062203\n",
      "Iteration 360, loss = 0.13035107\n",
      "Iteration 361, loss = 0.13029375\n",
      "Iteration 362, loss = 0.12999931\n",
      "Iteration 363, loss = 0.13000630\n",
      "Iteration 364, loss = 0.12953360\n",
      "Iteration 365, loss = 0.12942835\n",
      "Iteration 366, loss = 0.12936913\n",
      "Iteration 367, loss = 0.12889180\n",
      "Iteration 368, loss = 0.12847309\n",
      "Iteration 369, loss = 0.12857456\n",
      "Iteration 370, loss = 0.12854296\n",
      "Iteration 371, loss = 0.12804993\n",
      "Iteration 372, loss = 0.12813262\n",
      "Iteration 373, loss = 0.12785725\n",
      "Iteration 374, loss = 0.12772764\n",
      "Iteration 375, loss = 0.12733725\n",
      "Iteration 376, loss = 0.12695771\n",
      "Iteration 377, loss = 0.12702205\n",
      "Iteration 378, loss = 0.12672944\n",
      "Iteration 379, loss = 0.12649575\n",
      "Iteration 380, loss = 0.12644567\n",
      "Iteration 381, loss = 0.12621586\n",
      "Iteration 382, loss = 0.12617335\n",
      "Iteration 383, loss = 0.12590636\n",
      "Iteration 384, loss = 0.12578537\n",
      "Iteration 385, loss = 0.12538326\n",
      "Iteration 386, loss = 0.12527836\n",
      "Iteration 387, loss = 0.12502898\n",
      "Iteration 388, loss = 0.12464814\n",
      "Iteration 389, loss = 0.12467046\n",
      "Iteration 390, loss = 0.12433326\n",
      "Iteration 391, loss = 0.12424518\n",
      "Iteration 392, loss = 0.12462114\n",
      "Iteration 393, loss = 0.12395830\n",
      "Iteration 394, loss = 0.12397731\n",
      "Iteration 395, loss = 0.12358758\n",
      "Iteration 396, loss = 0.12365904\n",
      "Iteration 397, loss = 0.12335613\n",
      "Iteration 398, loss = 0.12307633\n",
      "Iteration 399, loss = 0.12338521\n",
      "Iteration 400, loss = 0.12270076\n",
      "Iteration 401, loss = 0.12255821\n",
      "Iteration 402, loss = 0.12233579\n",
      "Iteration 403, loss = 0.12248069\n",
      "Iteration 404, loss = 0.12224091\n",
      "Iteration 405, loss = 0.12203413\n",
      "Iteration 406, loss = 0.12166905\n",
      "Iteration 407, loss = 0.12149885\n",
      "Iteration 408, loss = 0.12173105\n",
      "Iteration 409, loss = 0.12129273\n",
      "Iteration 410, loss = 0.12142740\n",
      "Iteration 411, loss = 0.12093087\n",
      "Iteration 412, loss = 0.12082318\n",
      "Iteration 413, loss = 0.12054100\n",
      "Iteration 414, loss = 0.12036603\n",
      "Iteration 415, loss = 0.12027596\n",
      "Iteration 416, loss = 0.12036523\n",
      "Iteration 417, loss = 0.12006715\n",
      "Iteration 418, loss = 0.11995738\n",
      "Iteration 419, loss = 0.11994990\n",
      "Iteration 420, loss = 0.11971224\n",
      "Iteration 421, loss = 0.11933349\n",
      "Iteration 422, loss = 0.11933352\n",
      "Iteration 423, loss = 0.11893697\n",
      "Iteration 424, loss = 0.11890593\n",
      "Iteration 425, loss = 0.11889861\n",
      "Iteration 426, loss = 0.11854313\n",
      "Iteration 427, loss = 0.11835619\n",
      "Iteration 428, loss = 0.11837928\n",
      "Iteration 429, loss = 0.11809583\n",
      "Iteration 430, loss = 0.11827544\n",
      "Iteration 431, loss = 0.11801453\n",
      "Iteration 432, loss = 0.11778187\n",
      "Iteration 433, loss = 0.11763608\n",
      "Iteration 434, loss = 0.11761058\n",
      "Iteration 435, loss = 0.11741849\n",
      "Iteration 436, loss = 0.11741300\n",
      "Iteration 437, loss = 0.11745470\n",
      "Iteration 438, loss = 0.11700911\n",
      "Iteration 439, loss = 0.11691268\n",
      "Iteration 440, loss = 0.11688018\n",
      "Iteration 441, loss = 0.11667945\n",
      "Iteration 442, loss = 0.11635521\n",
      "Iteration 443, loss = 0.11655413\n",
      "Iteration 444, loss = 0.11630349\n",
      "Iteration 445, loss = 0.11596274\n",
      "Iteration 446, loss = 0.11586774\n",
      "Iteration 447, loss = 0.11586866\n",
      "Iteration 448, loss = 0.11581019\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[0.69529696 0.68323394 0.6818702  0.68295741 0.72217451]\n"
     ]
    }
   ],
   "source": [
    "# sgd optimizer \n",
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp_regress = MLPRegressor(solver='sgd', activation='relu',alpha=1e-4, hidden_layer_sizes=(128, 128),\n",
    "                    random_state=1, max_iter=1000,verbose=True)\n",
    "\n",
    "# lbfgs - very slow \n",
    "# mlp = MLPClassifier(solver='lbfgs', activation='relu',alpha=1e-4,hidden_layer_sizes=(100,100),\n",
    "#                     random_state=1,max_iter=50,verbose=10,learning_rate_init=.1)\n",
    "\n",
    "# adam \n",
    "# mlp = MLPClassifier(solver='adam', activation='relu',alpha=1e-4,hidden_layer_sizes=(100,100),\n",
    "#                     random_state=1,max_iter=50,verbose=10,learning_rate_init=.1)\n",
    "\n",
    "# mlp.fit(X_scale, output_scikit) \n",
    "# print(mlp.classes_)\n",
    "# print(mlp.n_layers_)\n",
    "# print(mlp.n_iter_)\n",
    "# print(mlp.loss_)\n",
    "# print(mlp.out_activation_)\n",
    "# print(mlp.n_outputs_)\n",
    "X_Re = preprocessing.scale(datasets)\n",
    "Y_Re = preprocessing.scale(results_map)\n",
    "\n",
    "score_regress = cross_val_score(mlp_regress, X_Re, Y_Re, cv=5)  \n",
    "print(score_regress)\n",
    "# [0.97351663 0.97390639 0.97552892 0.97428681 0.97503282] for 500000 data with [128,128]\n",
    "# [0.85165701 0.83106467 0.82760219 0.86254636 0.8405507 ] for 50000 data \n",
    "# [0.69529696 0.68323394 0.6818702  0.68295741 0.72217451] for 10000 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
