{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import*\n",
    "import __main__\n",
    "global PI\n",
    "import os\n",
    "import time\n",
    "PI=float(acos(-1))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "rs_number = 123\n",
    "random.seed(rs_number)\n",
    "\n",
    "data_size = 1000;\n",
    "\n",
    "random_selection = 0\n",
    "\n",
    "if random_selection==0:\n",
    "    ''' random selection'''\n",
    "#     mu = [0,0]\n",
    "#     cov = [[1, 0], [0, 1]]\n",
    "    datasets = np.random.multivariate_normal(mu, cov, data_size)\n",
    "#     datasets = np.random.uniform(0,1,data_size)\n",
    "\n",
    "else:\n",
    "    '''latin hypercube sampling'''\n",
    "\n",
    "# print(np.shape(datasets)) \n",
    "# plt.plot(datasets[:,0],datasets[:,1],'o')\n",
    "\n",
    "def ssh(X):\n",
    "    g = 4-X[0]/4 + sin(4*X[0])-X[1]\n",
    "#     g = 0.7*norm.pdf(X,3,1) + 0.3*norm.pdf(X,6,1)\n",
    "#     g = 0.7*norm.ppf(X, loc=3, scale=1) + 0.3*norm.ppf(X, loc=10, scale=1)\n",
    "#     g = lognorm.ppf([0.001, 0.5, 0.999], X)\n",
    "    return g\n",
    "\n",
    "\n",
    "# map async parallel \n",
    "# pool = mp.Pool(mp.cpu_count())\n",
    "pool = mp.Pool(processes = 2)\n",
    "\n",
    "start = time.time()\n",
    "results_map = pool.map(ssh, datasets)\n",
    "end = time.time() \n",
    "print(end - start) #0.0037827491760253906\n",
    "\n",
    "# print(results_map)\n",
    "\n",
    "# # rare event probability\n",
    "# num = 0\n",
    "# for i in range(data_size):\n",
    "#     if results_map[i]<0:\n",
    "#         num = num+1\n",
    "\n",
    "# prob = num/data_size\n",
    "# print(prob)\n",
    "\n",
    "plt.hist(results_map,50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data classification and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_map_sort = sorted(results_map) # note that sorted change the sort but didn't change the original one\n",
    "# print(results_map_sort)\n",
    "median_value = results_map_sort[data_size//2]\n",
    "print(median_value)\n",
    "\n",
    "results_map_new = np.zeros((data_size,1))\n",
    "output_scikit = np.zeros(data_size)\n",
    "\n",
    "for i in range(data_size):\n",
    "    if results_map[i] > median_value:\n",
    "        results_map_new[i,:] = 1\n",
    "        output_scikit[i] = 1\n",
    "    else:\n",
    "        results_map_new[i,:] = 0\n",
    "        output_scikit[i] = 0\n",
    "\n",
    "y_data = output_scikit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training by classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn MLP training \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# preprocessing data scale? X is [1,10]\n",
    "trans_mean = np.mean(datasets)\n",
    "trans_std = np.std(datasets)\n",
    "\n",
    "data_scale = 2\n",
    "if data_scale==0:\n",
    "    X_data = preprocessing.scale(datasets)\n",
    "elif data_scale ==1:\n",
    "    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    X_data = min_max_scaler.fit_transform(datasets)\n",
    "elif data_scale ==2:\n",
    "    scaler = preprocessing.StandardScaler().fit(datasets) # can be used for the following iteration \n",
    "    X_data = scaler.transform(datasets)  \n",
    "\n",
    "# sgd optimizer \n",
    "# mlp = MLPClassifier(solver='sgd', activation='relu',alpha=1e-4, hidden_layer_sizes=(128, 128),\n",
    "#                     random_state=1, max_iter=10000,verbose=True)\n",
    "\n",
    "mlp = MLPClassifier(solver='adam', activation='relu',alpha=1e-4, hidden_layer_sizes=(128, 128),\n",
    "                    random_state=1, max_iter=1000,verbose=True,learning_rate = 'adaptive')\n",
    "\n",
    "# lbfgs - very slow \n",
    "# mlp = MLPClassifier(solver='lbfgs', activation='relu',alpha=1e-4,hidden_layer_sizes=(100,100),\n",
    "#                     random_state=1,max_iter=50,verbose=10,learning_rate_init=.1)\n",
    "# adam \n",
    "# mlp = MLPClassifier(solver='adam', activation='relu',alpha=1e-4,hidden_layer_sizes=(100,100),\n",
    "#                     random_state=1,max_iter=50,verbose=10,learning_rate_init=.1)\n",
    "\n",
    "# testing and training data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2,random_state=rs_number)\n",
    "\n",
    "# training \n",
    "mlp.fit(X_train, y_train) \n",
    "\n",
    "#testing\n",
    "y_pred = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy (train) for %0.1f%% \" % (accuracy * 100))\n",
    "\n",
    "# # cross-validation \n",
    "# score_cv = cross_val_score(mlp, X_data, y_data,cv=5)  \n",
    "# print(score_cv)\n",
    "\n",
    "## tuning hyperparameters \n",
    "\n",
    "# 10000 data, [0.84457771 0.854      0.8385     0.845      0.85992996], default\n",
    "# 10000 data, [0.82908546 0.835      0.8235     0.8185     0.82391196], feature[1,10] normalization 效果一般\n",
    "# 10000 data, [0.86206897 0.8605     0.8375     0.854      0.8154077 ], feature[0,1] normalization 效果一般\n",
    "# 10000 data, [0.84457771 0.854      0.8385     0.845      0.85992996], StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML rank VS FEM rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_index_FEM = np.argsort(results_map)\n",
    "probs = mlp.predict_proba(X_data)\n",
    "sort_index_mlp = np.argsort(probs[:][:,1])\n",
    "\n",
    "sort_FEM = sort_index_FEM.tolist()\n",
    "sort_mlp = sort_index_mlp.tolist()\n",
    "\n",
    "rank_mlp = []\n",
    "for j in range(data_size):\n",
    "    rank_fem = sort_FEM[j]\n",
    "    rank_mlp0 = sort_mlp.index(rank_fem)\n",
    "    rank_mlp.append(rank_mlp0)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(data_size),rank_mlp,'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple loop training but no call FEM data collection \n",
    "\n",
    "loop = 10\n",
    "X_loop = X_data # scale data, 50000\n",
    "X_ori = datasets # orial data, 50000\n",
    "\n",
    "for i in range(loop):\n",
    "    print(i)\n",
    "    # classification probability \n",
    "    probs_all = mlp.predict_proba(X_loop)\n",
    "    sort_index_mlp = np.argsort(probs_all[:][:,1])\n",
    "\n",
    "    # top 10% selection \n",
    "    top_10 = X_loop[sort_index_mlp[int(data_size*0.9):data_size]]\n",
    "    top_10_ori = X_ori[sort_index_mlp[int(data_size*0.9):data_size]]\n",
    "#     print(top_10_ori)\n",
    "\n",
    "    # random selection 90%\n",
    "    if random_selection==0:\n",
    "        ''' random selection'''\n",
    "        data_random = np.random.randint(1, 10, size=[int(data_size*0.9), 8])\n",
    "    else:\n",
    "        '''latin hypercube sampling'''\n",
    "\n",
    "    X_90_scale = scaler.transform(data_random) \n",
    "    X_90_ori = data_random\n",
    "    X_loop = np.concatenate((X_90_scale, top_10), axis=0)\n",
    "    X_ori = np.concatenate((X_90_ori, top_10_ori), axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_test = top_10_ori\n",
    "\n",
    "def ssh_test(num):\n",
    "#     global datasets\n",
    "#     arr = np.random.randint(1, 10, size=[data_size, 8])\n",
    "#     datasets = arr.tolist()\n",
    "    ply_angle = datasets_test[num]\n",
    "    \n",
    "    bas_ply=[-60, -45, -30, -15, 0, 15, 30, 45, 60, 90]\n",
    "\n",
    "    AA1=bas_ply[ply_angle[0]]\n",
    "    AA2=bas_ply[ply_angle[1]]\n",
    "    AA3=bas_ply[ply_angle[2]]\n",
    "    AA4=bas_ply[ply_angle[3]]\n",
    "    AA5=bas_ply[ply_angle[4]]\n",
    "    AA6=bas_ply[ply_angle[5]]\n",
    "    AA7=bas_ply[ply_angle[6]]\n",
    "    AA8=bas_ply[ply_angle[7]]\n",
    "\n",
    "    ### ply stacking sequence###\n",
    "    AAA=[AA1/180.0*PI,AA2/180.0*PI,AA3/180.0*PI,AA4/180.0*PI,AA5/180.0*PI,AA6/180.0*PI,AA7/180.0*PI,AA8/180.0*PI,AA8/180.0*PI,AA7/180.0*PI,AA6/180.0*PI,AA5/180.0*PI,AA4/180.0*PI,AA3/180.0*PI,AA2/180.0*PI,AA1/180.0*PI]\n",
    "    pi=3.14159265358979\n",
    "\n",
    "    R=250.0   ##  radius##\n",
    "    H=510.0   ##  Height##\n",
    "    td=0.125  #layer thickness##\n",
    "\n",
    "    TTT=[-td*8,-td*7,-td*6,-td*5,-td*4,-td*3,-td*2,-td*1,td*0,td*1,td*2,td*3,td*4,td*5,td*6,td*7,td*8]\n",
    "\n",
    "    ###material property###\n",
    "\n",
    "    E1=123550.0  \n",
    "    E2=8707.9\n",
    "    G12=5695.0\n",
    "    miu12=0.31946\n",
    "\n",
    "    miu21=miu12*E2/E1\n",
    "    Q11=E1/(1-miu12*miu21)\n",
    "    Q12=miu21*E1/(1-miu12*miu21)\n",
    "    Q22=E2/(1-miu12*miu21)\n",
    "    Q66=G12\n",
    "\n",
    "    A11=0.0\n",
    "    A12=0.0\n",
    "    A22=0.0\n",
    "    A66=0.0\n",
    "\n",
    "    D11=0.0\n",
    "    D12=0.0\n",
    "    D22=0.0\n",
    "    D66=0.0\n",
    "\n",
    "    for i in range(0,16):\n",
    "        A11=A11+(Q11*cos(AAA[i])**4+2*(Q12+2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q22*sin(AAA[i])**4)*(TTT[i+1]-TTT[i])\n",
    "        A12=A12+((Q11+Q22-4*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q12*(sin(AAA[i])**4+cos(AAA[i])**4))*(TTT[i+1]-TTT[i])\n",
    "        A22=A22+(Q11*sin(AAA[i])**4+2*(Q12+2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q22*cos(AAA[i])**4)*(TTT[i+1]-TTT[i])\n",
    "        A66=A66+((Q11+Q22-2*Q12-2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q66*(sin(AAA[i])**4+cos(AAA[i])**4))*(TTT[i+1]-TTT[i])\n",
    "        D11=D11+(Q11*cos(AAA[i])**4+2*(Q12+2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q22*sin(AAA[i])**4)*(TTT[i+1]**3-TTT[i]**3)/3.0\n",
    "        D12=D12+((Q11+Q22-4*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q12*(sin(AAA[i])**4+cos(AAA[i])**4))*(TTT[i+1]**3-TTT[i]**3)/3.0\n",
    "        D22=D22+(Q11*sin(AAA[i])**4+2*(Q12+2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q22*cos(AAA[i])**4)*(TTT[i+1]**3-TTT[i]**3)/3.0\n",
    "        D66=D66+((Q11+Q22-2*Q12-2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q66*(sin(AAA[i])**4+cos(AAA[i])**4))*(TTT[i+1]**3-TTT[i]**3)/3.0\n",
    "\n",
    "    #####bianliang########\n",
    "\n",
    "    D= 2*R\n",
    "    L= H\n",
    "\n",
    "    #xian\n",
    "    a=[[A11,A12,0],[A12,A22,0],[0,0,A66]]\n",
    "    b=[[0,0,0],[0,0,0],[0,0,0]]\n",
    "    d=[[D11,D12,0],[D12,D22,0],[0,0,D66]]\n",
    "\n",
    "    alpha=PI/L\n",
    "    beta=2/D\n",
    "\n",
    "    mm=50\n",
    "    nn=50\n",
    "    kmm=0\n",
    "    knn=0\n",
    "    kmm11=0\n",
    "    knn11=0\n",
    "    F=[[0 for col in range(nn)] for row in range(mm)]\n",
    "    Fcr=1e16\n",
    "\n",
    "    for m in range(1,mm+1):\n",
    "        for n in range(1,nn+1):\n",
    "            xi11=2*a[0][0]*(m*alpha)**2+2*a[2][2]*(n*beta)**2\n",
    "            xi12=2*(a[0][1]+a[2][2])*m*alpha*n*beta\n",
    "            xi13=4*a[0][1]*m*alpha/D-2*b[0][0]*(m*alpha)**3-2*(b[0][1]+2*b[2][2])*m*alpha*(n*beta)**2\n",
    "            xi22=2*a[1][1]*(n*beta)**2+2*a[2][2]*(m*alpha)**2\n",
    "            xi23=4*a[1][1]*n*beta/D-2*b[1][1]*(n*beta)**3-2*(b[0][1]+2*b[2][2])*(m*alpha)**2*n*beta\n",
    "            xi33=4*(d[0][1]+2*d[2][2])*(m*alpha*n*beta)**2+8*a[1][1]/(D**2)\\\n",
    "            +2*d[0][0]*(m*alpha)**4+2*d[1][1]*(n*beta)**4-8*(b[1][1]*(n*beta)**2+b[0][1]*(m*alpha)**2)/D\n",
    "            xi21=xi12\n",
    "            xi31=xi13\n",
    "            xi32=xi23\n",
    "            det1=xi11*(xi22*xi33-xi32*xi23)-xi12*(xi21*xi33-xi31*xi23)+xi13*(xi21*xi32-xi31*xi22)\n",
    "            det2=xi11*xi22-xi21*xi12\n",
    "            Nx=det1/det2/(2*((m*alpha)**2))\n",
    "            F[m-1][n-1]=Nx*PI*D\n",
    "            if Fcr>F[m-1][n-1]:\n",
    "              Fcr=F[m-1][n-1]\n",
    "              kmm=m\n",
    "              knn=2*n\n",
    "\n",
    "    for m in range(1,mm+1):\n",
    "        xi11=2*a[0][0]*(m*alpha)**2\n",
    "        xi12=0\n",
    "        xi13=4*a[0][1]*m*alpha/D-2*b[0][0]*(m*alpha)**3\n",
    "        xi22=2*a[2][2]*(m*alpha)**2\n",
    "        xi23=0\n",
    "        xi33=8*a[1][1]/(D**2)+2*d[0][0]*(m*alpha)**4-8*b[0][1]*(m*alpha)**2/D\n",
    "        xi21=xi12\n",
    "        xi31=xi13\n",
    "        xi32=xi23\n",
    "        det1=xi11*(xi22*xi33-xi32*xi23)-xi12*(xi21*xi33-xi31*xi23)+xi13*(xi21*xi32-xi31*xi22)\n",
    "        det2=xi11*xi22-xi21*xi12\n",
    "        Nx=det1/det2/(2*((m*alpha)**2))\n",
    "        Fn1=Nx*PI*D\n",
    "        if Fcr>Fn1:\n",
    "          Fcr=Fn1\n",
    "          kmm=m\n",
    "          knn=1\n",
    "\n",
    "    return Fcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool = mp.Pool(processes = 8)\n",
    "# out_opt_value = pool.map(ssh_test, range(int(data_size*0.1)))\n",
    "# print(out_opt_value)\n",
    "# plt.figure()\n",
    "# plt.hist(out_opt_value)\n",
    "\n",
    "out_opt_proba = mlp.predict_proba(top_10)\n",
    "print(out_opt_proba[:][:,1])\n",
    "plt.figure()\n",
    "plt.hist(out_opt_proba[:][:,1])\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(results_map)\n",
    "# print(out_opt_value)\n",
    "# print(out_opt_proba[:][:,1])\n",
    "# print(np.shape(datasets))\n",
    "\n",
    "# print(out_opt_value)\n",
    "print(np.mean(out_opt_value))\n",
    "print(np.std(out_opt_value))\n",
    "print(min(out_opt_value))\n",
    "print(max(out_opt_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(results_map))\n",
    "print(np.mean(results_map))\n",
    "print(np.std(results_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
