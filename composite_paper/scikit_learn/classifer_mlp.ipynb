{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.717838048934937\n"
     ]
    }
   ],
   "source": [
    "from math import*\n",
    "import __main__\n",
    "global PI\n",
    "import os\n",
    "import time\n",
    "PI=float(acos(-1))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "# random.seed()\n",
    "\n",
    "data_size = 10000;\n",
    "\n",
    "arr = np.random.randint(1, 10, size=[data_size, 8])\n",
    "data_input = arr.tolist()\n",
    "datasets = data_input\n",
    "\n",
    "def ssh(num):\n",
    "#     global datasets\n",
    "#     arr = np.random.randint(1, 10, size=[data_size, 8])\n",
    "#     datasets = arr.tolist()\n",
    "    ply_angle = datasets[num]\n",
    "    \n",
    "    bas_ply=[-60, -45, -30, -15, 0, 15, 30, 45, 60, 90]\n",
    "\n",
    "    AA1=bas_ply[ply_angle[0]]\n",
    "    AA2=bas_ply[ply_angle[1]]\n",
    "    AA3=bas_ply[ply_angle[2]]\n",
    "    AA4=bas_ply[ply_angle[3]]\n",
    "    AA5=bas_ply[ply_angle[4]]\n",
    "    AA6=bas_ply[ply_angle[5]]\n",
    "    AA7=bas_ply[ply_angle[6]]\n",
    "    AA8=bas_ply[ply_angle[7]]\n",
    "\n",
    "    ### ply stacking sequence###\n",
    "    AAA=[AA1/180.0*PI,AA2/180.0*PI,AA3/180.0*PI,AA4/180.0*PI,AA5/180.0*PI,AA6/180.0*PI,AA7/180.0*PI,AA8/180.0*PI,AA8/180.0*PI,AA7/180.0*PI,AA6/180.0*PI,AA5/180.0*PI,AA4/180.0*PI,AA3/180.0*PI,AA2/180.0*PI,AA1/180.0*PI]\n",
    "    pi=3.14159265358979\n",
    "\n",
    "    R=250.0   ##  radius##\n",
    "    H=510.0   ##  Height##\n",
    "    td=0.125  #layer thickness##\n",
    "\n",
    "    TTT=[-td*8,-td*7,-td*6,-td*5,-td*4,-td*3,-td*2,-td*1,td*0,td*1,td*2,td*3,td*4,td*5,td*6,td*7,td*8]\n",
    "\n",
    "    ###material property###\n",
    "\n",
    "    E1=123550.0  \n",
    "    E2=8707.9\n",
    "    G12=5695.0\n",
    "    miu12=0.31946\n",
    "\n",
    "    miu21=miu12*E2/E1\n",
    "    Q11=E1/(1-miu12*miu21)\n",
    "    Q12=miu21*E1/(1-miu12*miu21)\n",
    "    Q22=E2/(1-miu12*miu21)\n",
    "    Q66=G12\n",
    "\n",
    "    A11=0.0\n",
    "    A12=0.0\n",
    "    A22=0.0\n",
    "    A66=0.0\n",
    "\n",
    "    D11=0.0\n",
    "    D12=0.0\n",
    "    D22=0.0\n",
    "    D66=0.0\n",
    "\n",
    "    for i in range(0,16):\n",
    "        A11=A11+(Q11*cos(AAA[i])**4+2*(Q12+2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q22*sin(AAA[i])**4)*(TTT[i+1]-TTT[i])\n",
    "        A12=A12+((Q11+Q22-4*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q12*(sin(AAA[i])**4+cos(AAA[i])**4))*(TTT[i+1]-TTT[i])\n",
    "        A22=A22+(Q11*sin(AAA[i])**4+2*(Q12+2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q22*cos(AAA[i])**4)*(TTT[i+1]-TTT[i])\n",
    "        A66=A66+((Q11+Q22-2*Q12-2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q66*(sin(AAA[i])**4+cos(AAA[i])**4))*(TTT[i+1]-TTT[i])\n",
    "        D11=D11+(Q11*cos(AAA[i])**4+2*(Q12+2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q22*sin(AAA[i])**4)*(TTT[i+1]**3-TTT[i]**3)/3.0\n",
    "        D12=D12+((Q11+Q22-4*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q12*(sin(AAA[i])**4+cos(AAA[i])**4))*(TTT[i+1]**3-TTT[i]**3)/3.0\n",
    "        D22=D22+(Q11*sin(AAA[i])**4+2*(Q12+2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q22*cos(AAA[i])**4)*(TTT[i+1]**3-TTT[i]**3)/3.0\n",
    "        D66=D66+((Q11+Q22-2*Q12-2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q66*(sin(AAA[i])**4+cos(AAA[i])**4))*(TTT[i+1]**3-TTT[i]**3)/3.0\n",
    "\n",
    "    #####bianliang########\n",
    "\n",
    "    D= 2*R\n",
    "    L= H\n",
    "\n",
    "    #xian\n",
    "    a=[[A11,A12,0],[A12,A22,0],[0,0,A66]]\n",
    "    b=[[0,0,0],[0,0,0],[0,0,0]]\n",
    "    d=[[D11,D12,0],[D12,D22,0],[0,0,D66]]\n",
    "\n",
    "    alpha=PI/L\n",
    "    beta=2/D\n",
    "\n",
    "    mm=50\n",
    "    nn=50\n",
    "    kmm=0\n",
    "    knn=0\n",
    "    kmm11=0\n",
    "    knn11=0\n",
    "    F=[[0 for col in range(nn)] for row in range(mm)]\n",
    "    Fcr=1e16\n",
    "\n",
    "    for m in range(1,mm+1):\n",
    "        for n in range(1,nn+1):\n",
    "            xi11=2*a[0][0]*(m*alpha)**2+2*a[2][2]*(n*beta)**2\n",
    "            xi12=2*(a[0][1]+a[2][2])*m*alpha*n*beta\n",
    "            xi13=4*a[0][1]*m*alpha/D-2*b[0][0]*(m*alpha)**3-2*(b[0][1]+2*b[2][2])*m*alpha*(n*beta)**2\n",
    "            xi22=2*a[1][1]*(n*beta)**2+2*a[2][2]*(m*alpha)**2\n",
    "            xi23=4*a[1][1]*n*beta/D-2*b[1][1]*(n*beta)**3-2*(b[0][1]+2*b[2][2])*(m*alpha)**2*n*beta\n",
    "            xi33=4*(d[0][1]+2*d[2][2])*(m*alpha*n*beta)**2+8*a[1][1]/(D**2)\\\n",
    "            +2*d[0][0]*(m*alpha)**4+2*d[1][1]*(n*beta)**4-8*(b[1][1]*(n*beta)**2+b[0][1]*(m*alpha)**2)/D\n",
    "            xi21=xi12\n",
    "            xi31=xi13\n",
    "            xi32=xi23\n",
    "            det1=xi11*(xi22*xi33-xi32*xi23)-xi12*(xi21*xi33-xi31*xi23)+xi13*(xi21*xi32-xi31*xi22)\n",
    "            det2=xi11*xi22-xi21*xi12\n",
    "            Nx=det1/det2/(2*((m*alpha)**2))\n",
    "            F[m-1][n-1]=Nx*PI*D\n",
    "            if Fcr>F[m-1][n-1]:\n",
    "              Fcr=F[m-1][n-1]\n",
    "              kmm=m\n",
    "              knn=2*n\n",
    "\n",
    "    for m in range(1,mm+1):\n",
    "        xi11=2*a[0][0]*(m*alpha)**2\n",
    "        xi12=0\n",
    "        xi13=4*a[0][1]*m*alpha/D-2*b[0][0]*(m*alpha)**3\n",
    "        xi22=2*a[2][2]*(m*alpha)**2\n",
    "        xi23=0\n",
    "        xi33=8*a[1][1]/(D**2)+2*d[0][0]*(m*alpha)**4-8*b[0][1]*(m*alpha)**2/D\n",
    "        xi21=xi12\n",
    "        xi31=xi13\n",
    "        xi32=xi23\n",
    "        det1=xi11*(xi22*xi33-xi32*xi23)-xi12*(xi21*xi33-xi31*xi23)+xi13*(xi21*xi32-xi31*xi22)\n",
    "        det2=xi11*xi22-xi21*xi12\n",
    "        Nx=det1/det2/(2*((m*alpha)**2))\n",
    "        Fn1=Nx*PI*D\n",
    "        if Fcr>Fn1:\n",
    "          Fcr=Fn1\n",
    "          kmm=m\n",
    "          knn=1\n",
    "\n",
    "    return Fcr\n",
    "\n",
    "\n",
    "# map async parallel \n",
    "# pool = mp.Pool(mp.cpu_count())\n",
    "pool = mp.Pool(processes = 8)\n",
    "\n",
    "start = time.time()\n",
    "results_map = pool.map(ssh, range(data_size))\n",
    "end = time.time() \n",
    "print(end - start) #0.0037827491760253906\n",
    "# print(results_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597624.0460489461\n",
      "torch.Size([10000, 8])\n",
      "torch.Size([10000, 1])\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "results_map_sort = sorted(results_map) # note that sorted change the sort but didn't change the original one\n",
    "# print(results_map_sort)\n",
    "median_value = results_map_sort[data_size//2]\n",
    "print(median_value)\n",
    "\n",
    "results_map_new = np.zeros((data_size,1))\n",
    "# print(type(results_map))\n",
    "output_scikit = np.zeros(data_size)\n",
    "\n",
    "for i in range(data_size):\n",
    "    if results_map[i] > median_value:\n",
    "        results_map_new[i,:] = 1\n",
    "        output_scikit[i] = 1\n",
    "    else:\n",
    "        results_map_new[i,:] = 0\n",
    "        output_scikit[i] = 0\n",
    "        \n",
    "# print(results_map) # new results_map transfer to [0,1]\n",
    "# print(datasets)\n",
    "# type(datasets)\n",
    "# type(results_map)\n",
    "\n",
    "X = torch.FloatTensor(datasets)\n",
    "Y = torch.FloatTensor(results_map_new)\n",
    "# type(data_tensor)\n",
    "#type(output_tensor)\n",
    "print(X.size())\n",
    "print(Y.size())\n",
    "# print(X)\n",
    "# print(Y)\n",
    "\n",
    "\n",
    "print(type(datasets))\n",
    "print(type(output_scikit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training by classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import svm\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# clf = svm.SVC(gamma=0.001,probability=True)\n",
    "# X = datasets\n",
    "# y = results_map\n",
    "\n",
    "# clf.fit(X, y) \n",
    "\n",
    "# score_svm = cross_val_score(clf, X, y, scoring='recall_macro',cv=5)  \n",
    "# print(score_svm)\n",
    "\n",
    "# clf.predict([datasets[3]])\n",
    "# clf.predict_proba([datasets[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69578929\n",
      "Iteration 2, loss = 0.68854120\n",
      "Iteration 3, loss = 0.68179479\n",
      "Iteration 4, loss = 0.67588315\n",
      "Iteration 5, loss = 0.67055195\n",
      "Iteration 6, loss = 0.66560179\n",
      "Iteration 7, loss = 0.66119054\n",
      "Iteration 8, loss = 0.65704214\n",
      "Iteration 9, loss = 0.65323425\n",
      "Iteration 10, loss = 0.64971791\n",
      "Iteration 11, loss = 0.64647671\n",
      "Iteration 12, loss = 0.64347127\n",
      "Iteration 13, loss = 0.64071231\n",
      "Iteration 14, loss = 0.63820508\n",
      "Iteration 15, loss = 0.63583294\n",
      "Iteration 16, loss = 0.63371811\n",
      "Iteration 17, loss = 0.63169330\n",
      "Iteration 18, loss = 0.62983295\n",
      "Iteration 19, loss = 0.62812876\n",
      "Iteration 20, loss = 0.62648560\n",
      "Iteration 21, loss = 0.62501629\n",
      "Iteration 22, loss = 0.62362740\n",
      "Iteration 23, loss = 0.62227025\n",
      "Iteration 24, loss = 0.62097799\n",
      "Iteration 25, loss = 0.61970758\n",
      "Iteration 26, loss = 0.61861665\n",
      "Iteration 27, loss = 0.61744765\n",
      "Iteration 28, loss = 0.61631135\n",
      "Iteration 29, loss = 0.61526012\n",
      "Iteration 30, loss = 0.61426927\n",
      "Iteration 31, loss = 0.61322993\n",
      "Iteration 32, loss = 0.61222887\n",
      "Iteration 33, loss = 0.61126225\n",
      "Iteration 34, loss = 0.61028101\n",
      "Iteration 35, loss = 0.60934194\n",
      "Iteration 36, loss = 0.60838253\n",
      "Iteration 37, loss = 0.60745022\n",
      "Iteration 38, loss = 0.60652918\n",
      "Iteration 39, loss = 0.60563377\n",
      "Iteration 40, loss = 0.60471746\n",
      "Iteration 41, loss = 0.60386859\n",
      "Iteration 42, loss = 0.60293359\n",
      "Iteration 43, loss = 0.60208358\n",
      "Iteration 44, loss = 0.60116759\n",
      "Iteration 45, loss = 0.60030469\n",
      "Iteration 46, loss = 0.59944959\n",
      "Iteration 47, loss = 0.59857974\n",
      "Iteration 48, loss = 0.59771979\n",
      "Iteration 49, loss = 0.59686670\n",
      "Iteration 50, loss = 0.59598915\n",
      "Iteration 51, loss = 0.59517777\n",
      "Iteration 52, loss = 0.59432437\n",
      "Iteration 53, loss = 0.59348440\n",
      "Iteration 54, loss = 0.59266252\n",
      "Iteration 55, loss = 0.59181577\n",
      "Iteration 56, loss = 0.59102438\n",
      "Iteration 57, loss = 0.59019566\n",
      "Iteration 58, loss = 0.58937204\n",
      "Iteration 59, loss = 0.58857205\n",
      "Iteration 60, loss = 0.58778680\n",
      "Iteration 61, loss = 0.58697827\n",
      "Iteration 62, loss = 0.58624012\n",
      "Iteration 63, loss = 0.58544966\n",
      "Iteration 64, loss = 0.58465631\n",
      "Iteration 65, loss = 0.58392301\n",
      "Iteration 66, loss = 0.58319843\n",
      "Iteration 67, loss = 0.58234216\n",
      "Iteration 68, loss = 0.58157377\n",
      "Iteration 69, loss = 0.58089049\n",
      "Iteration 70, loss = 0.58015283\n",
      "Iteration 71, loss = 0.57937418\n",
      "Iteration 72, loss = 0.57867737\n",
      "Iteration 73, loss = 0.57794059\n",
      "Iteration 74, loss = 0.57720769\n",
      "Iteration 75, loss = 0.57647864\n",
      "Iteration 76, loss = 0.57581028\n",
      "Iteration 77, loss = 0.57511248\n",
      "Iteration 78, loss = 0.57443353\n",
      "Iteration 79, loss = 0.57370484\n",
      "Iteration 80, loss = 0.57303654\n",
      "Iteration 81, loss = 0.57231333\n",
      "Iteration 82, loss = 0.57167389\n",
      "Iteration 83, loss = 0.57098879\n",
      "Iteration 84, loss = 0.57039241\n",
      "Iteration 85, loss = 0.56966927\n",
      "Iteration 86, loss = 0.56903506\n",
      "Iteration 87, loss = 0.56840001\n",
      "Iteration 88, loss = 0.56773324\n",
      "Iteration 89, loss = 0.56712207\n",
      "Iteration 90, loss = 0.56644819\n",
      "Iteration 91, loss = 0.56581573\n",
      "Iteration 92, loss = 0.56518404\n",
      "Iteration 93, loss = 0.56463779\n",
      "Iteration 94, loss = 0.56401866\n",
      "Iteration 95, loss = 0.56330559\n",
      "Iteration 96, loss = 0.56278341\n",
      "Iteration 97, loss = 0.56208036\n",
      "Iteration 98, loss = 0.56154558\n",
      "Iteration 99, loss = 0.56100817\n",
      "Iteration 100, loss = 0.56037462\n",
      "Iteration 101, loss = 0.55975486\n",
      "Iteration 102, loss = 0.55919565\n",
      "Iteration 103, loss = 0.55861790\n",
      "Iteration 104, loss = 0.55799844\n",
      "Iteration 105, loss = 0.55749690\n",
      "Iteration 106, loss = 0.55678686\n",
      "Iteration 107, loss = 0.55624070\n",
      "Iteration 108, loss = 0.55578201\n",
      "Iteration 109, loss = 0.55509182\n",
      "Iteration 110, loss = 0.55453443\n",
      "Iteration 111, loss = 0.55404896\n",
      "Iteration 112, loss = 0.55341733\n",
      "Iteration 113, loss = 0.55284824\n",
      "Iteration 114, loss = 0.55228802\n",
      "Iteration 115, loss = 0.55172756\n",
      "Iteration 116, loss = 0.55121791\n",
      "Iteration 117, loss = 0.55067687\n",
      "Iteration 118, loss = 0.55012779\n",
      "Iteration 119, loss = 0.54951054\n",
      "Iteration 120, loss = 0.54902615\n",
      "Iteration 121, loss = 0.54853183\n",
      "Iteration 122, loss = 0.54792116\n",
      "Iteration 123, loss = 0.54743099\n",
      "Iteration 124, loss = 0.54684792\n",
      "Iteration 125, loss = 0.54632856\n",
      "Iteration 126, loss = 0.54585266\n",
      "Iteration 127, loss = 0.54517865\n",
      "Iteration 128, loss = 0.54473890\n",
      "Iteration 129, loss = 0.54419112\n",
      "Iteration 130, loss = 0.54380300\n",
      "Iteration 131, loss = 0.54315570\n",
      "Iteration 132, loss = 0.54261157\n",
      "Iteration 133, loss = 0.54213684\n",
      "Iteration 134, loss = 0.54161040\n",
      "Iteration 135, loss = 0.54106510\n",
      "Iteration 136, loss = 0.54054216\n",
      "Iteration 137, loss = 0.54007783\n",
      "Iteration 138, loss = 0.53956117\n",
      "Iteration 139, loss = 0.53904226\n",
      "Iteration 140, loss = 0.53845532\n",
      "Iteration 141, loss = 0.53800589\n",
      "Iteration 142, loss = 0.53753731\n",
      "Iteration 143, loss = 0.53700186\n",
      "Iteration 144, loss = 0.53645079\n",
      "Iteration 145, loss = 0.53599624\n",
      "Iteration 146, loss = 0.53550617\n",
      "Iteration 147, loss = 0.53500521\n",
      "Iteration 148, loss = 0.53446308\n",
      "Iteration 149, loss = 0.53396687\n",
      "Iteration 150, loss = 0.53355266\n",
      "Iteration 151, loss = 0.53301116\n",
      "Iteration 152, loss = 0.53246560\n",
      "Iteration 153, loss = 0.53188471\n",
      "Iteration 154, loss = 0.53143788\n",
      "Iteration 155, loss = 0.53098463\n",
      "Iteration 156, loss = 0.53045488\n",
      "Iteration 157, loss = 0.52984897\n",
      "Iteration 158, loss = 0.52939091\n",
      "Iteration 159, loss = 0.52885320\n",
      "Iteration 160, loss = 0.52834732\n",
      "Iteration 161, loss = 0.52780294\n",
      "Iteration 162, loss = 0.52723166\n",
      "Iteration 163, loss = 0.52687528\n",
      "Iteration 164, loss = 0.52626742\n",
      "Iteration 165, loss = 0.52574967\n",
      "Iteration 166, loss = 0.52523525\n",
      "Iteration 167, loss = 0.52468164\n",
      "Iteration 168, loss = 0.52424938\n",
      "Iteration 169, loss = 0.52360409\n",
      "Iteration 170, loss = 0.52322495\n",
      "Iteration 171, loss = 0.52262214\n",
      "Iteration 172, loss = 0.52220697\n",
      "Iteration 173, loss = 0.52160813\n",
      "Iteration 174, loss = 0.52111057\n",
      "Iteration 175, loss = 0.52059638\n",
      "Iteration 176, loss = 0.51999032\n",
      "Iteration 177, loss = 0.51946549\n",
      "Iteration 178, loss = 0.51908884\n",
      "Iteration 179, loss = 0.51839697\n",
      "Iteration 180, loss = 0.51782988\n",
      "Iteration 181, loss = 0.51735796\n",
      "Iteration 182, loss = 0.51673319\n",
      "Iteration 183, loss = 0.51625813\n",
      "Iteration 184, loss = 0.51581189\n",
      "Iteration 185, loss = 0.51512200\n",
      "Iteration 186, loss = 0.51459673\n",
      "Iteration 187, loss = 0.51410697\n",
      "Iteration 188, loss = 0.51345855\n",
      "Iteration 189, loss = 0.51309641\n",
      "Iteration 190, loss = 0.51242798\n",
      "Iteration 191, loss = 0.51189525\n",
      "Iteration 192, loss = 0.51132416\n",
      "Iteration 193, loss = 0.51102647\n",
      "Iteration 194, loss = 0.51025961\n",
      "Iteration 195, loss = 0.50969892\n",
      "Iteration 196, loss = 0.50920568\n",
      "Iteration 197, loss = 0.50866463\n",
      "Iteration 198, loss = 0.50807453\n",
      "Iteration 199, loss = 0.50763537\n",
      "Iteration 200, loss = 0.50697926\n",
      "Iteration 201, loss = 0.50641943\n",
      "Iteration 202, loss = 0.50588923\n",
      "Iteration 203, loss = 0.50526754\n",
      "Iteration 204, loss = 0.50473346\n",
      "Iteration 205, loss = 0.50419428\n",
      "Iteration 206, loss = 0.50376586\n",
      "Iteration 207, loss = 0.50322024\n",
      "Iteration 208, loss = 0.50255600\n",
      "Iteration 209, loss = 0.50201559\n",
      "Iteration 210, loss = 0.50133931\n",
      "Iteration 211, loss = 0.50081234\n",
      "Iteration 212, loss = 0.50037085\n",
      "Iteration 213, loss = 0.49964176\n",
      "Iteration 214, loss = 0.49922980\n",
      "Iteration 215, loss = 0.49862483\n",
      "Iteration 216, loss = 0.49814122\n",
      "Iteration 217, loss = 0.49735741\n",
      "Iteration 218, loss = 0.49691111\n",
      "Iteration 219, loss = 0.49636605\n",
      "Iteration 220, loss = 0.49569500\n",
      "Iteration 221, loss = 0.49524866\n",
      "Iteration 222, loss = 0.49468252\n",
      "Iteration 223, loss = 0.49398781\n",
      "Iteration 224, loss = 0.49340817\n",
      "Iteration 225, loss = 0.49273490\n",
      "Iteration 226, loss = 0.49229300\n",
      "Iteration 227, loss = 0.49177104\n",
      "Iteration 228, loss = 0.49102538\n",
      "Iteration 229, loss = 0.49060919\n",
      "Iteration 230, loss = 0.49011063\n",
      "Iteration 231, loss = 0.48945807\n",
      "Iteration 232, loss = 0.48906641\n",
      "Iteration 233, loss = 0.48821021\n",
      "Iteration 234, loss = 0.48763266\n",
      "Iteration 235, loss = 0.48716563\n",
      "Iteration 236, loss = 0.48647485\n",
      "Iteration 237, loss = 0.48590827\n",
      "Iteration 238, loss = 0.48541284\n",
      "Iteration 239, loss = 0.48481297\n",
      "Iteration 240, loss = 0.48435968\n",
      "Iteration 241, loss = 0.48378395\n",
      "Iteration 242, loss = 0.48326048\n",
      "Iteration 243, loss = 0.48243069\n",
      "Iteration 244, loss = 0.48193130\n",
      "Iteration 245, loss = 0.48134741\n",
      "Iteration 246, loss = 0.48079588\n",
      "Iteration 247, loss = 0.48010533\n",
      "Iteration 248, loss = 0.47961289\n",
      "Iteration 249, loss = 0.47901942\n",
      "Iteration 250, loss = 0.47840849\n",
      "Iteration 251, loss = 0.47797791\n",
      "Iteration 252, loss = 0.47726072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.47670944\n",
      "Iteration 254, loss = 0.47603120\n",
      "Iteration 255, loss = 0.47552049\n",
      "Iteration 256, loss = 0.47508498\n",
      "Iteration 257, loss = 0.47431956\n",
      "Iteration 258, loss = 0.47380532\n",
      "Iteration 259, loss = 0.47311936\n",
      "Iteration 260, loss = 0.47265855\n",
      "Iteration 261, loss = 0.47209407\n",
      "Iteration 262, loss = 0.47159746\n",
      "Iteration 263, loss = 0.47099542\n",
      "Iteration 264, loss = 0.47028143\n",
      "Iteration 265, loss = 0.46972026\n",
      "Iteration 266, loss = 0.46921049\n",
      "Iteration 267, loss = 0.46859980\n",
      "Iteration 268, loss = 0.46809055\n",
      "Iteration 269, loss = 0.46741206\n",
      "Iteration 270, loss = 0.46685562\n",
      "Iteration 271, loss = 0.46642738\n",
      "Iteration 272, loss = 0.46599608\n",
      "Iteration 273, loss = 0.46529619\n",
      "Iteration 274, loss = 0.46474265\n",
      "Iteration 275, loss = 0.46413999\n",
      "Iteration 276, loss = 0.46360484\n",
      "Iteration 277, loss = 0.46318899\n",
      "Iteration 278, loss = 0.46242914\n",
      "Iteration 279, loss = 0.46192975\n",
      "Iteration 280, loss = 0.46138355\n",
      "Iteration 281, loss = 0.46086609\n",
      "Iteration 282, loss = 0.46037235\n",
      "Iteration 283, loss = 0.45959205\n",
      "Iteration 284, loss = 0.45939601\n",
      "Iteration 285, loss = 0.45897112\n",
      "Iteration 286, loss = 0.45844662\n",
      "Iteration 287, loss = 0.45757176\n",
      "Iteration 288, loss = 0.45727368\n",
      "Iteration 289, loss = 0.45664592\n",
      "Iteration 290, loss = 0.45615015\n",
      "Iteration 291, loss = 0.45556748\n",
      "Iteration 292, loss = 0.45519957\n",
      "Iteration 293, loss = 0.45467010\n",
      "Iteration 294, loss = 0.45414076\n",
      "Iteration 295, loss = 0.45340727\n",
      "Iteration 296, loss = 0.45292445\n",
      "Iteration 297, loss = 0.45223967\n",
      "Iteration 298, loss = 0.45221928\n",
      "Iteration 299, loss = 0.45141121\n",
      "Iteration 300, loss = 0.45090522\n",
      "Iteration 301, loss = 0.45019344\n",
      "Iteration 302, loss = 0.44967701\n",
      "Iteration 303, loss = 0.44932477\n",
      "Iteration 304, loss = 0.44904576\n",
      "Iteration 305, loss = 0.44827283\n",
      "Iteration 306, loss = 0.44788473\n",
      "Iteration 307, loss = 0.44707132\n",
      "Iteration 308, loss = 0.44661107\n",
      "Iteration 309, loss = 0.44602963\n",
      "Iteration 310, loss = 0.44560788\n",
      "Iteration 311, loss = 0.44505972\n",
      "Iteration 312, loss = 0.44439898\n",
      "Iteration 313, loss = 0.44418940\n",
      "Iteration 314, loss = 0.44336696\n",
      "Iteration 315, loss = 0.44295637\n",
      "Iteration 316, loss = 0.44247231\n",
      "Iteration 317, loss = 0.44199928\n",
      "Iteration 318, loss = 0.44132434\n",
      "Iteration 319, loss = 0.44090993\n",
      "Iteration 320, loss = 0.44054247\n",
      "Iteration 321, loss = 0.43989483\n",
      "Iteration 322, loss = 0.43922845\n",
      "Iteration 323, loss = 0.43912514\n",
      "Iteration 324, loss = 0.43834162\n",
      "Iteration 325, loss = 0.43797217\n",
      "Iteration 326, loss = 0.43730552\n",
      "Iteration 327, loss = 0.43728393\n",
      "Iteration 328, loss = 0.43641219\n",
      "Iteration 329, loss = 0.43577221\n",
      "Iteration 330, loss = 0.43535893\n",
      "Iteration 331, loss = 0.43486620\n",
      "Iteration 332, loss = 0.43431977\n",
      "Iteration 333, loss = 0.43394587\n",
      "Iteration 334, loss = 0.43347515\n",
      "Iteration 335, loss = 0.43286900\n",
      "Iteration 336, loss = 0.43235410\n",
      "Iteration 337, loss = 0.43198662\n",
      "Iteration 338, loss = 0.43167618\n",
      "Iteration 339, loss = 0.43095848\n",
      "Iteration 340, loss = 0.43039941\n",
      "Iteration 341, loss = 0.42999448\n",
      "Iteration 342, loss = 0.42941985\n",
      "Iteration 343, loss = 0.42903751\n",
      "Iteration 344, loss = 0.42840007\n",
      "Iteration 345, loss = 0.42806838\n",
      "Iteration 346, loss = 0.42766164\n",
      "Iteration 347, loss = 0.42710242\n",
      "Iteration 348, loss = 0.42656904\n",
      "Iteration 349, loss = 0.42615375\n",
      "Iteration 350, loss = 0.42544755\n",
      "Iteration 351, loss = 0.42521712\n",
      "Iteration 352, loss = 0.42475280\n",
      "Iteration 353, loss = 0.42396781\n",
      "Iteration 354, loss = 0.42362124\n",
      "Iteration 355, loss = 0.42334526\n",
      "Iteration 356, loss = 0.42252556\n",
      "Iteration 357, loss = 0.42226476\n",
      "Iteration 358, loss = 0.42169001\n",
      "Iteration 359, loss = 0.42115663\n",
      "Iteration 360, loss = 0.42096212\n",
      "Iteration 361, loss = 0.42050922\n",
      "Iteration 362, loss = 0.41989101\n",
      "Iteration 363, loss = 0.41965853\n",
      "Iteration 364, loss = 0.41923324\n",
      "Iteration 365, loss = 0.41862597\n",
      "Iteration 366, loss = 0.41831447\n",
      "Iteration 367, loss = 0.41755158\n",
      "Iteration 368, loss = 0.41707676\n",
      "Iteration 369, loss = 0.41662912\n",
      "Iteration 370, loss = 0.41612405\n",
      "Iteration 371, loss = 0.41581359\n",
      "Iteration 372, loss = 0.41548080\n",
      "Iteration 373, loss = 0.41481267\n",
      "Iteration 374, loss = 0.41465965\n",
      "Iteration 375, loss = 0.41372240\n",
      "Iteration 376, loss = 0.41338822\n",
      "Iteration 377, loss = 0.41268628\n",
      "Iteration 378, loss = 0.41259379\n",
      "Iteration 379, loss = 0.41240812\n",
      "Iteration 380, loss = 0.41185945\n",
      "Iteration 381, loss = 0.41103810\n",
      "Iteration 382, loss = 0.41061593\n",
      "Iteration 383, loss = 0.41056074\n",
      "Iteration 384, loss = 0.40954382\n",
      "Iteration 385, loss = 0.40949788\n",
      "Iteration 386, loss = 0.40904660\n",
      "Iteration 387, loss = 0.40822574\n",
      "Iteration 388, loss = 0.40797444\n",
      "Iteration 389, loss = 0.40745418\n",
      "Iteration 390, loss = 0.40700216\n",
      "Iteration 391, loss = 0.40665834\n",
      "Iteration 392, loss = 0.40668001\n",
      "Iteration 393, loss = 0.40584257\n",
      "Iteration 394, loss = 0.40554014\n",
      "Iteration 395, loss = 0.40495371\n",
      "Iteration 396, loss = 0.40463328\n",
      "Iteration 397, loss = 0.40401132\n",
      "Iteration 398, loss = 0.40397497\n",
      "Iteration 399, loss = 0.40315936\n",
      "Iteration 400, loss = 0.40272572\n",
      "Iteration 401, loss = 0.40214813\n",
      "Iteration 402, loss = 0.40178887\n",
      "Iteration 403, loss = 0.40140166\n",
      "Iteration 404, loss = 0.40090828\n",
      "Iteration 405, loss = 0.40040820\n",
      "Iteration 406, loss = 0.40046819\n",
      "Iteration 407, loss = 0.39985599\n",
      "Iteration 408, loss = 0.39916215\n",
      "Iteration 409, loss = 0.39913940\n",
      "Iteration 410, loss = 0.39859710\n",
      "Iteration 411, loss = 0.39799803\n",
      "Iteration 412, loss = 0.39764840\n",
      "Iteration 413, loss = 0.39702117\n",
      "Iteration 414, loss = 0.39661061\n",
      "Iteration 415, loss = 0.39638985\n",
      "Iteration 416, loss = 0.39579825\n",
      "Iteration 417, loss = 0.39548177\n",
      "Iteration 418, loss = 0.39515618\n",
      "Iteration 419, loss = 0.39480847\n",
      "Iteration 420, loss = 0.39415919\n",
      "Iteration 421, loss = 0.39373612\n",
      "Iteration 422, loss = 0.39347657\n",
      "Iteration 423, loss = 0.39284148\n",
      "Iteration 424, loss = 0.39251078\n",
      "Iteration 425, loss = 0.39241092\n",
      "Iteration 426, loss = 0.39198878\n",
      "Iteration 427, loss = 0.39130052\n",
      "Iteration 428, loss = 0.39077545\n",
      "Iteration 429, loss = 0.39059340\n",
      "Iteration 430, loss = 0.39009044\n",
      "Iteration 431, loss = 0.38987953\n",
      "Iteration 432, loss = 0.38926834\n",
      "Iteration 433, loss = 0.38881237\n",
      "Iteration 434, loss = 0.38867789\n",
      "Iteration 435, loss = 0.38832414\n",
      "Iteration 436, loss = 0.38784158\n",
      "Iteration 437, loss = 0.38745644\n",
      "Iteration 438, loss = 0.38698478\n",
      "Iteration 439, loss = 0.38666482\n",
      "Iteration 440, loss = 0.38614254\n",
      "Iteration 441, loss = 0.38565975\n",
      "Iteration 442, loss = 0.38557082\n",
      "Iteration 443, loss = 0.38497806\n",
      "Iteration 444, loss = 0.38461829\n",
      "Iteration 445, loss = 0.38381693\n",
      "Iteration 446, loss = 0.38364253\n",
      "Iteration 447, loss = 0.38332276\n",
      "Iteration 448, loss = 0.38280547\n",
      "Iteration 449, loss = 0.38229560\n",
      "Iteration 450, loss = 0.38202515\n",
      "Iteration 451, loss = 0.38174695\n",
      "Iteration 452, loss = 0.38126862\n",
      "Iteration 453, loss = 0.38084361\n",
      "Iteration 454, loss = 0.38062686\n",
      "Iteration 455, loss = 0.38015249\n",
      "Iteration 456, loss = 0.37951016\n",
      "Iteration 457, loss = 0.37948456\n",
      "Iteration 458, loss = 0.37886182\n",
      "Iteration 459, loss = 0.37896179\n",
      "Iteration 460, loss = 0.37816309\n",
      "Iteration 461, loss = 0.37778945\n",
      "Iteration 462, loss = 0.37754072\n",
      "Iteration 463, loss = 0.37714826\n",
      "Iteration 464, loss = 0.37681445\n",
      "Iteration 465, loss = 0.37632639\n",
      "Iteration 466, loss = 0.37624571\n",
      "Iteration 467, loss = 0.37531853\n",
      "Iteration 468, loss = 0.37534771\n",
      "Iteration 469, loss = 0.37497341\n",
      "Iteration 470, loss = 0.37430147\n",
      "Iteration 471, loss = 0.37391556\n",
      "Iteration 472, loss = 0.37396430\n",
      "Iteration 473, loss = 0.37343565\n",
      "Iteration 474, loss = 0.37303099\n",
      "Iteration 475, loss = 0.37274362\n",
      "Iteration 476, loss = 0.37237501\n",
      "Iteration 477, loss = 0.37191034\n",
      "Iteration 478, loss = 0.37159159\n",
      "Iteration 479, loss = 0.37123280\n",
      "Iteration 480, loss = 0.37087547\n",
      "Iteration 481, loss = 0.37056892\n",
      "Iteration 482, loss = 0.36994959\n",
      "Iteration 483, loss = 0.36966447\n",
      "Iteration 484, loss = 0.36927912\n",
      "Iteration 485, loss = 0.36873702\n",
      "Iteration 486, loss = 0.36917473\n",
      "Iteration 487, loss = 0.36846555\n",
      "Iteration 488, loss = 0.36784802\n",
      "Iteration 489, loss = 0.36736328\n",
      "Iteration 490, loss = 0.36732380\n",
      "Iteration 491, loss = 0.36679047\n",
      "Iteration 492, loss = 0.36622114\n",
      "Iteration 493, loss = 0.36610999\n",
      "Iteration 494, loss = 0.36596726\n",
      "Iteration 495, loss = 0.36560495\n",
      "Iteration 496, loss = 0.36532836\n",
      "Iteration 497, loss = 0.36460805\n",
      "Iteration 498, loss = 0.36441517\n",
      "Iteration 499, loss = 0.36394123\n",
      "Iteration 500, loss = 0.36356215\n",
      "Iteration 501, loss = 0.36313162\n",
      "Iteration 502, loss = 0.36329897\n",
      "Iteration 503, loss = 0.36246386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 504, loss = 0.36242082\n",
      "Iteration 505, loss = 0.36197711\n",
      "Iteration 506, loss = 0.36167079\n",
      "Iteration 507, loss = 0.36112779\n",
      "Iteration 508, loss = 0.36111237\n",
      "Iteration 509, loss = 0.36033722\n",
      "Iteration 510, loss = 0.35997714\n",
      "Iteration 511, loss = 0.35958728\n",
      "Iteration 512, loss = 0.35979388\n",
      "Iteration 513, loss = 0.35945987\n",
      "Iteration 514, loss = 0.35886615\n",
      "Iteration 515, loss = 0.35834669\n",
      "Iteration 516, loss = 0.35835351\n",
      "Iteration 517, loss = 0.35775115\n",
      "Iteration 518, loss = 0.35751123\n",
      "Iteration 519, loss = 0.35689819\n",
      "Iteration 520, loss = 0.35667331\n",
      "Iteration 521, loss = 0.35666786\n",
      "Iteration 522, loss = 0.35619591\n",
      "Iteration 523, loss = 0.35575792\n",
      "Iteration 524, loss = 0.35523396\n",
      "Iteration 525, loss = 0.35514054\n",
      "Iteration 526, loss = 0.35457253\n",
      "Iteration 527, loss = 0.35462330\n",
      "Iteration 528, loss = 0.35393986\n",
      "Iteration 529, loss = 0.35367554\n",
      "Iteration 530, loss = 0.35321476\n",
      "Iteration 531, loss = 0.35279262\n",
      "Iteration 532, loss = 0.35256242\n",
      "Iteration 533, loss = 0.35213790\n",
      "Iteration 534, loss = 0.35250036\n",
      "Iteration 535, loss = 0.35116573\n",
      "Iteration 536, loss = 0.35129474\n",
      "Iteration 537, loss = 0.35060196\n",
      "Iteration 538, loss = 0.35076087\n",
      "Iteration 539, loss = 0.35053997\n",
      "Iteration 540, loss = 0.35007727\n",
      "Iteration 541, loss = 0.34956191\n",
      "Iteration 542, loss = 0.34918203\n",
      "Iteration 543, loss = 0.34891051\n",
      "Iteration 544, loss = 0.34891083\n",
      "Iteration 545, loss = 0.34840561\n",
      "Iteration 546, loss = 0.34828153\n",
      "Iteration 547, loss = 0.34782992\n",
      "Iteration 548, loss = 0.34733263\n",
      "Iteration 549, loss = 0.34664137\n",
      "Iteration 550, loss = 0.34688300\n",
      "Iteration 551, loss = 0.34629285\n",
      "Iteration 552, loss = 0.34599999\n",
      "Iteration 553, loss = 0.34550507\n",
      "Iteration 554, loss = 0.34516914\n",
      "Iteration 555, loss = 0.34538866\n",
      "Iteration 556, loss = 0.34467628\n",
      "Iteration 557, loss = 0.34439063\n",
      "Iteration 558, loss = 0.34443475\n",
      "Iteration 559, loss = 0.34367207\n",
      "Iteration 560, loss = 0.34341108\n",
      "Iteration 561, loss = 0.34319317\n",
      "Iteration 562, loss = 0.34270526\n",
      "Iteration 563, loss = 0.34234261\n",
      "Iteration 564, loss = 0.34234941\n",
      "Iteration 565, loss = 0.34186408\n",
      "Iteration 566, loss = 0.34172803\n",
      "Iteration 567, loss = 0.34162756\n",
      "Iteration 568, loss = 0.34157770\n",
      "Iteration 569, loss = 0.34059246\n",
      "Iteration 570, loss = 0.34047109\n",
      "Iteration 571, loss = 0.34011601\n",
      "Iteration 572, loss = 0.33979625\n",
      "Iteration 573, loss = 0.33926409\n",
      "Iteration 574, loss = 0.33935464\n",
      "Iteration 575, loss = 0.33870558\n",
      "Iteration 576, loss = 0.33901657\n",
      "Iteration 577, loss = 0.33837708\n",
      "Iteration 578, loss = 0.33820124\n",
      "Iteration 579, loss = 0.33766632\n",
      "Iteration 580, loss = 0.33720172\n",
      "Iteration 581, loss = 0.33720211\n",
      "Iteration 582, loss = 0.33657313\n",
      "Iteration 583, loss = 0.33664303\n",
      "Iteration 584, loss = 0.33652924\n",
      "Iteration 585, loss = 0.33558109\n",
      "Iteration 586, loss = 0.33590356\n",
      "Iteration 587, loss = 0.33500279\n",
      "Iteration 588, loss = 0.33486760\n",
      "Iteration 589, loss = 0.33429158\n",
      "Iteration 590, loss = 0.33407059\n",
      "Iteration 591, loss = 0.33407055\n",
      "Iteration 592, loss = 0.33372460\n",
      "Iteration 593, loss = 0.33385683\n",
      "Iteration 594, loss = 0.33298805\n",
      "Iteration 595, loss = 0.33300843\n",
      "Iteration 596, loss = 0.33236688\n",
      "Iteration 597, loss = 0.33187450\n",
      "Iteration 598, loss = 0.33167770\n",
      "Iteration 599, loss = 0.33179758\n",
      "Iteration 600, loss = 0.33112692\n",
      "Iteration 601, loss = 0.33111359\n",
      "Iteration 602, loss = 0.33097970\n",
      "Iteration 603, loss = 0.33052638\n",
      "Iteration 604, loss = 0.33025418\n",
      "Iteration 605, loss = 0.32969374\n",
      "Iteration 606, loss = 0.32963868\n",
      "Iteration 607, loss = 0.32946457\n",
      "Iteration 608, loss = 0.32943462\n",
      "Iteration 609, loss = 0.32882426\n",
      "Iteration 610, loss = 0.32854639\n",
      "Iteration 611, loss = 0.32853705\n",
      "Iteration 612, loss = 0.32765211\n",
      "Iteration 613, loss = 0.32746718\n",
      "Iteration 614, loss = 0.32769496\n",
      "Iteration 615, loss = 0.32689332\n",
      "Iteration 616, loss = 0.32688153\n",
      "Iteration 617, loss = 0.32680321\n",
      "Iteration 618, loss = 0.32664473\n",
      "Iteration 619, loss = 0.32609596\n",
      "Iteration 620, loss = 0.32538482\n",
      "Iteration 621, loss = 0.32528972\n",
      "Iteration 622, loss = 0.32501935\n",
      "Iteration 623, loss = 0.32520980\n",
      "Iteration 624, loss = 0.32469159\n",
      "Iteration 625, loss = 0.32442586\n",
      "Iteration 626, loss = 0.32434671\n",
      "Iteration 627, loss = 0.32367822\n",
      "Iteration 628, loss = 0.32356738\n",
      "Iteration 629, loss = 0.32319837\n",
      "Iteration 630, loss = 0.32290374\n",
      "Iteration 631, loss = 0.32272898\n",
      "Iteration 632, loss = 0.32324656\n",
      "Iteration 633, loss = 0.32209264\n",
      "Iteration 634, loss = 0.32230219\n",
      "Iteration 635, loss = 0.32193617\n",
      "Iteration 636, loss = 0.32141082\n",
      "Iteration 637, loss = 0.32140870\n",
      "Iteration 638, loss = 0.32049880\n",
      "Iteration 639, loss = 0.32048303\n",
      "Iteration 640, loss = 0.32026447\n",
      "Iteration 641, loss = 0.32062596\n",
      "Iteration 642, loss = 0.32053831\n",
      "Iteration 643, loss = 0.31921780\n",
      "Iteration 644, loss = 0.32045397\n",
      "Iteration 645, loss = 0.31915724\n",
      "Iteration 646, loss = 0.31869503\n",
      "Iteration 647, loss = 0.31879810\n",
      "Iteration 648, loss = 0.31857969\n",
      "Iteration 649, loss = 0.31840786\n",
      "Iteration 650, loss = 0.31776438\n",
      "Iteration 651, loss = 0.31795567\n",
      "Iteration 652, loss = 0.31742589\n",
      "Iteration 653, loss = 0.31715735\n",
      "Iteration 654, loss = 0.31651987\n",
      "Iteration 655, loss = 0.31721641\n",
      "Iteration 656, loss = 0.31641479\n",
      "Iteration 657, loss = 0.31621589\n",
      "Iteration 658, loss = 0.31544614\n",
      "Iteration 659, loss = 0.31573916\n",
      "Iteration 660, loss = 0.31527298\n",
      "Iteration 661, loss = 0.31502213\n",
      "Iteration 662, loss = 0.31486250\n",
      "Iteration 663, loss = 0.31428208\n",
      "Iteration 664, loss = 0.31424712\n",
      "Iteration 665, loss = 0.31393046\n",
      "Iteration 666, loss = 0.31380041\n",
      "Iteration 667, loss = 0.31384549\n",
      "Iteration 668, loss = 0.31334416\n",
      "Iteration 669, loss = 0.31274873\n",
      "Iteration 670, loss = 0.31348860\n",
      "Iteration 671, loss = 0.31258943\n",
      "Iteration 672, loss = 0.31263339\n",
      "Iteration 673, loss = 0.31203636\n",
      "Iteration 674, loss = 0.31220360\n",
      "Iteration 675, loss = 0.31191669\n",
      "Iteration 676, loss = 0.31192546\n",
      "Iteration 677, loss = 0.31105163\n",
      "Iteration 678, loss = 0.31100732\n",
      "Iteration 679, loss = 0.31116069\n",
      "Iteration 680, loss = 0.31077887\n",
      "Iteration 681, loss = 0.31061076\n",
      "Iteration 682, loss = 0.31045611\n",
      "Iteration 683, loss = 0.31008104\n",
      "Iteration 684, loss = 0.30954978\n",
      "Iteration 685, loss = 0.30957319\n",
      "Iteration 686, loss = 0.30926934\n",
      "Iteration 687, loss = 0.30907738\n",
      "Iteration 688, loss = 0.30858842\n",
      "Iteration 689, loss = 0.30850450\n",
      "Iteration 690, loss = 0.30830841\n",
      "Iteration 691, loss = 0.30805621\n",
      "Iteration 692, loss = 0.30817331\n",
      "Iteration 693, loss = 0.30800677\n",
      "Iteration 694, loss = 0.30693856\n",
      "Iteration 695, loss = 0.30724445\n",
      "Iteration 696, loss = 0.30752948\n",
      "Iteration 697, loss = 0.30675378\n",
      "Iteration 698, loss = 0.30666099\n",
      "Iteration 699, loss = 0.30622471\n",
      "Iteration 700, loss = 0.30614024\n",
      "Iteration 701, loss = 0.30644826\n",
      "Iteration 702, loss = 0.30591310\n",
      "Iteration 703, loss = 0.30588526\n",
      "Iteration 704, loss = 0.30541644\n",
      "Iteration 705, loss = 0.30499654\n",
      "Iteration 706, loss = 0.30474259\n",
      "Iteration 707, loss = 0.30471559\n",
      "Iteration 708, loss = 0.30446439\n",
      "Iteration 709, loss = 0.30449404\n",
      "Iteration 710, loss = 0.30403951\n",
      "Iteration 711, loss = 0.30389668\n",
      "Iteration 712, loss = 0.30346063\n",
      "Iteration 713, loss = 0.30354814\n",
      "Iteration 714, loss = 0.30310150\n",
      "Iteration 715, loss = 0.30272054\n",
      "Iteration 716, loss = 0.30291505\n",
      "Iteration 717, loss = 0.30281804\n",
      "Iteration 718, loss = 0.30189699\n",
      "Iteration 719, loss = 0.30248408\n",
      "Iteration 720, loss = 0.30242872\n",
      "Iteration 721, loss = 0.30181415\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69574856\n",
      "Iteration 2, loss = 0.68852763\n",
      "Iteration 3, loss = 0.68191976\n",
      "Iteration 4, loss = 0.67606288\n",
      "Iteration 5, loss = 0.67085141\n",
      "Iteration 6, loss = 0.66610083\n",
      "Iteration 7, loss = 0.66171838\n",
      "Iteration 8, loss = 0.65773828\n",
      "Iteration 9, loss = 0.65407240\n",
      "Iteration 10, loss = 0.65071614\n",
      "Iteration 11, loss = 0.64758908\n",
      "Iteration 12, loss = 0.64475274\n",
      "Iteration 13, loss = 0.64217560\n",
      "Iteration 14, loss = 0.63975391\n",
      "Iteration 15, loss = 0.63748477\n",
      "Iteration 16, loss = 0.63547595\n",
      "Iteration 17, loss = 0.63354785\n",
      "Iteration 18, loss = 0.63178980\n",
      "Iteration 19, loss = 0.63016179\n",
      "Iteration 20, loss = 0.62861974\n",
      "Iteration 21, loss = 0.62719085\n",
      "Iteration 22, loss = 0.62582724\n",
      "Iteration 23, loss = 0.62455002\n",
      "Iteration 24, loss = 0.62334619\n",
      "Iteration 25, loss = 0.62222290\n",
      "Iteration 26, loss = 0.62110348\n",
      "Iteration 27, loss = 0.62003397\n",
      "Iteration 28, loss = 0.61896441\n",
      "Iteration 29, loss = 0.61797709\n",
      "Iteration 30, loss = 0.61700318\n",
      "Iteration 31, loss = 0.61605406\n",
      "Iteration 32, loss = 0.61509046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, loss = 0.61417527\n",
      "Iteration 34, loss = 0.61321808\n",
      "Iteration 35, loss = 0.61243978\n",
      "Iteration 36, loss = 0.61144525\n",
      "Iteration 37, loss = 0.61058727\n",
      "Iteration 38, loss = 0.60971794\n",
      "Iteration 39, loss = 0.60885596\n",
      "Iteration 40, loss = 0.60795624\n",
      "Iteration 41, loss = 0.60714075\n",
      "Iteration 42, loss = 0.60624028\n",
      "Iteration 43, loss = 0.60543032\n",
      "Iteration 44, loss = 0.60458994\n",
      "Iteration 45, loss = 0.60374675\n",
      "Iteration 46, loss = 0.60291459\n",
      "Iteration 47, loss = 0.60208401\n",
      "Iteration 48, loss = 0.60130721\n",
      "Iteration 49, loss = 0.60046993\n",
      "Iteration 50, loss = 0.59965154\n",
      "Iteration 51, loss = 0.59885078\n",
      "Iteration 52, loss = 0.59803752\n",
      "Iteration 53, loss = 0.59722395\n",
      "Iteration 54, loss = 0.59644009\n",
      "Iteration 55, loss = 0.59563404\n",
      "Iteration 56, loss = 0.59489530\n",
      "Iteration 57, loss = 0.59408572\n",
      "Iteration 58, loss = 0.59329151\n",
      "Iteration 59, loss = 0.59250498\n",
      "Iteration 60, loss = 0.59172935\n",
      "Iteration 61, loss = 0.59095214\n",
      "Iteration 62, loss = 0.59018954\n",
      "Iteration 63, loss = 0.58940794\n",
      "Iteration 64, loss = 0.58863990\n",
      "Iteration 65, loss = 0.58791129\n",
      "Iteration 66, loss = 0.58715716\n",
      "Iteration 67, loss = 0.58640846\n",
      "Iteration 68, loss = 0.58575075\n",
      "Iteration 69, loss = 0.58495203\n",
      "Iteration 70, loss = 0.58422602\n",
      "Iteration 71, loss = 0.58358626\n",
      "Iteration 72, loss = 0.58280909\n",
      "Iteration 73, loss = 0.58216739\n",
      "Iteration 74, loss = 0.58143602\n",
      "Iteration 75, loss = 0.58074882\n",
      "Iteration 76, loss = 0.58002059\n",
      "Iteration 77, loss = 0.57933422\n",
      "Iteration 78, loss = 0.57866880\n",
      "Iteration 79, loss = 0.57803667\n",
      "Iteration 80, loss = 0.57736752\n",
      "Iteration 81, loss = 0.57665509\n",
      "Iteration 82, loss = 0.57600054\n",
      "Iteration 83, loss = 0.57535761\n",
      "Iteration 84, loss = 0.57471209\n",
      "Iteration 85, loss = 0.57410816\n",
      "Iteration 86, loss = 0.57342110\n",
      "Iteration 87, loss = 0.57287753\n",
      "Iteration 88, loss = 0.57215423\n",
      "Iteration 89, loss = 0.57151938\n",
      "Iteration 90, loss = 0.57092909\n",
      "Iteration 91, loss = 0.57038060\n",
      "Iteration 92, loss = 0.56967768\n",
      "Iteration 93, loss = 0.56903782\n",
      "Iteration 94, loss = 0.56846720\n",
      "Iteration 95, loss = 0.56790609\n",
      "Iteration 96, loss = 0.56727729\n",
      "Iteration 97, loss = 0.56666115\n",
      "Iteration 98, loss = 0.56606806\n",
      "Iteration 99, loss = 0.56561833\n",
      "Iteration 100, loss = 0.56506443\n",
      "Iteration 101, loss = 0.56434550\n",
      "Iteration 102, loss = 0.56377424\n",
      "Iteration 103, loss = 0.56327081\n",
      "Iteration 104, loss = 0.56261578\n",
      "Iteration 105, loss = 0.56209645\n",
      "Iteration 106, loss = 0.56148547\n",
      "Iteration 107, loss = 0.56095499\n",
      "Iteration 108, loss = 0.56035022\n",
      "Iteration 109, loss = 0.55984870\n",
      "Iteration 110, loss = 0.55932351\n",
      "Iteration 111, loss = 0.55864734\n",
      "Iteration 112, loss = 0.55813397\n",
      "Iteration 113, loss = 0.55761330\n",
      "Iteration 114, loss = 0.55713358\n",
      "Iteration 115, loss = 0.55659500\n",
      "Iteration 116, loss = 0.55599829\n",
      "Iteration 117, loss = 0.55548147\n",
      "Iteration 118, loss = 0.55490002\n",
      "Iteration 119, loss = 0.55443789\n",
      "Iteration 120, loss = 0.55393362\n",
      "Iteration 121, loss = 0.55339285\n",
      "Iteration 122, loss = 0.55290376\n",
      "Iteration 123, loss = 0.55236162\n",
      "Iteration 124, loss = 0.55180429\n",
      "Iteration 125, loss = 0.55127243\n",
      "Iteration 126, loss = 0.55083758\n",
      "Iteration 127, loss = 0.55022989\n",
      "Iteration 128, loss = 0.54974347\n",
      "Iteration 129, loss = 0.54918617\n",
      "Iteration 130, loss = 0.54868377\n",
      "Iteration 131, loss = 0.54817817\n",
      "Iteration 132, loss = 0.54765426\n",
      "Iteration 133, loss = 0.54711658\n",
      "Iteration 134, loss = 0.54663493\n",
      "Iteration 135, loss = 0.54617090\n",
      "Iteration 136, loss = 0.54563666\n",
      "Iteration 137, loss = 0.54514604\n",
      "Iteration 138, loss = 0.54466121\n",
      "Iteration 139, loss = 0.54413937\n",
      "Iteration 140, loss = 0.54364057\n",
      "Iteration 141, loss = 0.54315577\n",
      "Iteration 142, loss = 0.54267599\n",
      "Iteration 143, loss = 0.54217363\n",
      "Iteration 144, loss = 0.54169767\n",
      "Iteration 145, loss = 0.54126243\n",
      "Iteration 146, loss = 0.54064579\n",
      "Iteration 147, loss = 0.54030026\n",
      "Iteration 148, loss = 0.53964686\n",
      "Iteration 149, loss = 0.53929365\n",
      "Iteration 150, loss = 0.53868913\n",
      "Iteration 151, loss = 0.53821155\n",
      "Iteration 152, loss = 0.53766518\n",
      "Iteration 153, loss = 0.53731283\n",
      "Iteration 154, loss = 0.53671796\n",
      "Iteration 155, loss = 0.53623308\n",
      "Iteration 156, loss = 0.53578696\n",
      "Iteration 157, loss = 0.53531413\n",
      "Iteration 158, loss = 0.53477574\n",
      "Iteration 159, loss = 0.53426648\n",
      "Iteration 160, loss = 0.53379259\n",
      "Iteration 161, loss = 0.53324041\n",
      "Iteration 162, loss = 0.53279580\n",
      "Iteration 163, loss = 0.53224346\n",
      "Iteration 164, loss = 0.53182921\n",
      "Iteration 165, loss = 0.53126053\n",
      "Iteration 166, loss = 0.53080222\n",
      "Iteration 167, loss = 0.53017541\n",
      "Iteration 168, loss = 0.52973002\n",
      "Iteration 169, loss = 0.52919611\n",
      "Iteration 170, loss = 0.52865910\n",
      "Iteration 171, loss = 0.52813498\n",
      "Iteration 172, loss = 0.52766836\n",
      "Iteration 173, loss = 0.52710031\n",
      "Iteration 174, loss = 0.52656084\n",
      "Iteration 175, loss = 0.52610740\n",
      "Iteration 176, loss = 0.52556958\n",
      "Iteration 177, loss = 0.52506432\n",
      "Iteration 178, loss = 0.52458308\n",
      "Iteration 179, loss = 0.52401099\n",
      "Iteration 180, loss = 0.52346966\n",
      "Iteration 181, loss = 0.52303641\n",
      "Iteration 182, loss = 0.52254096\n",
      "Iteration 183, loss = 0.52199261\n",
      "Iteration 184, loss = 0.52142408\n",
      "Iteration 185, loss = 0.52085542\n",
      "Iteration 186, loss = 0.52038137\n",
      "Iteration 187, loss = 0.51992140\n",
      "Iteration 188, loss = 0.51933932\n",
      "Iteration 189, loss = 0.51887879\n",
      "Iteration 190, loss = 0.51824295\n",
      "Iteration 191, loss = 0.51771740\n",
      "Iteration 192, loss = 0.51728715\n",
      "Iteration 193, loss = 0.51669619\n",
      "Iteration 194, loss = 0.51620398\n",
      "Iteration 195, loss = 0.51565338\n",
      "Iteration 196, loss = 0.51511962\n",
      "Iteration 197, loss = 0.51460979\n",
      "Iteration 198, loss = 0.51407176\n",
      "Iteration 199, loss = 0.51354600\n",
      "Iteration 200, loss = 0.51302714\n",
      "Iteration 201, loss = 0.51251211\n",
      "Iteration 202, loss = 0.51197228\n",
      "Iteration 203, loss = 0.51140406\n",
      "Iteration 204, loss = 0.51091772\n",
      "Iteration 205, loss = 0.51044787\n",
      "Iteration 206, loss = 0.50992625\n",
      "Iteration 207, loss = 0.50933643\n",
      "Iteration 208, loss = 0.50878784\n",
      "Iteration 209, loss = 0.50836860\n",
      "Iteration 210, loss = 0.50770591\n",
      "Iteration 211, loss = 0.50728722\n",
      "Iteration 212, loss = 0.50681682\n",
      "Iteration 213, loss = 0.50618713\n",
      "Iteration 214, loss = 0.50568000\n",
      "Iteration 215, loss = 0.50517179\n",
      "Iteration 216, loss = 0.50454567\n",
      "Iteration 217, loss = 0.50417283\n",
      "Iteration 218, loss = 0.50350703\n",
      "Iteration 219, loss = 0.50303778\n",
      "Iteration 220, loss = 0.50243041\n",
      "Iteration 221, loss = 0.50195090\n",
      "Iteration 222, loss = 0.50130125\n",
      "Iteration 223, loss = 0.50074996\n",
      "Iteration 224, loss = 0.50033645\n",
      "Iteration 225, loss = 0.49958816\n",
      "Iteration 226, loss = 0.49926099\n",
      "Iteration 227, loss = 0.49868099\n",
      "Iteration 228, loss = 0.49805001\n",
      "Iteration 229, loss = 0.49748304\n",
      "Iteration 230, loss = 0.49686648\n",
      "Iteration 231, loss = 0.49629704\n",
      "Iteration 232, loss = 0.49578628\n",
      "Iteration 233, loss = 0.49519335\n",
      "Iteration 234, loss = 0.49469858\n",
      "Iteration 235, loss = 0.49410605\n",
      "Iteration 236, loss = 0.49351923\n",
      "Iteration 237, loss = 0.49294995\n",
      "Iteration 238, loss = 0.49252401\n",
      "Iteration 239, loss = 0.49190740\n",
      "Iteration 240, loss = 0.49139173\n",
      "Iteration 241, loss = 0.49077992\n",
      "Iteration 242, loss = 0.49021870\n",
      "Iteration 243, loss = 0.48960280\n",
      "Iteration 244, loss = 0.48914701\n",
      "Iteration 245, loss = 0.48845515\n",
      "Iteration 246, loss = 0.48797315\n",
      "Iteration 247, loss = 0.48746883\n",
      "Iteration 248, loss = 0.48669468\n",
      "Iteration 249, loss = 0.48619925\n",
      "Iteration 250, loss = 0.48559953\n",
      "Iteration 251, loss = 0.48506149\n",
      "Iteration 252, loss = 0.48441152\n",
      "Iteration 253, loss = 0.48399381\n",
      "Iteration 254, loss = 0.48327827\n",
      "Iteration 255, loss = 0.48284388\n",
      "Iteration 256, loss = 0.48214770\n",
      "Iteration 257, loss = 0.48155705\n",
      "Iteration 258, loss = 0.48120137\n",
      "Iteration 259, loss = 0.48041219\n",
      "Iteration 260, loss = 0.47989557\n",
      "Iteration 261, loss = 0.47932112\n",
      "Iteration 262, loss = 0.47864829\n",
      "Iteration 263, loss = 0.47817744\n",
      "Iteration 264, loss = 0.47762401\n",
      "Iteration 265, loss = 0.47703144\n",
      "Iteration 266, loss = 0.47636560\n",
      "Iteration 267, loss = 0.47581889\n",
      "Iteration 268, loss = 0.47530451\n",
      "Iteration 269, loss = 0.47485945\n",
      "Iteration 270, loss = 0.47438164\n",
      "Iteration 271, loss = 0.47387644\n",
      "Iteration 272, loss = 0.47321370\n",
      "Iteration 273, loss = 0.47281761\n",
      "Iteration 274, loss = 0.47207769\n",
      "Iteration 275, loss = 0.47138639\n",
      "Iteration 276, loss = 0.47080838\n",
      "Iteration 277, loss = 0.47038475\n",
      "Iteration 278, loss = 0.46972937\n",
      "Iteration 279, loss = 0.46922905\n",
      "Iteration 280, loss = 0.46860556\n",
      "Iteration 281, loss = 0.46811002\n",
      "Iteration 282, loss = 0.46766016\n",
      "Iteration 283, loss = 0.46696163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 284, loss = 0.46656067\n",
      "Iteration 285, loss = 0.46604223\n",
      "Iteration 286, loss = 0.46531203\n",
      "Iteration 287, loss = 0.46479183\n",
      "Iteration 288, loss = 0.46434173\n",
      "Iteration 289, loss = 0.46368276\n",
      "Iteration 290, loss = 0.46331843\n",
      "Iteration 291, loss = 0.46271926\n",
      "Iteration 292, loss = 0.46201995\n",
      "Iteration 293, loss = 0.46174879\n",
      "Iteration 294, loss = 0.46106532\n",
      "Iteration 295, loss = 0.46049584\n",
      "Iteration 296, loss = 0.46002670\n",
      "Iteration 297, loss = 0.45951675\n",
      "Iteration 298, loss = 0.45897695\n",
      "Iteration 299, loss = 0.45853741\n",
      "Iteration 300, loss = 0.45806258\n",
      "Iteration 301, loss = 0.45750151\n",
      "Iteration 302, loss = 0.45692672\n",
      "Iteration 303, loss = 0.45651352\n",
      "Iteration 304, loss = 0.45575077\n",
      "Iteration 305, loss = 0.45530380\n",
      "Iteration 306, loss = 0.45494085\n",
      "Iteration 307, loss = 0.45435494\n",
      "Iteration 308, loss = 0.45374946\n",
      "Iteration 309, loss = 0.45351552\n",
      "Iteration 310, loss = 0.45283067\n",
      "Iteration 311, loss = 0.45239183\n",
      "Iteration 312, loss = 0.45179855\n",
      "Iteration 313, loss = 0.45111775\n",
      "Iteration 314, loss = 0.45078923\n",
      "Iteration 315, loss = 0.45030484\n",
      "Iteration 316, loss = 0.44975427\n",
      "Iteration 317, loss = 0.44917590\n",
      "Iteration 318, loss = 0.44888503\n",
      "Iteration 319, loss = 0.44817092\n",
      "Iteration 320, loss = 0.44769731\n",
      "Iteration 321, loss = 0.44721557\n",
      "Iteration 322, loss = 0.44676283\n",
      "Iteration 323, loss = 0.44631667\n",
      "Iteration 324, loss = 0.44569908\n",
      "Iteration 325, loss = 0.44515826\n",
      "Iteration 326, loss = 0.44466117\n",
      "Iteration 327, loss = 0.44428437\n",
      "Iteration 328, loss = 0.44378403\n",
      "Iteration 329, loss = 0.44319245\n",
      "Iteration 330, loss = 0.44267125\n",
      "Iteration 331, loss = 0.44238181\n",
      "Iteration 332, loss = 0.44156513\n",
      "Iteration 333, loss = 0.44123098\n",
      "Iteration 334, loss = 0.44083774\n",
      "Iteration 335, loss = 0.44011244\n",
      "Iteration 336, loss = 0.43967434\n",
      "Iteration 337, loss = 0.43929182\n",
      "Iteration 338, loss = 0.43870856\n",
      "Iteration 339, loss = 0.43806955\n",
      "Iteration 340, loss = 0.43770985\n",
      "Iteration 341, loss = 0.43738577\n",
      "Iteration 342, loss = 0.43673962\n",
      "Iteration 343, loss = 0.43626709\n",
      "Iteration 344, loss = 0.43609558\n",
      "Iteration 345, loss = 0.43535735\n",
      "Iteration 346, loss = 0.43483993\n",
      "Iteration 347, loss = 0.43434957\n",
      "Iteration 348, loss = 0.43383708\n",
      "Iteration 349, loss = 0.43343648\n",
      "Iteration 350, loss = 0.43296682\n",
      "Iteration 351, loss = 0.43257140\n",
      "Iteration 352, loss = 0.43203372\n",
      "Iteration 353, loss = 0.43158544\n",
      "Iteration 354, loss = 0.43109281\n",
      "Iteration 355, loss = 0.43051959\n",
      "Iteration 356, loss = 0.43009437\n",
      "Iteration 357, loss = 0.42956345\n",
      "Iteration 358, loss = 0.42913969\n",
      "Iteration 359, loss = 0.42904623\n",
      "Iteration 360, loss = 0.42812013\n",
      "Iteration 361, loss = 0.42780181\n",
      "Iteration 362, loss = 0.42718755\n",
      "Iteration 363, loss = 0.42675123\n",
      "Iteration 364, loss = 0.42632493\n",
      "Iteration 365, loss = 0.42594850\n",
      "Iteration 366, loss = 0.42542571\n",
      "Iteration 367, loss = 0.42497778\n",
      "Iteration 368, loss = 0.42446373\n",
      "Iteration 369, loss = 0.42392442\n",
      "Iteration 370, loss = 0.42371325\n",
      "Iteration 371, loss = 0.42309530\n",
      "Iteration 372, loss = 0.42283841\n",
      "Iteration 373, loss = 0.42222822\n",
      "Iteration 374, loss = 0.42188178\n",
      "Iteration 375, loss = 0.42135104\n",
      "Iteration 376, loss = 0.42088249\n",
      "Iteration 377, loss = 0.42062477\n",
      "Iteration 378, loss = 0.42002553\n",
      "Iteration 379, loss = 0.41955954\n",
      "Iteration 380, loss = 0.41913938\n",
      "Iteration 381, loss = 0.41872126\n",
      "Iteration 382, loss = 0.41821423\n",
      "Iteration 383, loss = 0.41775158\n",
      "Iteration 384, loss = 0.41720517\n",
      "Iteration 385, loss = 0.41714937\n",
      "Iteration 386, loss = 0.41643365\n",
      "Iteration 387, loss = 0.41614811\n",
      "Iteration 388, loss = 0.41542821\n",
      "Iteration 389, loss = 0.41493634\n",
      "Iteration 390, loss = 0.41497362\n",
      "Iteration 391, loss = 0.41447856\n",
      "Iteration 392, loss = 0.41355062\n",
      "Iteration 393, loss = 0.41343434\n",
      "Iteration 394, loss = 0.41322521\n",
      "Iteration 395, loss = 0.41237525\n",
      "Iteration 396, loss = 0.41188281\n",
      "Iteration 397, loss = 0.41165141\n",
      "Iteration 398, loss = 0.41120387\n",
      "Iteration 399, loss = 0.41077030\n",
      "Iteration 400, loss = 0.41030298\n",
      "Iteration 401, loss = 0.40983170\n",
      "Iteration 402, loss = 0.40957032\n",
      "Iteration 403, loss = 0.40935967\n",
      "Iteration 404, loss = 0.40902296\n",
      "Iteration 405, loss = 0.40816566\n",
      "Iteration 406, loss = 0.40763128\n",
      "Iteration 407, loss = 0.40728831\n",
      "Iteration 408, loss = 0.40685814\n",
      "Iteration 409, loss = 0.40642301\n",
      "Iteration 410, loss = 0.40646558\n",
      "Iteration 411, loss = 0.40564746\n",
      "Iteration 412, loss = 0.40533987\n",
      "Iteration 413, loss = 0.40482885\n",
      "Iteration 414, loss = 0.40455129\n",
      "Iteration 415, loss = 0.40411274\n",
      "Iteration 416, loss = 0.40357977\n",
      "Iteration 417, loss = 0.40317870\n",
      "Iteration 418, loss = 0.40266864\n",
      "Iteration 419, loss = 0.40222320\n",
      "Iteration 420, loss = 0.40194957\n",
      "Iteration 421, loss = 0.40153997\n",
      "Iteration 422, loss = 0.40094895\n",
      "Iteration 423, loss = 0.40058182\n",
      "Iteration 424, loss = 0.40050187\n",
      "Iteration 425, loss = 0.39974559\n",
      "Iteration 426, loss = 0.39953025\n",
      "Iteration 427, loss = 0.39925740\n",
      "Iteration 428, loss = 0.39852907\n",
      "Iteration 429, loss = 0.39821432\n",
      "Iteration 430, loss = 0.39776696\n",
      "Iteration 431, loss = 0.39739484\n",
      "Iteration 432, loss = 0.39712794\n",
      "Iteration 433, loss = 0.39664657\n",
      "Iteration 434, loss = 0.39630376\n",
      "Iteration 435, loss = 0.39601459\n",
      "Iteration 436, loss = 0.39536349\n",
      "Iteration 437, loss = 0.39503687\n",
      "Iteration 438, loss = 0.39461641\n",
      "Iteration 439, loss = 0.39425454\n",
      "Iteration 440, loss = 0.39387223\n",
      "Iteration 441, loss = 0.39336667\n",
      "Iteration 442, loss = 0.39300998\n",
      "Iteration 443, loss = 0.39265353\n",
      "Iteration 444, loss = 0.39227942\n",
      "Iteration 445, loss = 0.39165161\n",
      "Iteration 446, loss = 0.39127669\n",
      "Iteration 447, loss = 0.39103078\n",
      "Iteration 448, loss = 0.39054176\n",
      "Iteration 449, loss = 0.39035481\n",
      "Iteration 450, loss = 0.38974031\n",
      "Iteration 451, loss = 0.38931370\n",
      "Iteration 452, loss = 0.38906723\n",
      "Iteration 453, loss = 0.38855535\n",
      "Iteration 454, loss = 0.38847239\n",
      "Iteration 455, loss = 0.38833013\n",
      "Iteration 456, loss = 0.38728753\n",
      "Iteration 457, loss = 0.38721841\n",
      "Iteration 458, loss = 0.38675864\n",
      "Iteration 459, loss = 0.38617807\n",
      "Iteration 460, loss = 0.38570107\n",
      "Iteration 461, loss = 0.38571831\n",
      "Iteration 462, loss = 0.38525953\n",
      "Iteration 463, loss = 0.38517881\n",
      "Iteration 464, loss = 0.38423840\n",
      "Iteration 465, loss = 0.38413434\n",
      "Iteration 466, loss = 0.38349199\n",
      "Iteration 467, loss = 0.38326794\n",
      "Iteration 468, loss = 0.38287817\n",
      "Iteration 469, loss = 0.38247729\n",
      "Iteration 470, loss = 0.38200460\n",
      "Iteration 471, loss = 0.38172060\n",
      "Iteration 472, loss = 0.38137809\n",
      "Iteration 473, loss = 0.38079782\n",
      "Iteration 474, loss = 0.38061673\n",
      "Iteration 475, loss = 0.38028316\n",
      "Iteration 476, loss = 0.37965916\n",
      "Iteration 477, loss = 0.37926426\n",
      "Iteration 478, loss = 0.37912626\n",
      "Iteration 479, loss = 0.37870250\n",
      "Iteration 480, loss = 0.37841388\n",
      "Iteration 481, loss = 0.37802110\n",
      "Iteration 482, loss = 0.37755473\n",
      "Iteration 483, loss = 0.37722609\n",
      "Iteration 484, loss = 0.37694468\n",
      "Iteration 485, loss = 0.37640622\n",
      "Iteration 486, loss = 0.37599790\n",
      "Iteration 487, loss = 0.37591296\n",
      "Iteration 488, loss = 0.37537203\n",
      "Iteration 489, loss = 0.37537464\n",
      "Iteration 490, loss = 0.37444680\n",
      "Iteration 491, loss = 0.37413805\n",
      "Iteration 492, loss = 0.37398192\n",
      "Iteration 493, loss = 0.37314568\n",
      "Iteration 494, loss = 0.37282719\n",
      "Iteration 495, loss = 0.37250518\n",
      "Iteration 496, loss = 0.37236796\n",
      "Iteration 497, loss = 0.37181399\n",
      "Iteration 498, loss = 0.37155938\n",
      "Iteration 499, loss = 0.37132352\n",
      "Iteration 500, loss = 0.37089730\n",
      "Iteration 501, loss = 0.37028239\n",
      "Iteration 502, loss = 0.37047473\n",
      "Iteration 503, loss = 0.36960845\n",
      "Iteration 504, loss = 0.36933663\n",
      "Iteration 505, loss = 0.36886100\n",
      "Iteration 506, loss = 0.36878120\n",
      "Iteration 507, loss = 0.36819111\n",
      "Iteration 508, loss = 0.36754677\n",
      "Iteration 509, loss = 0.36765947\n",
      "Iteration 510, loss = 0.36692530\n",
      "Iteration 511, loss = 0.36652244\n",
      "Iteration 512, loss = 0.36603847\n",
      "Iteration 513, loss = 0.36615871\n",
      "Iteration 514, loss = 0.36538561\n",
      "Iteration 515, loss = 0.36551572\n",
      "Iteration 516, loss = 0.36468196\n",
      "Iteration 517, loss = 0.36462818\n",
      "Iteration 518, loss = 0.36426604\n",
      "Iteration 519, loss = 0.36374745\n",
      "Iteration 520, loss = 0.36333953\n",
      "Iteration 521, loss = 0.36295163\n",
      "Iteration 522, loss = 0.36292406\n",
      "Iteration 523, loss = 0.36231525\n",
      "Iteration 524, loss = 0.36185299\n",
      "Iteration 525, loss = 0.36219400\n",
      "Iteration 526, loss = 0.36148956\n",
      "Iteration 527, loss = 0.36098930\n",
      "Iteration 528, loss = 0.36035560\n",
      "Iteration 529, loss = 0.36047826\n",
      "Iteration 530, loss = 0.36011505\n",
      "Iteration 531, loss = 0.35992291\n",
      "Iteration 532, loss = 0.35903497\n",
      "Iteration 533, loss = 0.35867090\n",
      "Iteration 534, loss = 0.35820817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 535, loss = 0.35837965\n",
      "Iteration 536, loss = 0.35762813\n",
      "Iteration 537, loss = 0.35775939\n",
      "Iteration 538, loss = 0.35698868\n",
      "Iteration 539, loss = 0.35649865\n",
      "Iteration 540, loss = 0.35650097\n",
      "Iteration 541, loss = 0.35608244\n",
      "Iteration 542, loss = 0.35597913\n",
      "Iteration 543, loss = 0.35515868\n",
      "Iteration 544, loss = 0.35477768\n",
      "Iteration 545, loss = 0.35460270\n",
      "Iteration 546, loss = 0.35404839\n",
      "Iteration 547, loss = 0.35441346\n",
      "Iteration 548, loss = 0.35337958\n",
      "Iteration 549, loss = 0.35320569\n",
      "Iteration 550, loss = 0.35282307\n",
      "Iteration 551, loss = 0.35236940\n",
      "Iteration 552, loss = 0.35216240\n",
      "Iteration 553, loss = 0.35191856\n",
      "Iteration 554, loss = 0.35140335\n",
      "Iteration 555, loss = 0.35099637\n",
      "Iteration 556, loss = 0.35070127\n",
      "Iteration 557, loss = 0.35063240\n",
      "Iteration 558, loss = 0.35034601\n",
      "Iteration 559, loss = 0.35000584\n",
      "Iteration 560, loss = 0.34941182\n",
      "Iteration 561, loss = 0.34902265\n",
      "Iteration 562, loss = 0.34872160\n",
      "Iteration 563, loss = 0.34879936\n",
      "Iteration 564, loss = 0.34807857\n",
      "Iteration 565, loss = 0.34783181\n",
      "Iteration 566, loss = 0.34791699\n",
      "Iteration 567, loss = 0.34693667\n",
      "Iteration 568, loss = 0.34685894\n",
      "Iteration 569, loss = 0.34654951\n",
      "Iteration 570, loss = 0.34620630\n",
      "Iteration 571, loss = 0.34593546\n",
      "Iteration 572, loss = 0.34557392\n",
      "Iteration 573, loss = 0.34517343\n",
      "Iteration 574, loss = 0.34527442\n",
      "Iteration 575, loss = 0.34457506\n",
      "Iteration 576, loss = 0.34481724\n",
      "Iteration 577, loss = 0.34404358\n",
      "Iteration 578, loss = 0.34405247\n",
      "Iteration 579, loss = 0.34363831\n",
      "Iteration 580, loss = 0.34310471\n",
      "Iteration 581, loss = 0.34293640\n",
      "Iteration 582, loss = 0.34257995\n",
      "Iteration 583, loss = 0.34210359\n",
      "Iteration 584, loss = 0.34161528\n",
      "Iteration 585, loss = 0.34125573\n",
      "Iteration 586, loss = 0.34166847\n",
      "Iteration 587, loss = 0.34102715\n",
      "Iteration 588, loss = 0.34058391\n",
      "Iteration 589, loss = 0.34033662\n",
      "Iteration 590, loss = 0.33988832\n",
      "Iteration 591, loss = 0.33932034\n",
      "Iteration 592, loss = 0.33916933\n",
      "Iteration 593, loss = 0.33913784\n",
      "Iteration 594, loss = 0.33898377\n",
      "Iteration 595, loss = 0.33836572\n",
      "Iteration 596, loss = 0.33843282\n",
      "Iteration 597, loss = 0.33735203\n",
      "Iteration 598, loss = 0.33763036\n",
      "Iteration 599, loss = 0.33705621\n",
      "Iteration 600, loss = 0.33685877\n",
      "Iteration 601, loss = 0.33645517\n",
      "Iteration 602, loss = 0.33637807\n",
      "Iteration 603, loss = 0.33586055\n",
      "Iteration 604, loss = 0.33540885\n",
      "Iteration 605, loss = 0.33545006\n",
      "Iteration 606, loss = 0.33514754\n",
      "Iteration 607, loss = 0.33465489\n",
      "Iteration 608, loss = 0.33437145\n",
      "Iteration 609, loss = 0.33428442\n",
      "Iteration 610, loss = 0.33375109\n",
      "Iteration 611, loss = 0.33336094\n",
      "Iteration 612, loss = 0.33318000\n",
      "Iteration 613, loss = 0.33279042\n",
      "Iteration 614, loss = 0.33293252\n",
      "Iteration 615, loss = 0.33212930\n",
      "Iteration 616, loss = 0.33188419\n",
      "Iteration 617, loss = 0.33214342\n",
      "Iteration 618, loss = 0.33159223\n",
      "Iteration 619, loss = 0.33098301\n",
      "Iteration 620, loss = 0.33097989\n",
      "Iteration 621, loss = 0.33132945\n",
      "Iteration 622, loss = 0.33030384\n",
      "Iteration 623, loss = 0.32988510\n",
      "Iteration 624, loss = 0.32990080\n",
      "Iteration 625, loss = 0.32927784\n",
      "Iteration 626, loss = 0.32924308\n",
      "Iteration 627, loss = 0.32918061\n",
      "Iteration 628, loss = 0.32915289\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69625593\n",
      "Iteration 2, loss = 0.68851666\n",
      "Iteration 3, loss = 0.68141882\n",
      "Iteration 4, loss = 0.67508374\n",
      "Iteration 5, loss = 0.66942362\n",
      "Iteration 6, loss = 0.66425586\n",
      "Iteration 7, loss = 0.65951353\n",
      "Iteration 8, loss = 0.65513025\n",
      "Iteration 9, loss = 0.65111961\n",
      "Iteration 10, loss = 0.64739838\n",
      "Iteration 11, loss = 0.64401878\n",
      "Iteration 12, loss = 0.64087591\n",
      "Iteration 13, loss = 0.63800376\n",
      "Iteration 14, loss = 0.63537351\n",
      "Iteration 15, loss = 0.63291223\n",
      "Iteration 16, loss = 0.63072379\n",
      "Iteration 17, loss = 0.62859864\n",
      "Iteration 18, loss = 0.62666933\n",
      "Iteration 19, loss = 0.62489872\n",
      "Iteration 20, loss = 0.62323382\n",
      "Iteration 21, loss = 0.62168426\n",
      "Iteration 22, loss = 0.62021657\n",
      "Iteration 23, loss = 0.61882220\n",
      "Iteration 24, loss = 0.61752856\n",
      "Iteration 25, loss = 0.61634560\n",
      "Iteration 26, loss = 0.61511136\n",
      "Iteration 27, loss = 0.61395902\n",
      "Iteration 28, loss = 0.61284994\n",
      "Iteration 29, loss = 0.61184422\n",
      "Iteration 30, loss = 0.61074042\n",
      "Iteration 31, loss = 0.60971145\n",
      "Iteration 32, loss = 0.60871165\n",
      "Iteration 33, loss = 0.60769813\n",
      "Iteration 34, loss = 0.60671135\n",
      "Iteration 35, loss = 0.60580006\n",
      "Iteration 36, loss = 0.60486521\n",
      "Iteration 37, loss = 0.60392917\n",
      "Iteration 38, loss = 0.60293783\n",
      "Iteration 39, loss = 0.60203686\n",
      "Iteration 40, loss = 0.60109488\n",
      "Iteration 41, loss = 0.60020701\n",
      "Iteration 42, loss = 0.59931572\n",
      "Iteration 43, loss = 0.59840207\n",
      "Iteration 44, loss = 0.59752898\n",
      "Iteration 45, loss = 0.59660882\n",
      "Iteration 46, loss = 0.59576742\n",
      "Iteration 47, loss = 0.59484266\n",
      "Iteration 48, loss = 0.59398707\n",
      "Iteration 49, loss = 0.59313872\n",
      "Iteration 50, loss = 0.59224964\n",
      "Iteration 51, loss = 0.59138264\n",
      "Iteration 52, loss = 0.59054042\n",
      "Iteration 53, loss = 0.58971164\n",
      "Iteration 54, loss = 0.58885107\n",
      "Iteration 55, loss = 0.58801747\n",
      "Iteration 56, loss = 0.58721375\n",
      "Iteration 57, loss = 0.58637113\n",
      "Iteration 58, loss = 0.58554722\n",
      "Iteration 59, loss = 0.58475101\n",
      "Iteration 60, loss = 0.58392613\n",
      "Iteration 61, loss = 0.58314143\n",
      "Iteration 62, loss = 0.58230464\n",
      "Iteration 63, loss = 0.58154851\n",
      "Iteration 64, loss = 0.58070713\n",
      "Iteration 65, loss = 0.57993950\n",
      "Iteration 66, loss = 0.57917371\n",
      "Iteration 67, loss = 0.57841078\n",
      "Iteration 68, loss = 0.57764696\n",
      "Iteration 69, loss = 0.57688821\n",
      "Iteration 70, loss = 0.57613153\n",
      "Iteration 71, loss = 0.57540361\n",
      "Iteration 72, loss = 0.57461900\n",
      "Iteration 73, loss = 0.57390696\n",
      "Iteration 74, loss = 0.57318521\n",
      "Iteration 75, loss = 0.57245424\n",
      "Iteration 76, loss = 0.57174390\n",
      "Iteration 77, loss = 0.57100355\n",
      "Iteration 78, loss = 0.57028748\n",
      "Iteration 79, loss = 0.56962053\n",
      "Iteration 80, loss = 0.56892960\n",
      "Iteration 81, loss = 0.56822693\n",
      "Iteration 82, loss = 0.56756039\n",
      "Iteration 83, loss = 0.56692359\n",
      "Iteration 84, loss = 0.56622700\n",
      "Iteration 85, loss = 0.56558266\n",
      "Iteration 86, loss = 0.56492972\n",
      "Iteration 87, loss = 0.56438801\n",
      "Iteration 88, loss = 0.56365363\n",
      "Iteration 89, loss = 0.56303687\n",
      "Iteration 90, loss = 0.56237925\n",
      "Iteration 91, loss = 0.56182670\n",
      "Iteration 92, loss = 0.56112798\n",
      "Iteration 93, loss = 0.56051304\n",
      "Iteration 94, loss = 0.55992585\n",
      "Iteration 95, loss = 0.55934104\n",
      "Iteration 96, loss = 0.55873694\n",
      "Iteration 97, loss = 0.55811960\n",
      "Iteration 98, loss = 0.55751828\n",
      "Iteration 99, loss = 0.55696693\n",
      "Iteration 100, loss = 0.55643899\n",
      "Iteration 101, loss = 0.55576505\n",
      "Iteration 102, loss = 0.55524995\n",
      "Iteration 103, loss = 0.55470439\n",
      "Iteration 104, loss = 0.55410004\n",
      "Iteration 105, loss = 0.55361097\n",
      "Iteration 106, loss = 0.55293346\n",
      "Iteration 107, loss = 0.55240297\n",
      "Iteration 108, loss = 0.55183523\n",
      "Iteration 109, loss = 0.55132515\n",
      "Iteration 110, loss = 0.55075332\n",
      "Iteration 111, loss = 0.55013797\n",
      "Iteration 112, loss = 0.54961331\n",
      "Iteration 113, loss = 0.54906366\n",
      "Iteration 114, loss = 0.54850339\n",
      "Iteration 115, loss = 0.54799367\n",
      "Iteration 116, loss = 0.54748902\n",
      "Iteration 117, loss = 0.54697542\n",
      "Iteration 118, loss = 0.54638183\n",
      "Iteration 119, loss = 0.54590103\n",
      "Iteration 120, loss = 0.54535407\n",
      "Iteration 121, loss = 0.54483935\n",
      "Iteration 122, loss = 0.54434423\n",
      "Iteration 123, loss = 0.54383251\n",
      "Iteration 124, loss = 0.54330107\n",
      "Iteration 125, loss = 0.54275722\n",
      "Iteration 126, loss = 0.54243129\n",
      "Iteration 127, loss = 0.54176752\n",
      "Iteration 128, loss = 0.54120572\n",
      "Iteration 129, loss = 0.54076831\n",
      "Iteration 130, loss = 0.54022224\n",
      "Iteration 131, loss = 0.53967159\n",
      "Iteration 132, loss = 0.53924803\n",
      "Iteration 133, loss = 0.53871116\n",
      "Iteration 134, loss = 0.53818221\n",
      "Iteration 135, loss = 0.53775384\n",
      "Iteration 136, loss = 0.53715804\n",
      "Iteration 137, loss = 0.53670359\n",
      "Iteration 138, loss = 0.53624260\n",
      "Iteration 139, loss = 0.53577308\n",
      "Iteration 140, loss = 0.53529065\n",
      "Iteration 141, loss = 0.53479590\n",
      "Iteration 142, loss = 0.53430575\n",
      "Iteration 143, loss = 0.53384173\n",
      "Iteration 144, loss = 0.53328042\n",
      "Iteration 145, loss = 0.53287015\n",
      "Iteration 146, loss = 0.53235169\n",
      "Iteration 147, loss = 0.53196419\n",
      "Iteration 148, loss = 0.53136878\n",
      "Iteration 149, loss = 0.53091682\n",
      "Iteration 150, loss = 0.53038890\n",
      "Iteration 151, loss = 0.52994271\n",
      "Iteration 152, loss = 0.52939528\n",
      "Iteration 153, loss = 0.52907413\n",
      "Iteration 154, loss = 0.52854817\n",
      "Iteration 155, loss = 0.52797152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 156, loss = 0.52756769\n",
      "Iteration 157, loss = 0.52712880\n",
      "Iteration 158, loss = 0.52668091\n",
      "Iteration 159, loss = 0.52617924\n",
      "Iteration 160, loss = 0.52572911\n",
      "Iteration 161, loss = 0.52517189\n",
      "Iteration 162, loss = 0.52484836\n",
      "Iteration 163, loss = 0.52426426\n",
      "Iteration 164, loss = 0.52384712\n",
      "Iteration 165, loss = 0.52330105\n",
      "Iteration 166, loss = 0.52280750\n",
      "Iteration 167, loss = 0.52232443\n",
      "Iteration 168, loss = 0.52190283\n",
      "Iteration 169, loss = 0.52133271\n",
      "Iteration 170, loss = 0.52083777\n",
      "Iteration 171, loss = 0.52043651\n",
      "Iteration 172, loss = 0.51994420\n",
      "Iteration 173, loss = 0.51946673\n",
      "Iteration 174, loss = 0.51892253\n",
      "Iteration 175, loss = 0.51844441\n",
      "Iteration 176, loss = 0.51803379\n",
      "Iteration 177, loss = 0.51744601\n",
      "Iteration 178, loss = 0.51697380\n",
      "Iteration 179, loss = 0.51644118\n",
      "Iteration 180, loss = 0.51597045\n",
      "Iteration 181, loss = 0.51560095\n",
      "Iteration 182, loss = 0.51503724\n",
      "Iteration 183, loss = 0.51453080\n",
      "Iteration 184, loss = 0.51403908\n",
      "Iteration 185, loss = 0.51351651\n",
      "Iteration 186, loss = 0.51298814\n",
      "Iteration 187, loss = 0.51258681\n",
      "Iteration 188, loss = 0.51203192\n",
      "Iteration 189, loss = 0.51164184\n",
      "Iteration 190, loss = 0.51102570\n",
      "Iteration 191, loss = 0.51062626\n",
      "Iteration 192, loss = 0.51020023\n",
      "Iteration 193, loss = 0.50964918\n",
      "Iteration 194, loss = 0.50903026\n",
      "Iteration 195, loss = 0.50870216\n",
      "Iteration 196, loss = 0.50807834\n",
      "Iteration 197, loss = 0.50774347\n",
      "Iteration 198, loss = 0.50718909\n",
      "Iteration 199, loss = 0.50669614\n",
      "Iteration 200, loss = 0.50616997\n",
      "Iteration 201, loss = 0.50559916\n",
      "Iteration 202, loss = 0.50526957\n",
      "Iteration 203, loss = 0.50463311\n",
      "Iteration 204, loss = 0.50409533\n",
      "Iteration 205, loss = 0.50366167\n",
      "Iteration 206, loss = 0.50314953\n",
      "Iteration 207, loss = 0.50265646\n",
      "Iteration 208, loss = 0.50215872\n",
      "Iteration 209, loss = 0.50160531\n",
      "Iteration 210, loss = 0.50112776\n",
      "Iteration 211, loss = 0.50071895\n",
      "Iteration 212, loss = 0.50018741\n",
      "Iteration 213, loss = 0.49952832\n",
      "Iteration 214, loss = 0.49901383\n",
      "Iteration 215, loss = 0.49849199\n",
      "Iteration 216, loss = 0.49796276\n",
      "Iteration 217, loss = 0.49757512\n",
      "Iteration 218, loss = 0.49691334\n",
      "Iteration 219, loss = 0.49648783\n",
      "Iteration 220, loss = 0.49605469\n",
      "Iteration 221, loss = 0.49539828\n",
      "Iteration 222, loss = 0.49493344\n",
      "Iteration 223, loss = 0.49433390\n",
      "Iteration 224, loss = 0.49389528\n",
      "Iteration 225, loss = 0.49334479\n",
      "Iteration 226, loss = 0.49292216\n",
      "Iteration 227, loss = 0.49233887\n",
      "Iteration 228, loss = 0.49180598\n",
      "Iteration 229, loss = 0.49123240\n",
      "Iteration 230, loss = 0.49073009\n",
      "Iteration 231, loss = 0.49017707\n",
      "Iteration 232, loss = 0.48970287\n",
      "Iteration 233, loss = 0.48912358\n",
      "Iteration 234, loss = 0.48859656\n",
      "Iteration 235, loss = 0.48819252\n",
      "Iteration 236, loss = 0.48746598\n",
      "Iteration 237, loss = 0.48709433\n",
      "Iteration 238, loss = 0.48653757\n",
      "Iteration 239, loss = 0.48595145\n",
      "Iteration 240, loss = 0.48543060\n",
      "Iteration 241, loss = 0.48489480\n",
      "Iteration 242, loss = 0.48444850\n",
      "Iteration 243, loss = 0.48393364\n",
      "Iteration 244, loss = 0.48333829\n",
      "Iteration 245, loss = 0.48275349\n",
      "Iteration 246, loss = 0.48224397\n",
      "Iteration 247, loss = 0.48169551\n",
      "Iteration 248, loss = 0.48109119\n",
      "Iteration 249, loss = 0.48067324\n",
      "Iteration 250, loss = 0.48005379\n",
      "Iteration 251, loss = 0.47965107\n",
      "Iteration 252, loss = 0.47897398\n",
      "Iteration 253, loss = 0.47851201\n",
      "Iteration 254, loss = 0.47796869\n",
      "Iteration 255, loss = 0.47764910\n",
      "Iteration 256, loss = 0.47699690\n",
      "Iteration 257, loss = 0.47633839\n",
      "Iteration 258, loss = 0.47606046\n",
      "Iteration 259, loss = 0.47533116\n",
      "Iteration 260, loss = 0.47473139\n",
      "Iteration 261, loss = 0.47426757\n",
      "Iteration 262, loss = 0.47366528\n",
      "Iteration 263, loss = 0.47332773\n",
      "Iteration 264, loss = 0.47275298\n",
      "Iteration 265, loss = 0.47214863\n",
      "Iteration 266, loss = 0.47154161\n",
      "Iteration 267, loss = 0.47105565\n",
      "Iteration 268, loss = 0.47059937\n",
      "Iteration 269, loss = 0.47026031\n",
      "Iteration 270, loss = 0.46953887\n",
      "Iteration 271, loss = 0.46919299\n",
      "Iteration 272, loss = 0.46854700\n",
      "Iteration 273, loss = 0.46802685\n",
      "Iteration 274, loss = 0.46747790\n",
      "Iteration 275, loss = 0.46680431\n",
      "Iteration 276, loss = 0.46625654\n",
      "Iteration 277, loss = 0.46573876\n",
      "Iteration 278, loss = 0.46514440\n",
      "Iteration 279, loss = 0.46467867\n",
      "Iteration 280, loss = 0.46410856\n",
      "Iteration 281, loss = 0.46362810\n",
      "Iteration 282, loss = 0.46318301\n",
      "Iteration 283, loss = 0.46261445\n",
      "Iteration 284, loss = 0.46213762\n",
      "Iteration 285, loss = 0.46152414\n",
      "Iteration 286, loss = 0.46098982\n",
      "Iteration 287, loss = 0.46045900\n",
      "Iteration 288, loss = 0.45999911\n",
      "Iteration 289, loss = 0.45956584\n",
      "Iteration 290, loss = 0.45893796\n",
      "Iteration 291, loss = 0.45859719\n",
      "Iteration 292, loss = 0.45775135\n",
      "Iteration 293, loss = 0.45742345\n",
      "Iteration 294, loss = 0.45669061\n",
      "Iteration 295, loss = 0.45625075\n",
      "Iteration 296, loss = 0.45565520\n",
      "Iteration 297, loss = 0.45536115\n",
      "Iteration 298, loss = 0.45477243\n",
      "Iteration 299, loss = 0.45432945\n",
      "Iteration 300, loss = 0.45361941\n",
      "Iteration 301, loss = 0.45344346\n",
      "Iteration 302, loss = 0.45279364\n",
      "Iteration 303, loss = 0.45222251\n",
      "Iteration 304, loss = 0.45162067\n",
      "Iteration 305, loss = 0.45106688\n",
      "Iteration 306, loss = 0.45074579\n",
      "Iteration 307, loss = 0.45010750\n",
      "Iteration 308, loss = 0.44977378\n",
      "Iteration 309, loss = 0.44922985\n",
      "Iteration 310, loss = 0.44856041\n",
      "Iteration 311, loss = 0.44830783\n",
      "Iteration 312, loss = 0.44764838\n",
      "Iteration 313, loss = 0.44699203\n",
      "Iteration 314, loss = 0.44657613\n",
      "Iteration 315, loss = 0.44640917\n",
      "Iteration 316, loss = 0.44557216\n",
      "Iteration 317, loss = 0.44514665\n",
      "Iteration 318, loss = 0.44468184\n",
      "Iteration 319, loss = 0.44415873\n",
      "Iteration 320, loss = 0.44363206\n",
      "Iteration 321, loss = 0.44312853\n",
      "Iteration 322, loss = 0.44268077\n",
      "Iteration 323, loss = 0.44215597\n",
      "Iteration 324, loss = 0.44165937\n",
      "Iteration 325, loss = 0.44127349\n",
      "Iteration 326, loss = 0.44076561\n",
      "Iteration 327, loss = 0.44031173\n",
      "Iteration 328, loss = 0.43968454\n",
      "Iteration 329, loss = 0.43914907\n",
      "Iteration 330, loss = 0.43863836\n",
      "Iteration 331, loss = 0.43822695\n",
      "Iteration 332, loss = 0.43753113\n",
      "Iteration 333, loss = 0.43726945\n",
      "Iteration 334, loss = 0.43671914\n",
      "Iteration 335, loss = 0.43611003\n",
      "Iteration 336, loss = 0.43559539\n",
      "Iteration 337, loss = 0.43525590\n",
      "Iteration 338, loss = 0.43480577\n",
      "Iteration 339, loss = 0.43418688\n",
      "Iteration 340, loss = 0.43376010\n",
      "Iteration 341, loss = 0.43328370\n",
      "Iteration 342, loss = 0.43275710\n",
      "Iteration 343, loss = 0.43238805\n",
      "Iteration 344, loss = 0.43219190\n",
      "Iteration 345, loss = 0.43137380\n",
      "Iteration 346, loss = 0.43082683\n",
      "Iteration 347, loss = 0.43062530\n",
      "Iteration 348, loss = 0.42985005\n",
      "Iteration 349, loss = 0.42963301\n",
      "Iteration 350, loss = 0.42896679\n",
      "Iteration 351, loss = 0.42858628\n",
      "Iteration 352, loss = 0.42796631\n",
      "Iteration 353, loss = 0.42767466\n",
      "Iteration 354, loss = 0.42720355\n",
      "Iteration 355, loss = 0.42663155\n",
      "Iteration 356, loss = 0.42613405\n",
      "Iteration 357, loss = 0.42533420\n",
      "Iteration 358, loss = 0.42522368\n",
      "Iteration 359, loss = 0.42518572\n",
      "Iteration 360, loss = 0.42409148\n",
      "Iteration 361, loss = 0.42372985\n",
      "Iteration 362, loss = 0.42327124\n",
      "Iteration 363, loss = 0.42273537\n",
      "Iteration 364, loss = 0.42226120\n",
      "Iteration 365, loss = 0.42185011\n",
      "Iteration 366, loss = 0.42145794\n",
      "Iteration 367, loss = 0.42099171\n",
      "Iteration 368, loss = 0.42050832\n",
      "Iteration 369, loss = 0.42008161\n",
      "Iteration 370, loss = 0.41954190\n",
      "Iteration 371, loss = 0.41930425\n",
      "Iteration 372, loss = 0.41870278\n",
      "Iteration 373, loss = 0.41812004\n",
      "Iteration 374, loss = 0.41781054\n",
      "Iteration 375, loss = 0.41727735\n",
      "Iteration 376, loss = 0.41680736\n",
      "Iteration 377, loss = 0.41656054\n",
      "Iteration 378, loss = 0.41606706\n",
      "Iteration 379, loss = 0.41533687\n",
      "Iteration 380, loss = 0.41515416\n",
      "Iteration 381, loss = 0.41466691\n",
      "Iteration 382, loss = 0.41399134\n",
      "Iteration 383, loss = 0.41366288\n",
      "Iteration 384, loss = 0.41315488\n",
      "Iteration 385, loss = 0.41296101\n",
      "Iteration 386, loss = 0.41243578\n",
      "Iteration 387, loss = 0.41192456\n",
      "Iteration 388, loss = 0.41154404\n",
      "Iteration 389, loss = 0.41081502\n",
      "Iteration 390, loss = 0.41096190\n",
      "Iteration 391, loss = 0.41002716\n",
      "Iteration 392, loss = 0.40954312\n",
      "Iteration 393, loss = 0.40935564\n",
      "Iteration 394, loss = 0.40901573\n",
      "Iteration 395, loss = 0.40823002\n",
      "Iteration 396, loss = 0.40778068\n",
      "Iteration 397, loss = 0.40760193\n",
      "Iteration 398, loss = 0.40704288\n",
      "Iteration 399, loss = 0.40653583\n",
      "Iteration 400, loss = 0.40614057\n",
      "Iteration 401, loss = 0.40578108\n",
      "Iteration 402, loss = 0.40538799\n",
      "Iteration 403, loss = 0.40561094\n",
      "Iteration 404, loss = 0.40471491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 405, loss = 0.40425863\n",
      "Iteration 406, loss = 0.40353082\n",
      "Iteration 407, loss = 0.40330238\n",
      "Iteration 408, loss = 0.40274590\n",
      "Iteration 409, loss = 0.40280020\n",
      "Iteration 410, loss = 0.40205020\n",
      "Iteration 411, loss = 0.40163706\n",
      "Iteration 412, loss = 0.40106808\n",
      "Iteration 413, loss = 0.40063715\n",
      "Iteration 414, loss = 0.40032906\n",
      "Iteration 415, loss = 0.39992808\n",
      "Iteration 416, loss = 0.39939114\n",
      "Iteration 417, loss = 0.39874201\n",
      "Iteration 418, loss = 0.39837484\n",
      "Iteration 419, loss = 0.39819087\n",
      "Iteration 420, loss = 0.39767847\n",
      "Iteration 421, loss = 0.39731411\n",
      "Iteration 422, loss = 0.39677918\n",
      "Iteration 423, loss = 0.39630130\n",
      "Iteration 424, loss = 0.39636362\n",
      "Iteration 425, loss = 0.39539025\n",
      "Iteration 426, loss = 0.39504328\n",
      "Iteration 427, loss = 0.39464015\n",
      "Iteration 428, loss = 0.39419584\n",
      "Iteration 429, loss = 0.39358973\n",
      "Iteration 430, loss = 0.39366048\n",
      "Iteration 431, loss = 0.39306602\n",
      "Iteration 432, loss = 0.39266606\n",
      "Iteration 433, loss = 0.39227285\n",
      "Iteration 434, loss = 0.39182239\n",
      "Iteration 435, loss = 0.39159501\n",
      "Iteration 436, loss = 0.39109762\n",
      "Iteration 437, loss = 0.39041148\n",
      "Iteration 438, loss = 0.39017327\n",
      "Iteration 439, loss = 0.38981049\n",
      "Iteration 440, loss = 0.38934753\n",
      "Iteration 441, loss = 0.38905605\n",
      "Iteration 442, loss = 0.38846975\n",
      "Iteration 443, loss = 0.38840866\n",
      "Iteration 444, loss = 0.38788697\n",
      "Iteration 445, loss = 0.38732935\n",
      "Iteration 446, loss = 0.38681082\n",
      "Iteration 447, loss = 0.38628889\n",
      "Iteration 448, loss = 0.38621745\n",
      "Iteration 449, loss = 0.38598686\n",
      "Iteration 450, loss = 0.38535651\n",
      "Iteration 451, loss = 0.38491773\n",
      "Iteration 452, loss = 0.38475539\n",
      "Iteration 453, loss = 0.38415959\n",
      "Iteration 454, loss = 0.38403953\n",
      "Iteration 455, loss = 0.38371915\n",
      "Iteration 456, loss = 0.38300990\n",
      "Iteration 457, loss = 0.38285304\n",
      "Iteration 458, loss = 0.38237442\n",
      "Iteration 459, loss = 0.38165086\n",
      "Iteration 460, loss = 0.38155805\n",
      "Iteration 461, loss = 0.38138212\n",
      "Iteration 462, loss = 0.38066564\n",
      "Iteration 463, loss = 0.38080658\n",
      "Iteration 464, loss = 0.38005773\n",
      "Iteration 465, loss = 0.37971376\n",
      "Iteration 466, loss = 0.37938521\n",
      "Iteration 467, loss = 0.37931486\n",
      "Iteration 468, loss = 0.37848157\n",
      "Iteration 469, loss = 0.37802225\n",
      "Iteration 470, loss = 0.37763848\n",
      "Iteration 471, loss = 0.37742154\n",
      "Iteration 472, loss = 0.37690786\n",
      "Iteration 473, loss = 0.37654497\n",
      "Iteration 474, loss = 0.37649296\n",
      "Iteration 475, loss = 0.37594032\n",
      "Iteration 476, loss = 0.37541559\n",
      "Iteration 477, loss = 0.37506412\n",
      "Iteration 478, loss = 0.37464964\n",
      "Iteration 479, loss = 0.37409344\n",
      "Iteration 480, loss = 0.37381548\n",
      "Iteration 481, loss = 0.37351271\n",
      "Iteration 482, loss = 0.37341383\n",
      "Iteration 483, loss = 0.37262932\n",
      "Iteration 484, loss = 0.37236706\n",
      "Iteration 485, loss = 0.37204112\n",
      "Iteration 486, loss = 0.37152488\n",
      "Iteration 487, loss = 0.37133284\n",
      "Iteration 488, loss = 0.37094240\n",
      "Iteration 489, loss = 0.37059636\n",
      "Iteration 490, loss = 0.37047120\n",
      "Iteration 491, loss = 0.36969769\n",
      "Iteration 492, loss = 0.36938026\n",
      "Iteration 493, loss = 0.36897523\n",
      "Iteration 494, loss = 0.36809751\n",
      "Iteration 495, loss = 0.36806406\n",
      "Iteration 496, loss = 0.36791196\n",
      "Iteration 497, loss = 0.36747140\n",
      "Iteration 498, loss = 0.36694303\n",
      "Iteration 499, loss = 0.36680879\n",
      "Iteration 500, loss = 0.36643962\n",
      "Iteration 501, loss = 0.36603352\n",
      "Iteration 502, loss = 0.36573503\n",
      "Iteration 503, loss = 0.36497001\n",
      "Iteration 504, loss = 0.36469153\n",
      "Iteration 505, loss = 0.36447901\n",
      "Iteration 506, loss = 0.36433931\n",
      "Iteration 507, loss = 0.36389117\n",
      "Iteration 508, loss = 0.36336527\n",
      "Iteration 509, loss = 0.36311487\n",
      "Iteration 510, loss = 0.36262705\n",
      "Iteration 511, loss = 0.36229988\n",
      "Iteration 512, loss = 0.36166203\n",
      "Iteration 513, loss = 0.36142409\n",
      "Iteration 514, loss = 0.36089489\n",
      "Iteration 515, loss = 0.36091670\n",
      "Iteration 516, loss = 0.36048274\n",
      "Iteration 517, loss = 0.36017927\n",
      "Iteration 518, loss = 0.35987428\n",
      "Iteration 519, loss = 0.35925490\n",
      "Iteration 520, loss = 0.35902325\n",
      "Iteration 521, loss = 0.35866956\n",
      "Iteration 522, loss = 0.35816007\n",
      "Iteration 523, loss = 0.35789497\n",
      "Iteration 524, loss = 0.35771338\n",
      "Iteration 525, loss = 0.35724873\n",
      "Iteration 526, loss = 0.35675528\n",
      "Iteration 527, loss = 0.35647745\n",
      "Iteration 528, loss = 0.35592902\n",
      "Iteration 529, loss = 0.35568100\n",
      "Iteration 530, loss = 0.35563657\n",
      "Iteration 531, loss = 0.35518125\n",
      "Iteration 532, loss = 0.35469967\n",
      "Iteration 533, loss = 0.35383276\n",
      "Iteration 534, loss = 0.35346050\n",
      "Iteration 535, loss = 0.35331970\n",
      "Iteration 536, loss = 0.35312598\n",
      "Iteration 537, loss = 0.35287399\n",
      "Iteration 538, loss = 0.35246634\n",
      "Iteration 539, loss = 0.35183125\n",
      "Iteration 540, loss = 0.35132559\n",
      "Iteration 541, loss = 0.35114336\n",
      "Iteration 542, loss = 0.35073522\n",
      "Iteration 543, loss = 0.35023456\n",
      "Iteration 544, loss = 0.34983496\n",
      "Iteration 545, loss = 0.34959906\n",
      "Iteration 546, loss = 0.34913578\n",
      "Iteration 547, loss = 0.34947360\n",
      "Iteration 548, loss = 0.34850487\n",
      "Iteration 549, loss = 0.34819062\n",
      "Iteration 550, loss = 0.34778061\n",
      "Iteration 551, loss = 0.34726165\n",
      "Iteration 552, loss = 0.34718347\n",
      "Iteration 553, loss = 0.34712135\n",
      "Iteration 554, loss = 0.34615548\n",
      "Iteration 555, loss = 0.34610131\n",
      "Iteration 556, loss = 0.34556939\n",
      "Iteration 557, loss = 0.34553739\n",
      "Iteration 558, loss = 0.34507171\n",
      "Iteration 559, loss = 0.34471311\n",
      "Iteration 560, loss = 0.34405710\n",
      "Iteration 561, loss = 0.34388098\n",
      "Iteration 562, loss = 0.34364721\n",
      "Iteration 563, loss = 0.34352403\n",
      "Iteration 564, loss = 0.34290086\n",
      "Iteration 565, loss = 0.34278831\n",
      "Iteration 566, loss = 0.34273361\n",
      "Iteration 567, loss = 0.34170912\n",
      "Iteration 568, loss = 0.34164454\n",
      "Iteration 569, loss = 0.34116676\n",
      "Iteration 570, loss = 0.34072361\n",
      "Iteration 571, loss = 0.34083746\n",
      "Iteration 572, loss = 0.34021549\n",
      "Iteration 573, loss = 0.33986235\n",
      "Iteration 574, loss = 0.33965107\n",
      "Iteration 575, loss = 0.33903719\n",
      "Iteration 576, loss = 0.33932764\n",
      "Iteration 577, loss = 0.33849070\n",
      "Iteration 578, loss = 0.33849773\n",
      "Iteration 579, loss = 0.33776872\n",
      "Iteration 580, loss = 0.33735996\n",
      "Iteration 581, loss = 0.33700632\n",
      "Iteration 582, loss = 0.33716813\n",
      "Iteration 583, loss = 0.33652345\n",
      "Iteration 584, loss = 0.33598883\n",
      "Iteration 585, loss = 0.33593181\n",
      "Iteration 586, loss = 0.33642122\n",
      "Iteration 587, loss = 0.33576997\n",
      "Iteration 588, loss = 0.33460485\n",
      "Iteration 589, loss = 0.33442118\n",
      "Iteration 590, loss = 0.33412467\n",
      "Iteration 591, loss = 0.33391078\n",
      "Iteration 592, loss = 0.33395341\n",
      "Iteration 593, loss = 0.33320120\n",
      "Iteration 594, loss = 0.33315440\n",
      "Iteration 595, loss = 0.33248014\n",
      "Iteration 596, loss = 0.33252849\n",
      "Iteration 597, loss = 0.33200349\n",
      "Iteration 598, loss = 0.33199085\n",
      "Iteration 599, loss = 0.33145556\n",
      "Iteration 600, loss = 0.33090756\n",
      "Iteration 601, loss = 0.33095386\n",
      "Iteration 602, loss = 0.33043453\n",
      "Iteration 603, loss = 0.33033002\n",
      "Iteration 604, loss = 0.32958285\n",
      "Iteration 605, loss = 0.32976540\n",
      "Iteration 606, loss = 0.32985673\n",
      "Iteration 607, loss = 0.32920284\n",
      "Iteration 608, loss = 0.32856189\n",
      "Iteration 609, loss = 0.32853894\n",
      "Iteration 610, loss = 0.32784842\n",
      "Iteration 611, loss = 0.32768151\n",
      "Iteration 612, loss = 0.32754917\n",
      "Iteration 613, loss = 0.32723561\n",
      "Iteration 614, loss = 0.32748849\n",
      "Iteration 615, loss = 0.32700811\n",
      "Iteration 616, loss = 0.32603284\n",
      "Iteration 617, loss = 0.32640278\n",
      "Iteration 618, loss = 0.32560615\n",
      "Iteration 619, loss = 0.32511121\n",
      "Iteration 620, loss = 0.32496616\n",
      "Iteration 621, loss = 0.32530277\n",
      "Iteration 622, loss = 0.32447678\n",
      "Iteration 623, loss = 0.32425323\n",
      "Iteration 624, loss = 0.32397300\n",
      "Iteration 625, loss = 0.32357023\n",
      "Iteration 626, loss = 0.32338551\n",
      "Iteration 627, loss = 0.32393722\n",
      "Iteration 628, loss = 0.32335751\n",
      "Iteration 629, loss = 0.32263123\n",
      "Iteration 630, loss = 0.32201718\n",
      "Iteration 631, loss = 0.32198556\n",
      "Iteration 632, loss = 0.32239808\n",
      "Iteration 633, loss = 0.32155104\n",
      "Iteration 634, loss = 0.32134314\n",
      "Iteration 635, loss = 0.32100493\n",
      "Iteration 636, loss = 0.32092934\n",
      "Iteration 637, loss = 0.32045234\n",
      "Iteration 638, loss = 0.31983545\n",
      "Iteration 639, loss = 0.31991267\n",
      "Iteration 640, loss = 0.31974984\n",
      "Iteration 641, loss = 0.31939451\n",
      "Iteration 642, loss = 0.31898435\n",
      "Iteration 643, loss = 0.31888041\n",
      "Iteration 644, loss = 0.31895679\n",
      "Iteration 645, loss = 0.31810822\n",
      "Iteration 646, loss = 0.31785993\n",
      "Iteration 647, loss = 0.31743244\n",
      "Iteration 648, loss = 0.31757899\n",
      "Iteration 649, loss = 0.31712845\n",
      "Iteration 650, loss = 0.31702741\n",
      "Iteration 651, loss = 0.31675506\n",
      "Iteration 652, loss = 0.31641583\n",
      "Iteration 653, loss = 0.31594714\n",
      "Iteration 654, loss = 0.31623059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 655, loss = 0.31600640\n",
      "Iteration 656, loss = 0.31514366\n",
      "Iteration 657, loss = 0.31486794\n",
      "Iteration 658, loss = 0.31511605\n",
      "Iteration 659, loss = 0.31523830\n",
      "Iteration 660, loss = 0.31436054\n",
      "Iteration 661, loss = 0.31408651\n",
      "Iteration 662, loss = 0.31357219\n",
      "Iteration 663, loss = 0.31351964\n",
      "Iteration 664, loss = 0.31320072\n",
      "Iteration 665, loss = 0.31309697\n",
      "Iteration 666, loss = 0.31262248\n",
      "Iteration 667, loss = 0.31247838\n",
      "Iteration 668, loss = 0.31204971\n",
      "Iteration 669, loss = 0.31184765\n",
      "Iteration 670, loss = 0.31149043\n",
      "Iteration 671, loss = 0.31152675\n",
      "Iteration 672, loss = 0.31108427\n",
      "Iteration 673, loss = 0.31086691\n",
      "Iteration 674, loss = 0.31071271\n",
      "Iteration 675, loss = 0.31059456\n",
      "Iteration 676, loss = 0.30997917\n",
      "Iteration 677, loss = 0.31035014\n",
      "Iteration 678, loss = 0.30971779\n",
      "Iteration 679, loss = 0.30924649\n",
      "Iteration 680, loss = 0.30931348\n",
      "Iteration 681, loss = 0.30908264\n",
      "Iteration 682, loss = 0.30847957\n",
      "Iteration 683, loss = 0.30827020\n",
      "Iteration 684, loss = 0.30801641\n",
      "Iteration 685, loss = 0.30790564\n",
      "Iteration 686, loss = 0.30774187\n",
      "Iteration 687, loss = 0.30728744\n",
      "Iteration 688, loss = 0.30764418\n",
      "Iteration 689, loss = 0.30698932\n",
      "Iteration 690, loss = 0.30715437\n",
      "Iteration 691, loss = 0.30715901\n",
      "Iteration 692, loss = 0.30608457\n",
      "Iteration 693, loss = 0.30588195\n",
      "Iteration 694, loss = 0.30573392\n",
      "Iteration 695, loss = 0.30580659\n",
      "Iteration 696, loss = 0.30508798\n",
      "Iteration 697, loss = 0.30570542\n",
      "Iteration 698, loss = 0.30456281\n",
      "Iteration 699, loss = 0.30460714\n",
      "Iteration 700, loss = 0.30418934\n",
      "Iteration 701, loss = 0.30460933\n",
      "Iteration 702, loss = 0.30432102\n",
      "Iteration 703, loss = 0.30389365\n",
      "Iteration 704, loss = 0.30358790\n",
      "Iteration 705, loss = 0.30291587\n",
      "Iteration 706, loss = 0.30295740\n",
      "Iteration 707, loss = 0.30332406\n",
      "Iteration 708, loss = 0.30209886\n",
      "Iteration 709, loss = 0.30212068\n",
      "Iteration 710, loss = 0.30220425\n",
      "Iteration 711, loss = 0.30176202\n",
      "Iteration 712, loss = 0.30155651\n",
      "Iteration 713, loss = 0.30130418\n",
      "Iteration 714, loss = 0.30129989\n",
      "Iteration 715, loss = 0.30075779\n",
      "Iteration 716, loss = 0.30082124\n",
      "Iteration 717, loss = 0.30030222\n",
      "Iteration 718, loss = 0.30021013\n",
      "Iteration 719, loss = 0.30093361\n",
      "Iteration 720, loss = 0.30029718\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69564246\n",
      "Iteration 2, loss = 0.68813339\n",
      "Iteration 3, loss = 0.68125491\n",
      "Iteration 4, loss = 0.67510715\n",
      "Iteration 5, loss = 0.66964311\n",
      "Iteration 6, loss = 0.66469252\n",
      "Iteration 7, loss = 0.66012102\n",
      "Iteration 8, loss = 0.65590281\n",
      "Iteration 9, loss = 0.65204132\n",
      "Iteration 10, loss = 0.64846655\n",
      "Iteration 11, loss = 0.64518936\n",
      "Iteration 12, loss = 0.64215455\n",
      "Iteration 13, loss = 0.63935854\n",
      "Iteration 14, loss = 0.63682789\n",
      "Iteration 15, loss = 0.63443177\n",
      "Iteration 16, loss = 0.63226756\n",
      "Iteration 17, loss = 0.63026422\n",
      "Iteration 18, loss = 0.62841530\n",
      "Iteration 19, loss = 0.62666362\n",
      "Iteration 20, loss = 0.62502643\n",
      "Iteration 21, loss = 0.62351478\n",
      "Iteration 22, loss = 0.62211059\n",
      "Iteration 23, loss = 0.62075542\n",
      "Iteration 24, loss = 0.61947568\n",
      "Iteration 25, loss = 0.61837567\n",
      "Iteration 26, loss = 0.61714916\n",
      "Iteration 27, loss = 0.61600568\n",
      "Iteration 28, loss = 0.61498272\n",
      "Iteration 29, loss = 0.61392347\n",
      "Iteration 30, loss = 0.61289255\n",
      "Iteration 31, loss = 0.61186826\n",
      "Iteration 32, loss = 0.61091718\n",
      "Iteration 33, loss = 0.60993223\n",
      "Iteration 34, loss = 0.60899082\n",
      "Iteration 35, loss = 0.60806761\n",
      "Iteration 36, loss = 0.60717148\n",
      "Iteration 37, loss = 0.60623098\n",
      "Iteration 38, loss = 0.60531887\n",
      "Iteration 39, loss = 0.60440893\n",
      "Iteration 40, loss = 0.60350867\n",
      "Iteration 41, loss = 0.60262063\n",
      "Iteration 42, loss = 0.60177350\n",
      "Iteration 43, loss = 0.60088047\n",
      "Iteration 44, loss = 0.60007153\n",
      "Iteration 45, loss = 0.59919672\n",
      "Iteration 46, loss = 0.59832103\n",
      "Iteration 47, loss = 0.59742813\n",
      "Iteration 48, loss = 0.59659793\n",
      "Iteration 49, loss = 0.59575261\n",
      "Iteration 50, loss = 0.59490301\n",
      "Iteration 51, loss = 0.59404750\n",
      "Iteration 52, loss = 0.59325537\n",
      "Iteration 53, loss = 0.59243437\n",
      "Iteration 54, loss = 0.59159335\n",
      "Iteration 55, loss = 0.59074580\n",
      "Iteration 56, loss = 0.58998099\n",
      "Iteration 57, loss = 0.58918008\n",
      "Iteration 58, loss = 0.58837905\n",
      "Iteration 59, loss = 0.58761646\n",
      "Iteration 60, loss = 0.58680912\n",
      "Iteration 61, loss = 0.58607849\n",
      "Iteration 62, loss = 0.58525836\n",
      "Iteration 63, loss = 0.58448412\n",
      "Iteration 64, loss = 0.58368839\n",
      "Iteration 65, loss = 0.58295408\n",
      "Iteration 66, loss = 0.58219756\n",
      "Iteration 67, loss = 0.58144824\n",
      "Iteration 68, loss = 0.58070811\n",
      "Iteration 69, loss = 0.57997510\n",
      "Iteration 70, loss = 0.57924972\n",
      "Iteration 71, loss = 0.57851316\n",
      "Iteration 72, loss = 0.57778036\n",
      "Iteration 73, loss = 0.57706516\n",
      "Iteration 74, loss = 0.57631352\n",
      "Iteration 75, loss = 0.57566565\n",
      "Iteration 76, loss = 0.57493150\n",
      "Iteration 77, loss = 0.57422822\n",
      "Iteration 78, loss = 0.57356113\n",
      "Iteration 79, loss = 0.57285583\n",
      "Iteration 80, loss = 0.57220249\n",
      "Iteration 81, loss = 0.57147775\n",
      "Iteration 82, loss = 0.57081397\n",
      "Iteration 83, loss = 0.57014293\n",
      "Iteration 84, loss = 0.56945370\n",
      "Iteration 85, loss = 0.56882775\n",
      "Iteration 86, loss = 0.56821543\n",
      "Iteration 87, loss = 0.56767223\n",
      "Iteration 88, loss = 0.56690369\n",
      "Iteration 89, loss = 0.56627416\n",
      "Iteration 90, loss = 0.56561109\n",
      "Iteration 91, loss = 0.56502216\n",
      "Iteration 92, loss = 0.56431641\n",
      "Iteration 93, loss = 0.56374546\n",
      "Iteration 94, loss = 0.56309857\n",
      "Iteration 95, loss = 0.56253079\n",
      "Iteration 96, loss = 0.56192411\n",
      "Iteration 97, loss = 0.56134074\n",
      "Iteration 98, loss = 0.56069772\n",
      "Iteration 99, loss = 0.56013825\n",
      "Iteration 100, loss = 0.55952925\n",
      "Iteration 101, loss = 0.55891896\n",
      "Iteration 102, loss = 0.55841486\n",
      "Iteration 103, loss = 0.55777630\n",
      "Iteration 104, loss = 0.55719329\n",
      "Iteration 105, loss = 0.55670094\n",
      "Iteration 106, loss = 0.55604495\n",
      "Iteration 107, loss = 0.55543444\n",
      "Iteration 108, loss = 0.55488027\n",
      "Iteration 109, loss = 0.55431019\n",
      "Iteration 110, loss = 0.55370955\n",
      "Iteration 111, loss = 0.55317103\n",
      "Iteration 112, loss = 0.55258598\n",
      "Iteration 113, loss = 0.55202038\n",
      "Iteration 114, loss = 0.55145563\n",
      "Iteration 115, loss = 0.55089761\n",
      "Iteration 116, loss = 0.55038300\n",
      "Iteration 117, loss = 0.54984439\n",
      "Iteration 118, loss = 0.54924689\n",
      "Iteration 119, loss = 0.54869187\n",
      "Iteration 120, loss = 0.54814722\n",
      "Iteration 121, loss = 0.54763254\n",
      "Iteration 122, loss = 0.54709369\n",
      "Iteration 123, loss = 0.54659047\n",
      "Iteration 124, loss = 0.54611467\n",
      "Iteration 125, loss = 0.54550852\n",
      "Iteration 126, loss = 0.54514902\n",
      "Iteration 127, loss = 0.54459148\n",
      "Iteration 128, loss = 0.54396267\n",
      "Iteration 129, loss = 0.54346636\n",
      "Iteration 130, loss = 0.54291850\n",
      "Iteration 131, loss = 0.54240975\n",
      "Iteration 132, loss = 0.54195575\n",
      "Iteration 133, loss = 0.54143837\n",
      "Iteration 134, loss = 0.54089315\n",
      "Iteration 135, loss = 0.54041163\n",
      "Iteration 136, loss = 0.53989740\n",
      "Iteration 137, loss = 0.53937473\n",
      "Iteration 138, loss = 0.53890744\n",
      "Iteration 139, loss = 0.53844629\n",
      "Iteration 140, loss = 0.53799266\n",
      "Iteration 141, loss = 0.53740531\n",
      "Iteration 142, loss = 0.53695723\n",
      "Iteration 143, loss = 0.53642017\n",
      "Iteration 144, loss = 0.53594827\n",
      "Iteration 145, loss = 0.53538705\n",
      "Iteration 146, loss = 0.53488327\n",
      "Iteration 147, loss = 0.53449418\n",
      "Iteration 148, loss = 0.53392598\n",
      "Iteration 149, loss = 0.53341443\n",
      "Iteration 150, loss = 0.53286693\n",
      "Iteration 151, loss = 0.53254412\n",
      "Iteration 152, loss = 0.53188965\n",
      "Iteration 153, loss = 0.53144870\n",
      "Iteration 154, loss = 0.53092655\n",
      "Iteration 155, loss = 0.53036038\n",
      "Iteration 156, loss = 0.52987236\n",
      "Iteration 157, loss = 0.52938665\n",
      "Iteration 158, loss = 0.52894457\n",
      "Iteration 159, loss = 0.52836492\n",
      "Iteration 160, loss = 0.52795570\n",
      "Iteration 161, loss = 0.52735187\n",
      "Iteration 162, loss = 0.52694304\n",
      "Iteration 163, loss = 0.52638354\n",
      "Iteration 164, loss = 0.52594026\n",
      "Iteration 165, loss = 0.52547521\n",
      "Iteration 166, loss = 0.52485478\n",
      "Iteration 167, loss = 0.52427809\n",
      "Iteration 168, loss = 0.52388619\n",
      "Iteration 169, loss = 0.52330896\n",
      "Iteration 170, loss = 0.52269199\n",
      "Iteration 171, loss = 0.52225765\n",
      "Iteration 172, loss = 0.52170850\n",
      "Iteration 173, loss = 0.52121588\n",
      "Iteration 174, loss = 0.52064450\n",
      "Iteration 175, loss = 0.52012844\n",
      "Iteration 176, loss = 0.51973583\n",
      "Iteration 177, loss = 0.51906889\n",
      "Iteration 178, loss = 0.51857221\n",
      "Iteration 179, loss = 0.51811884\n",
      "Iteration 180, loss = 0.51752540\n",
      "Iteration 181, loss = 0.51720142\n",
      "Iteration 182, loss = 0.51648403\n",
      "Iteration 183, loss = 0.51602486\n",
      "Iteration 184, loss = 0.51550518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 185, loss = 0.51493159\n",
      "Iteration 186, loss = 0.51440217\n",
      "Iteration 187, loss = 0.51392282\n",
      "Iteration 188, loss = 0.51339093\n",
      "Iteration 189, loss = 0.51289515\n",
      "Iteration 190, loss = 0.51232961\n",
      "Iteration 191, loss = 0.51187014\n",
      "Iteration 192, loss = 0.51138337\n",
      "Iteration 193, loss = 0.51079040\n",
      "Iteration 194, loss = 0.51026379\n",
      "Iteration 195, loss = 0.50981667\n",
      "Iteration 196, loss = 0.50913714\n",
      "Iteration 197, loss = 0.50876499\n",
      "Iteration 198, loss = 0.50827867\n",
      "Iteration 199, loss = 0.50767961\n",
      "Iteration 200, loss = 0.50715995\n",
      "Iteration 201, loss = 0.50663830\n",
      "Iteration 202, loss = 0.50611347\n",
      "Iteration 203, loss = 0.50557256\n",
      "Iteration 204, loss = 0.50498659\n",
      "Iteration 205, loss = 0.50453505\n",
      "Iteration 206, loss = 0.50387566\n",
      "Iteration 207, loss = 0.50335969\n",
      "Iteration 208, loss = 0.50290148\n",
      "Iteration 209, loss = 0.50234684\n",
      "Iteration 210, loss = 0.50180731\n",
      "Iteration 211, loss = 0.50129130\n",
      "Iteration 212, loss = 0.50077465\n",
      "Iteration 213, loss = 0.50029266\n",
      "Iteration 214, loss = 0.49970202\n",
      "Iteration 215, loss = 0.49909740\n",
      "Iteration 216, loss = 0.49858064\n",
      "Iteration 217, loss = 0.49805498\n",
      "Iteration 218, loss = 0.49750218\n",
      "Iteration 219, loss = 0.49705921\n",
      "Iteration 220, loss = 0.49660134\n",
      "Iteration 221, loss = 0.49589726\n",
      "Iteration 222, loss = 0.49547065\n",
      "Iteration 223, loss = 0.49497973\n",
      "Iteration 224, loss = 0.49433964\n",
      "Iteration 225, loss = 0.49389772\n",
      "Iteration 226, loss = 0.49345227\n",
      "Iteration 227, loss = 0.49277680\n",
      "Iteration 228, loss = 0.49234360\n",
      "Iteration 229, loss = 0.49177575\n",
      "Iteration 230, loss = 0.49125290\n",
      "Iteration 231, loss = 0.49065287\n",
      "Iteration 232, loss = 0.49022512\n",
      "Iteration 233, loss = 0.48960800\n",
      "Iteration 234, loss = 0.48915210\n",
      "Iteration 235, loss = 0.48863201\n",
      "Iteration 236, loss = 0.48795257\n",
      "Iteration 237, loss = 0.48760464\n",
      "Iteration 238, loss = 0.48702192\n",
      "Iteration 239, loss = 0.48645606\n",
      "Iteration 240, loss = 0.48595979\n",
      "Iteration 241, loss = 0.48536414\n",
      "Iteration 242, loss = 0.48498089\n",
      "Iteration 243, loss = 0.48441838\n",
      "Iteration 244, loss = 0.48389954\n",
      "Iteration 245, loss = 0.48330535\n",
      "Iteration 246, loss = 0.48289910\n",
      "Iteration 247, loss = 0.48227869\n",
      "Iteration 248, loss = 0.48163487\n",
      "Iteration 249, loss = 0.48108700\n",
      "Iteration 250, loss = 0.48066326\n",
      "Iteration 251, loss = 0.48011610\n",
      "Iteration 252, loss = 0.47946735\n",
      "Iteration 253, loss = 0.47908137\n",
      "Iteration 254, loss = 0.47848484\n",
      "Iteration 255, loss = 0.47806053\n",
      "Iteration 256, loss = 0.47746108\n",
      "Iteration 257, loss = 0.47692352\n",
      "Iteration 258, loss = 0.47640818\n",
      "Iteration 259, loss = 0.47570019\n",
      "Iteration 260, loss = 0.47518952\n",
      "Iteration 261, loss = 0.47470820\n",
      "Iteration 262, loss = 0.47414926\n",
      "Iteration 263, loss = 0.47386781\n",
      "Iteration 264, loss = 0.47325475\n",
      "Iteration 265, loss = 0.47257959\n",
      "Iteration 266, loss = 0.47205926\n",
      "Iteration 267, loss = 0.47152105\n",
      "Iteration 268, loss = 0.47100739\n",
      "Iteration 269, loss = 0.47070607\n",
      "Iteration 270, loss = 0.47006687\n",
      "Iteration 271, loss = 0.46953457\n",
      "Iteration 272, loss = 0.46890404\n",
      "Iteration 273, loss = 0.46850427\n",
      "Iteration 274, loss = 0.46791064\n",
      "Iteration 275, loss = 0.46734039\n",
      "Iteration 276, loss = 0.46672447\n",
      "Iteration 277, loss = 0.46621335\n",
      "Iteration 278, loss = 0.46567550\n",
      "Iteration 279, loss = 0.46526074\n",
      "Iteration 280, loss = 0.46468644\n",
      "Iteration 281, loss = 0.46424373\n",
      "Iteration 282, loss = 0.46351447\n",
      "Iteration 283, loss = 0.46301213\n",
      "Iteration 284, loss = 0.46257654\n",
      "Iteration 285, loss = 0.46204385\n",
      "Iteration 286, loss = 0.46145279\n",
      "Iteration 287, loss = 0.46098452\n",
      "Iteration 288, loss = 0.46057453\n",
      "Iteration 289, loss = 0.46007588\n",
      "Iteration 290, loss = 0.45937988\n",
      "Iteration 291, loss = 0.45912365\n",
      "Iteration 292, loss = 0.45833884\n",
      "Iteration 293, loss = 0.45798205\n",
      "Iteration 294, loss = 0.45722624\n",
      "Iteration 295, loss = 0.45679957\n",
      "Iteration 296, loss = 0.45623913\n",
      "Iteration 297, loss = 0.45604717\n",
      "Iteration 298, loss = 0.45528175\n",
      "Iteration 299, loss = 0.45479938\n",
      "Iteration 300, loss = 0.45409460\n",
      "Iteration 301, loss = 0.45389390\n",
      "Iteration 302, loss = 0.45329887\n",
      "Iteration 303, loss = 0.45261484\n",
      "Iteration 304, loss = 0.45231729\n",
      "Iteration 305, loss = 0.45166692\n",
      "Iteration 306, loss = 0.45116474\n",
      "Iteration 307, loss = 0.45070237\n",
      "Iteration 308, loss = 0.45050069\n",
      "Iteration 309, loss = 0.44975550\n",
      "Iteration 310, loss = 0.44912859\n",
      "Iteration 311, loss = 0.44862134\n",
      "Iteration 312, loss = 0.44811846\n",
      "Iteration 313, loss = 0.44764520\n",
      "Iteration 314, loss = 0.44700297\n",
      "Iteration 315, loss = 0.44669540\n",
      "Iteration 316, loss = 0.44602353\n",
      "Iteration 317, loss = 0.44563005\n",
      "Iteration 318, loss = 0.44514041\n",
      "Iteration 319, loss = 0.44470718\n",
      "Iteration 320, loss = 0.44417817\n",
      "Iteration 321, loss = 0.44355067\n",
      "Iteration 322, loss = 0.44335406\n",
      "Iteration 323, loss = 0.44259101\n",
      "Iteration 324, loss = 0.44203823\n",
      "Iteration 325, loss = 0.44160881\n",
      "Iteration 326, loss = 0.44126770\n",
      "Iteration 327, loss = 0.44067329\n",
      "Iteration 328, loss = 0.44027461\n",
      "Iteration 329, loss = 0.43982464\n",
      "Iteration 330, loss = 0.43919060\n",
      "Iteration 331, loss = 0.43886236\n",
      "Iteration 332, loss = 0.43815740\n",
      "Iteration 333, loss = 0.43785357\n",
      "Iteration 334, loss = 0.43723505\n",
      "Iteration 335, loss = 0.43680890\n",
      "Iteration 336, loss = 0.43628620\n",
      "Iteration 337, loss = 0.43590448\n",
      "Iteration 338, loss = 0.43552002\n",
      "Iteration 339, loss = 0.43493235\n",
      "Iteration 340, loss = 0.43451951\n",
      "Iteration 341, loss = 0.43384737\n",
      "Iteration 342, loss = 0.43361610\n",
      "Iteration 343, loss = 0.43314452\n",
      "Iteration 344, loss = 0.43271833\n",
      "Iteration 345, loss = 0.43223906\n",
      "Iteration 346, loss = 0.43158128\n",
      "Iteration 347, loss = 0.43145510\n",
      "Iteration 348, loss = 0.43063279\n",
      "Iteration 349, loss = 0.43023930\n",
      "Iteration 350, loss = 0.42980733\n",
      "Iteration 351, loss = 0.42934805\n",
      "Iteration 352, loss = 0.42902985\n",
      "Iteration 353, loss = 0.42865369\n",
      "Iteration 354, loss = 0.42804590\n",
      "Iteration 355, loss = 0.42733573\n",
      "Iteration 356, loss = 0.42710381\n",
      "Iteration 357, loss = 0.42629306\n",
      "Iteration 358, loss = 0.42611967\n",
      "Iteration 359, loss = 0.42594001\n",
      "Iteration 360, loss = 0.42508039\n",
      "Iteration 361, loss = 0.42459554\n",
      "Iteration 362, loss = 0.42430968\n",
      "Iteration 363, loss = 0.42368213\n",
      "Iteration 364, loss = 0.42334413\n",
      "Iteration 365, loss = 0.42295534\n",
      "Iteration 366, loss = 0.42260548\n",
      "Iteration 367, loss = 0.42200596\n",
      "Iteration 368, loss = 0.42163782\n",
      "Iteration 369, loss = 0.42120856\n",
      "Iteration 370, loss = 0.42043830\n",
      "Iteration 371, loss = 0.42079381\n",
      "Iteration 372, loss = 0.41990676\n",
      "Iteration 373, loss = 0.41936159\n",
      "Iteration 374, loss = 0.41896972\n",
      "Iteration 375, loss = 0.41846627\n",
      "Iteration 376, loss = 0.41811583\n",
      "Iteration 377, loss = 0.41768209\n",
      "Iteration 378, loss = 0.41717070\n",
      "Iteration 379, loss = 0.41678093\n",
      "Iteration 380, loss = 0.41650265\n",
      "Iteration 381, loss = 0.41597150\n",
      "Iteration 382, loss = 0.41553786\n",
      "Iteration 383, loss = 0.41505081\n",
      "Iteration 384, loss = 0.41470571\n",
      "Iteration 385, loss = 0.41423548\n",
      "Iteration 386, loss = 0.41378183\n",
      "Iteration 387, loss = 0.41327188\n",
      "Iteration 388, loss = 0.41284958\n",
      "Iteration 389, loss = 0.41250578\n",
      "Iteration 390, loss = 0.41228803\n",
      "Iteration 391, loss = 0.41163389\n",
      "Iteration 392, loss = 0.41125667\n",
      "Iteration 393, loss = 0.41108838\n",
      "Iteration 394, loss = 0.41031490\n",
      "Iteration 395, loss = 0.40987663\n",
      "Iteration 396, loss = 0.40949323\n",
      "Iteration 397, loss = 0.40925263\n",
      "Iteration 398, loss = 0.40871066\n",
      "Iteration 399, loss = 0.40837318\n",
      "Iteration 400, loss = 0.40794907\n",
      "Iteration 401, loss = 0.40744560\n",
      "Iteration 402, loss = 0.40714428\n",
      "Iteration 403, loss = 0.40701205\n",
      "Iteration 404, loss = 0.40636317\n",
      "Iteration 405, loss = 0.40603632\n",
      "Iteration 406, loss = 0.40542662\n",
      "Iteration 407, loss = 0.40534526\n",
      "Iteration 408, loss = 0.40449799\n",
      "Iteration 409, loss = 0.40427381\n",
      "Iteration 410, loss = 0.40419912\n",
      "Iteration 411, loss = 0.40336523\n",
      "Iteration 412, loss = 0.40301169\n",
      "Iteration 413, loss = 0.40260313\n",
      "Iteration 414, loss = 0.40248185\n",
      "Iteration 415, loss = 0.40171135\n",
      "Iteration 416, loss = 0.40151361\n",
      "Iteration 417, loss = 0.40103241\n",
      "Iteration 418, loss = 0.40065892\n",
      "Iteration 419, loss = 0.40022943\n",
      "Iteration 420, loss = 0.39967660\n",
      "Iteration 421, loss = 0.39961047\n",
      "Iteration 422, loss = 0.39897748\n",
      "Iteration 423, loss = 0.39846211\n",
      "Iteration 424, loss = 0.39834323\n",
      "Iteration 425, loss = 0.39777474\n",
      "Iteration 426, loss = 0.39746440\n",
      "Iteration 427, loss = 0.39695896\n",
      "Iteration 428, loss = 0.39666086\n",
      "Iteration 429, loss = 0.39628084\n",
      "Iteration 430, loss = 0.39622351\n",
      "Iteration 431, loss = 0.39552882\n",
      "Iteration 432, loss = 0.39494022\n",
      "Iteration 433, loss = 0.39475124\n",
      "Iteration 434, loss = 0.39426022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 435, loss = 0.39403141\n",
      "Iteration 436, loss = 0.39401469\n",
      "Iteration 437, loss = 0.39304148\n",
      "Iteration 438, loss = 0.39318073\n",
      "Iteration 439, loss = 0.39262646\n",
      "Iteration 440, loss = 0.39211891\n",
      "Iteration 441, loss = 0.39175445\n",
      "Iteration 442, loss = 0.39140003\n",
      "Iteration 443, loss = 0.39116210\n",
      "Iteration 444, loss = 0.39076427\n",
      "Iteration 445, loss = 0.39025831\n",
      "Iteration 446, loss = 0.38975750\n",
      "Iteration 447, loss = 0.38937309\n",
      "Iteration 448, loss = 0.38903423\n",
      "Iteration 449, loss = 0.38889337\n",
      "Iteration 450, loss = 0.38823139\n",
      "Iteration 451, loss = 0.38801143\n",
      "Iteration 452, loss = 0.38781828\n",
      "Iteration 453, loss = 0.38731008\n",
      "Iteration 454, loss = 0.38687115\n",
      "Iteration 455, loss = 0.38675760\n",
      "Iteration 456, loss = 0.38616107\n",
      "Iteration 457, loss = 0.38586144\n",
      "Iteration 458, loss = 0.38531690\n",
      "Iteration 459, loss = 0.38491356\n",
      "Iteration 460, loss = 0.38454301\n",
      "Iteration 461, loss = 0.38423385\n",
      "Iteration 462, loss = 0.38406308\n",
      "Iteration 463, loss = 0.38366678\n",
      "Iteration 464, loss = 0.38327005\n",
      "Iteration 465, loss = 0.38316863\n",
      "Iteration 466, loss = 0.38254832\n",
      "Iteration 467, loss = 0.38246499\n",
      "Iteration 468, loss = 0.38196594\n",
      "Iteration 469, loss = 0.38130371\n",
      "Iteration 470, loss = 0.38098119\n",
      "Iteration 471, loss = 0.38062038\n",
      "Iteration 472, loss = 0.38044395\n",
      "Iteration 473, loss = 0.37979821\n",
      "Iteration 474, loss = 0.37947487\n",
      "Iteration 475, loss = 0.37903959\n",
      "Iteration 476, loss = 0.37898990\n",
      "Iteration 477, loss = 0.37847594\n",
      "Iteration 478, loss = 0.37824899\n",
      "Iteration 479, loss = 0.37748767\n",
      "Iteration 480, loss = 0.37741430\n",
      "Iteration 481, loss = 0.37688855\n",
      "Iteration 482, loss = 0.37682315\n",
      "Iteration 483, loss = 0.37588832\n",
      "Iteration 484, loss = 0.37619506\n",
      "Iteration 485, loss = 0.37582127\n",
      "Iteration 486, loss = 0.37522008\n",
      "Iteration 487, loss = 0.37500294\n",
      "Iteration 488, loss = 0.37506594\n",
      "Iteration 489, loss = 0.37427330\n",
      "Iteration 490, loss = 0.37392981\n",
      "Iteration 491, loss = 0.37359231\n",
      "Iteration 492, loss = 0.37334134\n",
      "Iteration 493, loss = 0.37265070\n",
      "Iteration 494, loss = 0.37242267\n",
      "Iteration 495, loss = 0.37191816\n",
      "Iteration 496, loss = 0.37183404\n",
      "Iteration 497, loss = 0.37131173\n",
      "Iteration 498, loss = 0.37122977\n",
      "Iteration 499, loss = 0.37067165\n",
      "Iteration 500, loss = 0.37016785\n",
      "Iteration 501, loss = 0.36989824\n",
      "Iteration 502, loss = 0.36994715\n",
      "Iteration 503, loss = 0.36929961\n",
      "Iteration 504, loss = 0.36869646\n",
      "Iteration 505, loss = 0.36831175\n",
      "Iteration 506, loss = 0.36843038\n",
      "Iteration 507, loss = 0.36789964\n",
      "Iteration 508, loss = 0.36737603\n",
      "Iteration 509, loss = 0.36707651\n",
      "Iteration 510, loss = 0.36680527\n",
      "Iteration 511, loss = 0.36688984\n",
      "Iteration 512, loss = 0.36622068\n",
      "Iteration 513, loss = 0.36566804\n",
      "Iteration 514, loss = 0.36521188\n",
      "Iteration 515, loss = 0.36501986\n",
      "Iteration 516, loss = 0.36487052\n",
      "Iteration 517, loss = 0.36476501\n",
      "Iteration 518, loss = 0.36394139\n",
      "Iteration 519, loss = 0.36375746\n",
      "Iteration 520, loss = 0.36316531\n",
      "Iteration 521, loss = 0.36284966\n",
      "Iteration 522, loss = 0.36253588\n",
      "Iteration 523, loss = 0.36223913\n",
      "Iteration 524, loss = 0.36207148\n",
      "Iteration 525, loss = 0.36167841\n",
      "Iteration 526, loss = 0.36143842\n",
      "Iteration 527, loss = 0.36112604\n",
      "Iteration 528, loss = 0.36086131\n",
      "Iteration 529, loss = 0.36004350\n",
      "Iteration 530, loss = 0.36002720\n",
      "Iteration 531, loss = 0.35997335\n",
      "Iteration 532, loss = 0.35948261\n",
      "Iteration 533, loss = 0.35867194\n",
      "Iteration 534, loss = 0.35841091\n",
      "Iteration 535, loss = 0.35826615\n",
      "Iteration 536, loss = 0.35800937\n",
      "Iteration 537, loss = 0.35774563\n",
      "Iteration 538, loss = 0.35764763\n",
      "Iteration 539, loss = 0.35657224\n",
      "Iteration 540, loss = 0.35671707\n",
      "Iteration 541, loss = 0.35643440\n",
      "Iteration 542, loss = 0.35607907\n",
      "Iteration 543, loss = 0.35575734\n",
      "Iteration 544, loss = 0.35536407\n",
      "Iteration 545, loss = 0.35485891\n",
      "Iteration 546, loss = 0.35453960\n",
      "Iteration 547, loss = 0.35467764\n",
      "Iteration 548, loss = 0.35374867\n",
      "Iteration 549, loss = 0.35386852\n",
      "Iteration 550, loss = 0.35345812\n",
      "Iteration 551, loss = 0.35306053\n",
      "Iteration 552, loss = 0.35265940\n",
      "Iteration 553, loss = 0.35254950\n",
      "Iteration 554, loss = 0.35196448\n",
      "Iteration 555, loss = 0.35169668\n",
      "Iteration 556, loss = 0.35148763\n",
      "Iteration 557, loss = 0.35098362\n",
      "Iteration 558, loss = 0.35072806\n",
      "Iteration 559, loss = 0.35016312\n",
      "Iteration 560, loss = 0.34983459\n",
      "Iteration 561, loss = 0.34975742\n",
      "Iteration 562, loss = 0.34961740\n",
      "Iteration 563, loss = 0.34939895\n",
      "Iteration 564, loss = 0.34899808\n",
      "Iteration 565, loss = 0.34864870\n",
      "Iteration 566, loss = 0.34846508\n",
      "Iteration 567, loss = 0.34785794\n",
      "Iteration 568, loss = 0.34747585\n",
      "Iteration 569, loss = 0.34735370\n",
      "Iteration 570, loss = 0.34687434\n",
      "Iteration 571, loss = 0.34727598\n",
      "Iteration 572, loss = 0.34630958\n",
      "Iteration 573, loss = 0.34568784\n",
      "Iteration 574, loss = 0.34597074\n",
      "Iteration 575, loss = 0.34508908\n",
      "Iteration 576, loss = 0.34539348\n",
      "Iteration 577, loss = 0.34461651\n",
      "Iteration 578, loss = 0.34483488\n",
      "Iteration 579, loss = 0.34402811\n",
      "Iteration 580, loss = 0.34348593\n",
      "Iteration 581, loss = 0.34344187\n",
      "Iteration 582, loss = 0.34340898\n",
      "Iteration 583, loss = 0.34306552\n",
      "Iteration 584, loss = 0.34244058\n",
      "Iteration 585, loss = 0.34234373\n",
      "Iteration 586, loss = 0.34258957\n",
      "Iteration 587, loss = 0.34160353\n",
      "Iteration 588, loss = 0.34119510\n",
      "Iteration 589, loss = 0.34126134\n",
      "Iteration 590, loss = 0.34067005\n",
      "Iteration 591, loss = 0.34064251\n",
      "Iteration 592, loss = 0.34025939\n",
      "Iteration 593, loss = 0.33980772\n",
      "Iteration 594, loss = 0.33972771\n",
      "Iteration 595, loss = 0.33909771\n",
      "Iteration 596, loss = 0.33919114\n",
      "Iteration 597, loss = 0.33886066\n",
      "Iteration 598, loss = 0.33858367\n",
      "Iteration 599, loss = 0.33809647\n",
      "Iteration 600, loss = 0.33753179\n",
      "Iteration 601, loss = 0.33753911\n",
      "Iteration 602, loss = 0.33712683\n",
      "Iteration 603, loss = 0.33701511\n",
      "Iteration 604, loss = 0.33667084\n",
      "Iteration 605, loss = 0.33650900\n",
      "Iteration 606, loss = 0.33639925\n",
      "Iteration 607, loss = 0.33591371\n",
      "Iteration 608, loss = 0.33550999\n",
      "Iteration 609, loss = 0.33520024\n",
      "Iteration 610, loss = 0.33468242\n",
      "Iteration 611, loss = 0.33418643\n",
      "Iteration 612, loss = 0.33510451\n",
      "Iteration 613, loss = 0.33419021\n",
      "Iteration 614, loss = 0.33408385\n",
      "Iteration 615, loss = 0.33404690\n",
      "Iteration 616, loss = 0.33332170\n",
      "Iteration 617, loss = 0.33340217\n",
      "Iteration 618, loss = 0.33230043\n",
      "Iteration 619, loss = 0.33233776\n",
      "Iteration 620, loss = 0.33182186\n",
      "Iteration 621, loss = 0.33220588\n",
      "Iteration 622, loss = 0.33141657\n",
      "Iteration 623, loss = 0.33146562\n",
      "Iteration 624, loss = 0.33105709\n",
      "Iteration 625, loss = 0.33012314\n",
      "Iteration 626, loss = 0.33081313\n",
      "Iteration 627, loss = 0.33057399\n",
      "Iteration 628, loss = 0.33042077\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69557048\n",
      "Iteration 2, loss = 0.68832078\n",
      "Iteration 3, loss = 0.68129072\n",
      "Iteration 4, loss = 0.67580843\n",
      "Iteration 5, loss = 0.67064266\n",
      "Iteration 6, loss = 0.66626475\n",
      "Iteration 7, loss = 0.66181960\n",
      "Iteration 8, loss = 0.65802427\n",
      "Iteration 9, loss = 0.65438494\n",
      "Iteration 10, loss = 0.65102107\n",
      "Iteration 11, loss = 0.64803174\n",
      "Iteration 12, loss = 0.64504337\n",
      "Iteration 13, loss = 0.64221373\n",
      "Iteration 14, loss = 0.63985705\n",
      "Iteration 15, loss = 0.63752045\n",
      "Iteration 16, loss = 0.63554171\n",
      "Iteration 17, loss = 0.63369077\n",
      "Iteration 18, loss = 0.63190619\n",
      "Iteration 19, loss = 0.62997864\n",
      "Iteration 20, loss = 0.62873338\n",
      "Iteration 21, loss = 0.62713208\n",
      "Iteration 22, loss = 0.62573013\n",
      "Iteration 23, loss = 0.62433261\n",
      "Iteration 24, loss = 0.62320466\n",
      "Iteration 25, loss = 0.62193331\n",
      "Iteration 26, loss = 0.62066539\n",
      "Iteration 27, loss = 0.61981110\n",
      "Iteration 28, loss = 0.61861837\n",
      "Iteration 29, loss = 0.61762059\n",
      "Iteration 30, loss = 0.61664769\n",
      "Iteration 31, loss = 0.61606316\n",
      "Iteration 32, loss = 0.61513647\n",
      "Iteration 33, loss = 0.61433762\n",
      "Iteration 34, loss = 0.61363863\n",
      "Iteration 35, loss = 0.61254082\n",
      "Iteration 36, loss = 0.61167222\n",
      "Iteration 37, loss = 0.61054677\n",
      "Iteration 38, loss = 0.60958473\n",
      "Iteration 39, loss = 0.60882592\n",
      "Iteration 40, loss = 0.60800781\n",
      "Iteration 41, loss = 0.60755787\n",
      "Iteration 42, loss = 0.60700703\n",
      "Iteration 43, loss = 0.60636916\n",
      "Iteration 44, loss = 0.60486958\n",
      "Iteration 45, loss = 0.60395449\n",
      "Iteration 46, loss = 0.60293976\n",
      "Iteration 47, loss = 0.60250434\n",
      "Iteration 48, loss = 0.60131185\n",
      "Iteration 49, loss = 0.60061525\n",
      "Iteration 50, loss = 0.59959561\n",
      "Iteration 51, loss = 0.59866139\n",
      "Iteration 52, loss = 0.59794449\n",
      "Iteration 53, loss = 0.59701007\n",
      "Iteration 54, loss = 0.59616625\n",
      "Iteration 55, loss = 0.59524445\n",
      "Iteration 56, loss = 0.59466916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 57, loss = 0.59360498\n",
      "Iteration 58, loss = 0.59283411\n",
      "Iteration 59, loss = 0.59236077\n",
      "Iteration 60, loss = 0.59129629\n",
      "Iteration 61, loss = 0.59014916\n",
      "Iteration 62, loss = 0.58937613\n",
      "Iteration 63, loss = 0.58847161\n",
      "Iteration 64, loss = 0.58784822\n",
      "Iteration 65, loss = 0.58756885\n",
      "Iteration 66, loss = 0.58669904\n",
      "Iteration 67, loss = 0.58586686\n",
      "Iteration 68, loss = 0.58499874\n",
      "Iteration 69, loss = 0.58433383\n",
      "Iteration 70, loss = 0.58356722\n",
      "Iteration 71, loss = 0.58298345\n",
      "Iteration 72, loss = 0.58238775\n",
      "Iteration 73, loss = 0.58123554\n",
      "Iteration 74, loss = 0.58181940\n",
      "Iteration 75, loss = 0.58110377\n",
      "Iteration 76, loss = 0.57968063\n",
      "Iteration 77, loss = 0.57911078\n",
      "Iteration 78, loss = 0.57821905\n",
      "Iteration 79, loss = 0.57760236\n",
      "Iteration 80, loss = 0.57682875\n",
      "Iteration 81, loss = 0.57570619\n",
      "Iteration 82, loss = 0.57500264\n",
      "Iteration 83, loss = 0.57427978\n",
      "Iteration 84, loss = 0.57372682\n",
      "Iteration 85, loss = 0.57364677\n",
      "Iteration 86, loss = 0.57249943\n",
      "Iteration 87, loss = 0.57140596\n",
      "Iteration 88, loss = 0.57085800\n",
      "Iteration 89, loss = 0.57099548\n",
      "Iteration 90, loss = 0.56949015\n",
      "Iteration 91, loss = 0.56859517\n",
      "Iteration 92, loss = 0.56791710\n",
      "Iteration 93, loss = 0.56725049\n",
      "Iteration 94, loss = 0.56680171\n",
      "Iteration 95, loss = 0.56599617\n",
      "Iteration 96, loss = 0.56558637\n",
      "Iteration 97, loss = 0.56475833\n",
      "Iteration 98, loss = 0.56416738\n",
      "Iteration 99, loss = 0.56347934\n",
      "Iteration 100, loss = 0.56278261\n",
      "Iteration 101, loss = 0.56274616\n",
      "Iteration 102, loss = 0.56166509\n",
      "Iteration 103, loss = 0.56094746\n",
      "Iteration 104, loss = 0.56068499\n",
      "Iteration 105, loss = 0.55999284\n",
      "Iteration 106, loss = 0.55996152\n",
      "Iteration 107, loss = 0.55898480\n",
      "Iteration 108, loss = 0.55843430\n",
      "Iteration 109, loss = 0.55766164\n",
      "Iteration 110, loss = 0.55745274\n",
      "Iteration 111, loss = 0.55680424\n",
      "Iteration 112, loss = 0.55609493\n",
      "Iteration 113, loss = 0.55518493\n",
      "Iteration 114, loss = 0.55797280\n",
      "Iteration 115, loss = 0.55550014\n",
      "Iteration 116, loss = 0.55520088\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[0.83858071 0.839      0.8405     0.846      0.73186593]\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn MLP training \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# preprocessing data \n",
    "\n",
    "X_scale = preprocessing.scale(datasets)\n",
    "# print(X_scale)\n",
    "\n",
    "# sgd optimizer \n",
    "mlp = MLPClassifier(solver='sgd', activation='relu',alpha=1e-4, hidden_layer_sizes=(128, 128),\n",
    "                    random_state=1, max_iter=1000,verbose=True)\n",
    "\n",
    "# lbfgs - very slow \n",
    "# mlp = MLPClassifier(solver='lbfgs', activation='relu',alpha=1e-4,hidden_layer_sizes=(100,100),\n",
    "#                     random_state=1,max_iter=50,verbose=10,learning_rate_init=.1)\n",
    "\n",
    "# adam \n",
    "# mlp = MLPClassifier(solver='adam', activation='relu',alpha=1e-4,hidden_layer_sizes=(100,100),\n",
    "#                     random_state=1,max_iter=50,verbose=10,learning_rate_init=.1)\n",
    "\n",
    "# mlp.fit(X_scale, output_scikit) \n",
    "# print(mlp.classes_)\n",
    "# print(mlp.n_layers_)\n",
    "# print(mlp.n_iter_)\n",
    "# print(mlp.loss_)\n",
    "# print(mlp.out_activation_)\n",
    "# print(mlp.n_outputs_)\n",
    "\n",
    "score_mlp = cross_val_score(mlp, X_scale, output_scikit,cv=5)  \n",
    "print(score_mlp)\n",
    "\n",
    "# 100000 data ~ 93%\n",
    "# 10000 data - 85%\n",
    "# 100000 data using 256, 256 - 91%\n",
    "# 500000 data (128,128) - 96%   0.96623034 0.96747    0.96845    0.96848    0.96720967\n",
    "# 50000 (128,128) [0.89191081 0.8818     0.8794     0.8831     0.86838684]\n",
    "# 10000[0.82358821 0.8385     0.8275     0.8375     0.76788394]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This MLPClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-72d2f272ecfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# FEM rank VS DNN probability rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0mare\u001b[0m \u001b[0mordered\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \"\"\"\n\u001b[0;32m-> 1049\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coefs_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This MLPClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "# FEM rank VS DNN probability rank \n",
    "\n",
    "prob = mlp.predict_proba(X_scale[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sgd optimizer \n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "# mlp_regress = MLPRegressor(solver='sgd', activation='relu',alpha=1e-4, hidden_layer_sizes=(128, 128),\n",
    "#                     random_state=1, max_iter=1000,verbose=True)\n",
    "\n",
    "# # lbfgs - very slow \n",
    "# # mlp = MLPClassifier(solver='lbfgs', activation='relu',alpha=1e-4,hidden_layer_sizes=(100,100),\n",
    "# #                     random_state=1,max_iter=50,verbose=10,learning_rate_init=.1)\n",
    "\n",
    "# # adam \n",
    "# # mlp = MLPClassifier(solver='adam', activation='relu',alpha=1e-4,hidden_layer_sizes=(100,100),\n",
    "# #                     random_state=1,max_iter=50,verbose=10,learning_rate_init=.1)\n",
    "\n",
    "# # mlp.fit(X_scale, output_scikit) \n",
    "# # print(mlp.classes_)\n",
    "# # print(mlp.n_layers_)\n",
    "# # print(mlp.n_iter_)\n",
    "# # print(mlp.loss_)\n",
    "# # print(mlp.out_activation_)\n",
    "# # print(mlp.n_outputs_)\n",
    "# X_Re = preprocessing.scale(datasets)\n",
    "# Y_Re = preprocessing.scale(results_map)\n",
    "\n",
    "# score_regress = cross_val_score(mlp_regress, X_Re, Y_Re, cv=5)  \n",
    "# print(score_regress)\n",
    "# # [0.97351663 0.97390639 0.97552892 0.97428681 0.97503282] for 500000 data with [128,128]\n",
    "# # [0.85165701 0.83106467 0.82760219 0.86254636 0.8405507 ] for 50000 data \n",
    "# # [0.69529696 0.68323394 0.6818702  0.68295741 0.72217451] for 10000 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
