{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.435136079788208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([  2.,   2.,   0.,   2.,   2.,   6.,   1.,  11.,   4.,  10.,   8.,\n",
       "         42.,  76.,  82.,  86., 206., 262., 316., 380., 334., 364., 616.,\n",
       "        388., 426., 370., 300., 490., 324., 640., 462., 356., 310., 330.,\n",
       "        296., 288., 242., 294., 292., 292., 176., 204., 168.,  84., 116.,\n",
       "        144.,  48.,  48.,  36.,  40.,  24.]),\n",
       " array([294180.29831659, 303010.98369762, 311841.66907865, 320672.35445968,\n",
       "        329503.0398407 , 338333.72522173, 347164.41060276, 355995.09598379,\n",
       "        364825.78136482, 373656.46674585, 382487.15212688, 391317.8375079 ,\n",
       "        400148.52288893, 408979.20826996, 417809.89365099, 426640.57903202,\n",
       "        435471.26441305, 444301.94979408, 453132.63517511, 461963.32055613,\n",
       "        470794.00593716, 479624.69131819, 488455.37669922, 497286.06208025,\n",
       "        506116.74746128, 514947.43284231, 523778.11822333, 532608.80360436,\n",
       "        541439.48898539, 550270.17436642, 559100.85974745, 567931.54512848,\n",
       "        576762.23050951, 585592.91589053, 594423.60127156, 603254.28665259,\n",
       "        612084.97203362, 620915.65741465, 629746.34279568, 638577.02817671,\n",
       "        647407.71355774, 656238.39893876, 665069.08431979, 673899.76970082,\n",
       "        682730.45508185, 691561.14046288, 700391.82584391, 709222.51122494,\n",
       "        718053.19660596, 726883.88198699, 735714.56736802]),\n",
       " <a list of 50 Patch objects>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from math import*\n",
    "import __main__\n",
    "global PI\n",
    "import os\n",
    "import time\n",
    "PI=float(acos(-1))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from itertools import combinations, permutations\n",
    "import itertools\n",
    "\n",
    "# rs_number = 123\n",
    "# random.seed(rs_number)\n",
    "\n",
    "num_layer = 4\n",
    "data_size = 10**num_layer ;\n",
    "\n",
    "datasets0 = []\n",
    "x_range = [0,1,2,3,4,5,6,7,8,9]\n",
    "for i in itertools.product(x_range, repeat=num_layer):\n",
    "    datasets0.append(list(i))\n",
    "\n",
    "datasets = np.reshape(datasets0,(data_size,num_layer))\n",
    "# print(datasets)\n",
    "\n",
    "def ssh(num):\n",
    "#     global datasets\n",
    "#     arr = np.random.randint(1, 10, size=[data_size, 8])\n",
    "#     datasets = arr.tolist()\n",
    "    ply_angle = datasets[num]\n",
    "    \n",
    "#     bas_ply=[-60, -45, -30, -15, 0, 15, 30, 45, 60, 90]\n",
    "    bas_ply=[-60, -45, -30, -15, 0, 15, 30, 45, 60, 90]\n",
    "\n",
    "    AA1=bas_ply[ply_angle[0]]\n",
    "    AA2=bas_ply[ply_angle[1]]\n",
    "    AA3=bas_ply[ply_angle[2]]\n",
    "    AA4=bas_ply[ply_angle[3]]\n",
    "#     AA5=bas_ply[ply_angle[4]]\n",
    "#     AA6=bas_ply[ply_angle[5]]\n",
    "#     AA7=bas_ply[ply_angle[6]]\n",
    "#     AA8=bas_ply[ply_angle[7]]\n",
    "\n",
    "    ### ply stacking sequence###\n",
    "    # 8 layers\n",
    "#     AAA=[AA1/180.0*PI,AA2/180.0*PI,AA3/180.0*PI,AA4/180.0*PI,AA5/180.0*PI,AA6/180.0*PI,AA7/180.0*PI,AA8/180.0*PI,AA8/180.0*PI,AA7/180.0*PI,AA6/180.0*PI,AA5/180.0*PI,AA4/180.0*PI,AA3/180.0*PI,AA2/180.0*PI,AA1/180.0*PI]\n",
    "    # 4 layers\n",
    "    AAA=[AA1/180.0*PI,AA2/180.0*PI,AA3/180.0*PI,AA4/180.0*PI,AA4/180.0*PI,AA3/180.0*PI,AA2/180.0*PI,AA1/180.0*PI]\n",
    "    \n",
    "    \n",
    "    pi=3.14159265358979\n",
    "\n",
    "    R=250.0   ##  radius##\n",
    "    H=510.0   ##  Height##\n",
    "    td=1.0/num_layer  #layer thickness##\n",
    "    \n",
    "\n",
    "    # 8 layers\n",
    "#     TTT=[-td*8,-td*7,-td*6,-td*5,-td*4,-td*3,-td*2,-td*1,td*0,td*1,td*2,td*3,td*4,td*5,td*6,td*7,td*8]\n",
    "    # 4 layers\n",
    "    TTT=[-td*4,-td*3,-td*2,-td*1,td*0,td*1,td*2,td*3,td*4]\n",
    "\n",
    "    ###material property###\n",
    "\n",
    "    E1=123550.0  \n",
    "    E2=8707.9\n",
    "    G12=5695.0\n",
    "    miu12=0.31946\n",
    "\n",
    "    miu21=miu12*E2/E1\n",
    "    Q11=E1/(1-miu12*miu21)\n",
    "    Q12=miu21*E1/(1-miu12*miu21)\n",
    "    Q22=E2/(1-miu12*miu21)\n",
    "    Q66=G12\n",
    "\n",
    "    A11=0.0\n",
    "    A12=0.0\n",
    "    A22=0.0\n",
    "    A66=0.0\n",
    "\n",
    "    D11=0.0\n",
    "    D12=0.0\n",
    "    D22=0.0\n",
    "    D66=0.0\n",
    "\n",
    "    for i in range(0,num_layer*2):\n",
    "        A11=A11+(Q11*cos(AAA[i])**4+2*(Q12+2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q22*sin(AAA[i])**4)*(TTT[i+1]-TTT[i])\n",
    "        A12=A12+((Q11+Q22-4*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q12*(sin(AAA[i])**4+cos(AAA[i])**4))*(TTT[i+1]-TTT[i])\n",
    "        A22=A22+(Q11*sin(AAA[i])**4+2*(Q12+2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q22*cos(AAA[i])**4)*(TTT[i+1]-TTT[i])\n",
    "        A66=A66+((Q11+Q22-2*Q12-2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q66*(sin(AAA[i])**4+cos(AAA[i])**4))*(TTT[i+1]-TTT[i])\n",
    "        D11=D11+(Q11*cos(AAA[i])**4+2*(Q12+2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q22*sin(AAA[i])**4)*(TTT[i+1]**3-TTT[i]**3)/3.0\n",
    "        D12=D12+((Q11+Q22-4*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q12*(sin(AAA[i])**4+cos(AAA[i])**4))*(TTT[i+1]**3-TTT[i]**3)/3.0\n",
    "        D22=D22+(Q11*sin(AAA[i])**4+2*(Q12+2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q22*cos(AAA[i])**4)*(TTT[i+1]**3-TTT[i]**3)/3.0\n",
    "        D66=D66+((Q11+Q22-2*Q12-2*Q66)*sin(AAA[i])**2*cos(AAA[i])**2+Q66*(sin(AAA[i])**4+cos(AAA[i])**4))*(TTT[i+1]**3-TTT[i]**3)/3.0\n",
    "\n",
    "    #####bianliang########\n",
    "\n",
    "    D= 2*R\n",
    "    L= H\n",
    "\n",
    "    #xian\n",
    "    a=[[A11,A12,0],[A12,A22,0],[0,0,A66]]\n",
    "    b=[[0,0,0],[0,0,0],[0,0,0]]\n",
    "    d=[[D11,D12,0],[D12,D22,0],[0,0,D66]]\n",
    "\n",
    "    alpha=PI/L\n",
    "    beta=2/D\n",
    "\n",
    "    mm=50\n",
    "    nn=50\n",
    "    kmm=0\n",
    "    knn=0\n",
    "    kmm11=0\n",
    "    knn11=0\n",
    "    F=[[0 for col in range(nn)] for row in range(mm)]\n",
    "    Fcr=1e16\n",
    "\n",
    "    for m in range(1,mm+1):\n",
    "        for n in range(1,nn+1):\n",
    "            xi11=2*a[0][0]*(m*alpha)**2+2*a[2][2]*(n*beta)**2\n",
    "            xi12=2*(a[0][1]+a[2][2])*m*alpha*n*beta\n",
    "            xi13=4*a[0][1]*m*alpha/D-2*b[0][0]*(m*alpha)**3-2*(b[0][1]+2*b[2][2])*m*alpha*(n*beta)**2\n",
    "            xi22=2*a[1][1]*(n*beta)**2+2*a[2][2]*(m*alpha)**2\n",
    "            xi23=4*a[1][1]*n*beta/D-2*b[1][1]*(n*beta)**3-2*(b[0][1]+2*b[2][2])*(m*alpha)**2*n*beta\n",
    "            xi33=4*(d[0][1]+2*d[2][2])*(m*alpha*n*beta)**2+8*a[1][1]/(D**2)\\\n",
    "            +2*d[0][0]*(m*alpha)**4+2*d[1][1]*(n*beta)**4-8*(b[1][1]*(n*beta)**2+b[0][1]*(m*alpha)**2)/D\n",
    "            xi21=xi12\n",
    "            xi31=xi13\n",
    "            xi32=xi23\n",
    "            det1=xi11*(xi22*xi33-xi32*xi23)-xi12*(xi21*xi33-xi31*xi23)+xi13*(xi21*xi32-xi31*xi22)\n",
    "            det2=xi11*xi22-xi21*xi12\n",
    "            Nx=det1/det2/(2*((m*alpha)**2))\n",
    "            F[m-1][n-1]=Nx*PI*D\n",
    "            if Fcr>F[m-1][n-1]:\n",
    "              Fcr=F[m-1][n-1]\n",
    "              kmm=m\n",
    "              knn=2*n\n",
    "\n",
    "    for m in range(1,mm+1):\n",
    "        xi11=2*a[0][0]*(m*alpha)**2\n",
    "        xi12=0\n",
    "        xi13=4*a[0][1]*m*alpha/D-2*b[0][0]*(m*alpha)**3\n",
    "        xi22=2*a[2][2]*(m*alpha)**2\n",
    "        xi23=0\n",
    "        xi33=8*a[1][1]/(D**2)+2*d[0][0]*(m*alpha)**4-8*b[0][1]*(m*alpha)**2/D\n",
    "        xi21=xi12\n",
    "        xi31=xi13\n",
    "        xi32=xi23\n",
    "        det1=xi11*(xi22*xi33-xi32*xi23)-xi12*(xi21*xi33-xi31*xi23)+xi13*(xi21*xi32-xi31*xi22)\n",
    "        det2=xi11*xi22-xi21*xi12\n",
    "        Nx=det1/det2/(2*((m*alpha)**2))\n",
    "        Fn1=Nx*PI*D\n",
    "        if Fcr>Fn1:\n",
    "          Fcr=Fn1\n",
    "          kmm=m\n",
    "          knn=1\n",
    "\n",
    "#     return np.log(Fcr/800000)\n",
    "    return Fcr\n",
    "\n",
    "# map async parallel \n",
    "# pool = mp.Pool(mp.cpu_count())\n",
    "pool = mp.Pool(processes = 8)\n",
    "\n",
    "start = time.time()\n",
    "Fcr = pool.map(ssh, range(data_size))\n",
    "end = time.time() \n",
    "print(end - start) #0.0037827491760253906\n",
    "\n",
    "results_map = Fcr\n",
    "# plt.figure()\n",
    "# plt.hist(results_map,50)\n",
    "plt.figure()\n",
    "plt.hist(Fcr,50)\n",
    "\n",
    "## data export to csv\n",
    "print(len(datasets))\n",
    "print(type(Fcr))\n",
    "\n",
    "y_output = np.asarray(Fcr)\n",
    "y2 = np.reshape(y_output,(10000,-1))\n",
    "print(len(y2))\n",
    "\n",
    "print(datasets)\n",
    "print(y2)\n",
    "full_data = np.concatenate((datasets,y2),axis=1)\n",
    "print(len(full_data))\n",
    "\n",
    "print(full_data)\n",
    "np.savetxt(\"composite.csv\", full_data, delimiter=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "<class 'list'>\n",
      "10000\n",
      "[[0 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 2]\n",
      " ...\n",
      " [9 9 9 7]\n",
      " [9 9 9 8]\n",
      " [9 9 9 9]]\n",
      "[[426661.95790378]\n",
      " [405575.8766555 ]\n",
      " [427810.79595699]\n",
      " ...\n",
      " [398285.85283158]\n",
      " [378202.54300688]\n",
      " [294180.29831659]]\n",
      "10000\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  4.26661958e+05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  4.05575877e+05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 2.00000000e+00\n",
      "  4.27810796e+05]\n",
      " ...\n",
      " [9.00000000e+00 9.00000000e+00 9.00000000e+00 7.00000000e+00\n",
      "  3.98285853e+05]\n",
      " [9.00000000e+00 9.00000000e+00 9.00000000e+00 8.00000000e+00\n",
      "  3.78202543e+05]\n",
      " [9.00000000e+00 9.00000000e+00 9.00000000e+00 9.00000000e+00\n",
      "  2.94180298e+05]]\n"
     ]
    }
   ],
   "source": [
    "print(len(datasets))\n",
    "print(type(Fcr))\n",
    "\n",
    "y_output = np.asarray(Fcr)\n",
    "y2 = np.reshape(y_output,(10000,-1))\n",
    "print(len(y2))\n",
    "\n",
    "print(datasets)\n",
    "print(y2)\n",
    "full_data = np.concatenate((datasets,y2),axis=1)\n",
    "print(len(full_data))\n",
    "\n",
    "print(full_data)\n",
    "np.savetxt(\"composite.csv\", full_data, delimiter=\" \")\n",
    "# a = np.array([[1, 2], [3, 4]])\n",
    "# print(a)\n",
    "# b = np.array([[5], [6]])\n",
    "# print(b)\n",
    "# c = np.concatenate((a, b), axis=1)\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1600.)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "View more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "My Youtube Channel: https://www.youtube.com/user/MorvanZhou\n",
    "Dependencies:\n",
    "torch: 0.1.11\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas\n",
    "from torch.autograd import Variable # torch 中 Variable 模块\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(1)    # reproducible\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "data_size = 10000 # n = 10000000 the minimum is 81, so hard to find 64 which is the global minimum\n",
    "x = torch.randint(1,10,(data_size,8))\n",
    "x = x.float()\n",
    "y0 = torch.sum(x,1)**2\n",
    "\n",
    "\n",
    "y_sort = sorted(y0) \n",
    "median_value = y_sort[data_size//2]\n",
    "print(median_value)\n",
    "y = torch.zeros(data_size,1)\n",
    "\n",
    "for i in range(data_size):\n",
    "    if y0[i] > median_value:\n",
    "        y[i] = 1\n",
    "    else:\n",
    "        y[i] = 0\n",
    "\n",
    "torch_dataset = Data.TensorDataset(x, y)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # random shuffle for training\n",
    "    num_workers=2,              # subprocesses for loading data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = nn.Linear(8, 100)   # hidden layer\n",
    "        self.fc1 = nn.Linear(100, 1)   # output layer\n",
    "        self.out = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum = 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.657\n",
      "[2,   100] loss: 0.605\n",
      "[3,   100] loss: 0.569\n",
      "[4,   100] loss: 0.540\n",
      "[5,   100] loss: 0.518\n",
      "[6,   100] loss: 0.503\n",
      "[7,   100] loss: 0.488\n",
      "[8,   100] loss: 0.476\n",
      "[9,   100] loss: 0.464\n",
      "[10,   100] loss: 0.455\n",
      "[11,   100] loss: 0.449\n",
      "[12,   100] loss: 0.442\n",
      "[13,   100] loss: 0.434\n",
      "[14,   100] loss: 0.426\n",
      "[15,   100] loss: 0.421\n",
      "[16,   100] loss: 0.414\n",
      "[17,   100] loss: 0.410\n",
      "[18,   100] loss: 0.402\n",
      "[19,   100] loss: 0.400\n",
      "[20,   100] loss: 0.392\n",
      "[21,   100] loss: 0.386\n",
      "[22,   100] loss: 0.382\n",
      "[23,   100] loss: 0.375\n",
      "[24,   100] loss: 0.368\n",
      "[25,   100] loss: 0.362\n",
      "[26,   100] loss: 0.361\n",
      "[27,   100] loss: 0.354\n",
      "[28,   100] loss: 0.348\n",
      "[29,   100] loss: 0.345\n",
      "[30,   100] loss: 0.341\n",
      "[31,   100] loss: 0.336\n",
      "[32,   100] loss: 0.327\n",
      "[33,   100] loss: 0.322\n",
      "[34,   100] loss: 0.320\n",
      "[35,   100] loss: 0.314\n",
      "[36,   100] loss: 0.308\n",
      "[37,   100] loss: 0.305\n",
      "[38,   100] loss: 0.299\n",
      "[39,   100] loss: 0.294\n",
      "[40,   100] loss: 0.290\n",
      "[41,   100] loss: 0.283\n",
      "[42,   100] loss: 0.280\n",
      "[43,   100] loss: 0.279\n",
      "[44,   100] loss: 0.272\n",
      "[45,   100] loss: 0.266\n",
      "[46,   100] loss: 0.264\n",
      "[47,   100] loss: 0.259\n",
      "[48,   100] loss: 0.257\n",
      "[49,   100] loss: 0.253\n",
      "[50,   100] loss: 0.248\n",
      "[51,   100] loss: 0.248\n",
      "[52,   100] loss: 0.243\n",
      "[53,   100] loss: 0.236\n",
      "[54,   100] loss: 0.234\n",
      "[55,   100] loss: 0.230\n",
      "[56,   100] loss: 0.226\n",
      "[57,   100] loss: 0.223\n",
      "[58,   100] loss: 0.223\n",
      "[59,   100] loss: 0.218\n",
      "[60,   100] loss: 0.215\n",
      "[61,   100] loss: 0.212\n",
      "[62,   100] loss: 0.211\n",
      "[63,   100] loss: 0.208\n",
      "[64,   100] loss: 0.204\n",
      "[65,   100] loss: 0.206\n",
      "[66,   100] loss: 0.200\n",
      "[67,   100] loss: 0.197\n",
      "[68,   100] loss: 0.193\n",
      "[69,   100] loss: 0.192\n",
      "[70,   100] loss: 0.189\n",
      "[71,   100] loss: 0.188\n",
      "[72,   100] loss: 0.187\n",
      "[73,   100] loss: 0.184\n",
      "[74,   100] loss: 0.183\n",
      "[75,   100] loss: 0.182\n",
      "[76,   100] loss: 0.180\n",
      "[77,   100] loss: 0.174\n",
      "[78,   100] loss: 0.173\n",
      "[79,   100] loss: 0.172\n",
      "[80,   100] loss: 0.174\n",
      "[81,   100] loss: 0.168\n",
      "[82,   100] loss: 0.165\n",
      "[83,   100] loss: 0.165\n",
      "[84,   100] loss: 0.164\n",
      "[85,   100] loss: 0.164\n",
      "[86,   100] loss: 0.160\n",
      "[87,   100] loss: 0.158\n",
      "[88,   100] loss: 0.158\n",
      "[89,   100] loss: 0.161\n",
      "[90,   100] loss: 0.153\n",
      "[91,   100] loss: 0.153\n",
      "[92,   100] loss: 0.151\n",
      "[93,   100] loss: 0.153\n",
      "[94,   100] loss: 0.150\n",
      "[95,   100] loss: 0.148\n",
      "[96,   100] loss: 0.150\n",
      "[97,   100] loss: 0.144\n",
      "[98,   100] loss: 0.142\n",
      "[99,   100] loss: 0.141\n",
      "[100,   100] loss: 0.147\n",
      "[101,   100] loss: 0.140\n",
      "[102,   100] loss: 0.138\n",
      "[103,   100] loss: 0.139\n",
      "[104,   100] loss: 0.136\n",
      "[105,   100] loss: 0.135\n",
      "[106,   100] loss: 0.136\n",
      "[107,   100] loss: 0.134\n",
      "[108,   100] loss: 0.132\n",
      "[109,   100] loss: 0.131\n",
      "[110,   100] loss: 0.131\n",
      "[111,   100] loss: 0.132\n",
      "[112,   100] loss: 0.130\n",
      "[113,   100] loss: 0.133\n",
      "[114,   100] loss: 0.129\n",
      "[115,   100] loss: 0.129\n",
      "[116,   100] loss: 0.124\n",
      "[117,   100] loss: 0.128\n",
      "[118,   100] loss: 0.123\n",
      "[119,   100] loss: 0.124\n",
      "[120,   100] loss: 0.127\n",
      "[121,   100] loss: 0.122\n",
      "[122,   100] loss: 0.119\n",
      "[123,   100] loss: 0.119\n",
      "[124,   100] loss: 0.120\n",
      "[125,   100] loss: 0.117\n",
      "[126,   100] loss: 0.119\n",
      "[127,   100] loss: 0.127\n",
      "[128,   100] loss: 0.115\n",
      "[129,   100] loss: 0.118\n",
      "[130,   100] loss: 0.115\n",
      "[131,   100] loss: 0.113\n",
      "[132,   100] loss: 0.112\n",
      "[133,   100] loss: 0.113\n",
      "[134,   100] loss: 0.111\n",
      "[135,   100] loss: 0.112\n",
      "[136,   100] loss: 0.110\n",
      "[137,   100] loss: 0.109\n",
      "[138,   100] loss: 0.108\n",
      "[139,   100] loss: 0.107\n",
      "[140,   100] loss: 0.110\n",
      "[141,   100] loss: 0.106\n",
      "[142,   100] loss: 0.110\n",
      "[143,   100] loss: 0.107\n",
      "[144,   100] loss: 0.110\n",
      "[145,   100] loss: 0.107\n",
      "[146,   100] loss: 0.106\n",
      "[147,   100] loss: 0.104\n",
      "[148,   100] loss: 0.102\n",
      "[149,   100] loss: 0.101\n",
      "[150,   100] loss: 0.103\n",
      "[151,   100] loss: 0.102\n",
      "[152,   100] loss: 0.101\n",
      "[153,   100] loss: 0.101\n",
      "[154,   100] loss: 0.100\n",
      "[155,   100] loss: 0.102\n",
      "[156,   100] loss: 0.101\n",
      "[157,   100] loss: 0.098\n",
      "[158,   100] loss: 0.096\n",
      "[159,   100] loss: 0.098\n",
      "[160,   100] loss: 0.099\n",
      "[161,   100] loss: 0.097\n",
      "[162,   100] loss: 0.099\n",
      "[163,   100] loss: 0.097\n",
      "[164,   100] loss: 0.095\n",
      "[165,   100] loss: 0.098\n",
      "[166,   100] loss: 0.095\n",
      "[167,   100] loss: 0.094\n",
      "[168,   100] loss: 0.093\n",
      "[169,   100] loss: 0.092\n",
      "[170,   100] loss: 0.091\n",
      "[171,   100] loss: 0.091\n",
      "[172,   100] loss: 0.093\n",
      "[173,   100] loss: 0.092\n",
      "[174,   100] loss: 0.093\n",
      "[175,   100] loss: 0.091\n",
      "[176,   100] loss: 0.094\n",
      "[177,   100] loss: 0.090\n",
      "[178,   100] loss: 0.095\n",
      "[179,   100] loss: 0.099\n",
      "[180,   100] loss: 0.089\n",
      "[181,   100] loss: 0.087\n",
      "[182,   100] loss: 0.088\n",
      "[183,   100] loss: 0.086\n",
      "[184,   100] loss: 0.090\n",
      "[185,   100] loss: 0.086\n",
      "[186,   100] loss: 0.087\n",
      "[187,   100] loss: 0.089\n",
      "[188,   100] loss: 0.088\n",
      "[189,   100] loss: 0.085\n",
      "[190,   100] loss: 0.089\n",
      "[191,   100] loss: 0.087\n",
      "[192,   100] loss: 0.085\n",
      "[193,   100] loss: 0.089\n",
      "[194,   100] loss: 0.083\n",
      "[195,   100] loss: 0.087\n",
      "[196,   100] loss: 0.085\n",
      "[197,   100] loss: 0.089\n",
      "[198,   100] loss: 0.084\n",
      "[199,   100] loss: 0.083\n",
      "[200,   100] loss: 0.084\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "niter = x.size(0) // BATCH_SIZE\n",
    "for epoch in range(200):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "#         print(i)\n",
    "        # get the inputs \n",
    "        inputs, labels = data\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimizer\n",
    "#         print(inputs)\n",
    "        outputs = net(inputs)\n",
    "#         print(outputs)\n",
    "#         print(labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print statistics\n",
    "        running_loss +=loss.item()\n",
    "        if i % niter == niter-1: \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch+1, i+1, running_loss/niter))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "print('done')\n",
    "# print(running_loss)\n",
    "# plt.plot(running_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "测试模型的时候，把tensor 转换成numpy 再用scikit learn 做容易一些\n",
    "\n",
    "if x is tensor  - x.numpy()\n",
    "if x is Variable - x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9892\n",
      "[2.8119332e-12]\n"
     ]
    }
   ],
   "source": [
    "# y_output = net(x).data.numpy()\n",
    "# print(y_output)\n",
    "# print(y)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_output = net(x).data.numpy()\n",
    "y_true = y\n",
    "y_pred = torch.zeros(data_size,1)\n",
    "\n",
    "for i in range(data_size):\n",
    "    if y_output[i]>0.5:\n",
    "        y_pred[i] = 1\n",
    "    else:\n",
    "        y_pred[i] = 0\n",
    "\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "        \n",
    "# x1 = torch.tensor([[2,1,1,2,2,1,6,2]])\n",
    "# print(x1)\n",
    "# print(x)\n",
    "# net(x1.float()).data.numpy()\n",
    "print(min(y_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross-validation \n",
    "#A scikit-learn compatible neural network library that wraps PyTorch.\n",
    "https://skorch.readthedocs.io/en/latest/?badge=latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(x.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
