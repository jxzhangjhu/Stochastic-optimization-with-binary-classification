{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generating\n",
    "1. generate data \n",
    "2. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 6., 1., 6., 8., 2., 3., 6.],\n",
      "        [9., 1., 3., 4., 2., 9., 5., 1.],\n",
      "        [4., 7., 3., 8., 7., 7., 9., 8.],\n",
      "        [7., 1., 8., 9., 9., 5., 6., 3.],\n",
      "        [7., 7., 8., 7., 9., 7., 3., 3.],\n",
      "        [7., 6., 6., 6., 1., 7., 1., 7.],\n",
      "        [3., 5., 4., 4., 5., 7., 1., 8.],\n",
      "        [6., 9., 4., 4., 5., 8., 4., 6.],\n",
      "        [4., 4., 8., 6., 2., 8., 5., 3.],\n",
      "        [3., 9., 8., 4., 3., 4., 9., 8.],\n",
      "        [4., 1., 9., 1., 2., 7., 8., 5.],\n",
      "        [8., 7., 6., 5., 2., 7., 1., 1.],\n",
      "        [1., 3., 3., 8., 6., 6., 7., 8.],\n",
      "        [5., 7., 2., 6., 7., 8., 2., 5.],\n",
      "        [9., 7., 2., 9., 7., 2., 6., 4.],\n",
      "        [5., 8., 3., 3., 3., 3., 8., 6.],\n",
      "        [2., 4., 7., 9., 4., 4., 2., 6.],\n",
      "        [4., 1., 7., 1., 7., 5., 6., 6.],\n",
      "        [1., 9., 1., 3., 4., 8., 1., 9.],\n",
      "        [7., 7., 5., 1., 8., 1., 8., 2.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "t() expects a 2D tensor, but self is 1D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-d0e0314c0a29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mfull_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: t() expects a 2D tensor, but self is 1D"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.manual_seed(1)    # reproducible\n",
    "\n",
    "data_size = 20 # n = 10000000 the minimum is 81, so hard to find 64 which is the global minimum\n",
    "x = torch.randint(1,10,(data_size,8))\n",
    "x = x.float()\n",
    "y = torch.sum(x,1)**2\n",
    "print(x)\n",
    "print(y.t())\n",
    "\n",
    "full_dataset = torch.cat([x, y], dim=0)\n",
    "print(full_dataset)\n",
    "\n",
    "# train_size = int(0.8 * len(data_size))\n",
    "# test_size = len(data_size) - train_size\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(x)\n",
    "# print(y)\n",
    "# print(min(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of datasets into good and bad value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sort = sorted(y) # note that sorted change the sort but didn't change the original one\n",
    "# print(results_map_sort)\n",
    "median_value = y_sort[data_size//2]\n",
    "# median_value = 3.5\n",
    "# print(median_value)\n",
    "\n",
    "# y_new = torch.LongTensor(data_size,1)\n",
    "y_new = torch.zeros(data_size,1)\n",
    "\n",
    "for i in range(data_size):\n",
    "    if y[i] > median_value:\n",
    "        y_new[i] = 1\n",
    "    else:\n",
    "        y_new[i] = 0\n",
    "\n",
    "# y_new = y_new.long()\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create NN and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden): Linear(in_features=8, out_features=100, bias=True)\n",
      "  (out): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (out_act): Sigmoid()\n",
      ")\n",
      "tensor([[0.4322],\n",
      "        [0.6143],\n",
      "        [0.4486],\n",
      "        [0.5717],\n",
      "        [0.3903],\n",
      "        [0.4116],\n",
      "        [0.3882],\n",
      "        [0.3752],\n",
      "        [0.4354],\n",
      "        [0.4010],\n",
      "        [0.4029],\n",
      "        [0.3624],\n",
      "        [0.5032],\n",
      "        [0.4551],\n",
      "        [0.4941],\n",
      "        [0.4040],\n",
      "        [0.5068],\n",
      "        [0.4126],\n",
      "        [0.3925],\n",
      "        [0.3033],\n",
      "        [0.3859],\n",
      "        [0.4518],\n",
      "        [0.4153],\n",
      "        [0.5760],\n",
      "        [0.3988],\n",
      "        [0.5258],\n",
      "        [0.4778],\n",
      "        [0.4342],\n",
      "        [0.3939],\n",
      "        [0.4165],\n",
      "        [0.5835],\n",
      "        [0.4182],\n",
      "        [0.4061],\n",
      "        [0.6304],\n",
      "        [0.5006],\n",
      "        [0.3618],\n",
      "        [0.5770],\n",
      "        [0.5068],\n",
      "        [0.5334],\n",
      "        [0.4223],\n",
      "        [0.3138],\n",
      "        [0.4502],\n",
      "        [0.4741],\n",
      "        [0.4111],\n",
      "        [0.3346],\n",
      "        [0.4407],\n",
      "        [0.4742],\n",
      "        [0.4461],\n",
      "        [0.4485],\n",
      "        [0.5542],\n",
      "        [0.4490],\n",
      "        [0.3846],\n",
      "        [0.4846],\n",
      "        [0.4556],\n",
      "        [0.4027],\n",
      "        [0.4637],\n",
      "        [0.4554],\n",
      "        [0.5668],\n",
      "        [0.5362],\n",
      "        [0.4223],\n",
      "        [0.4032],\n",
      "        [0.6108],\n",
      "        [0.2712],\n",
      "        [0.4343],\n",
      "        [0.4710],\n",
      "        [0.4358],\n",
      "        [0.4840],\n",
      "        [0.4344],\n",
      "        [0.3932],\n",
      "        [0.4827],\n",
      "        [0.3909],\n",
      "        [0.4725],\n",
      "        [0.4780],\n",
      "        [0.4340],\n",
      "        [0.4627],\n",
      "        [0.5415],\n",
      "        [0.4283],\n",
      "        [0.5223],\n",
      "        [0.4536],\n",
      "        [0.4353],\n",
      "        [0.4011],\n",
      "        [0.4023],\n",
      "        [0.4399],\n",
      "        [0.4707],\n",
      "        [0.3049],\n",
      "        [0.6313],\n",
      "        [0.4333],\n",
      "        [0.3862],\n",
      "        [0.4803],\n",
      "        [0.5295],\n",
      "        [0.5579],\n",
      "        [0.4258],\n",
      "        [0.4286],\n",
      "        [0.3956],\n",
      "        [0.5545],\n",
      "        [0.5445],\n",
      "        [0.3253],\n",
      "        [0.4104],\n",
      "        [0.3821],\n",
      "        [0.4852],\n",
      "        [0.4070],\n",
      "        [0.4671],\n",
      "        [0.3925],\n",
      "        [0.6226],\n",
      "        [0.3327],\n",
      "        [0.4916],\n",
      "        [0.4879],\n",
      "        [0.5137],\n",
      "        [0.4378],\n",
      "        [0.4187],\n",
      "        [0.4169],\n",
      "        [0.3907],\n",
      "        [0.3978],\n",
      "        [0.4154],\n",
      "        [0.5034],\n",
      "        [0.6428],\n",
      "        [0.5757],\n",
      "        [0.4485],\n",
      "        [0.4772],\n",
      "        [0.4921],\n",
      "        [0.3649],\n",
      "        [0.4181],\n",
      "        [0.3962],\n",
      "        [0.3582],\n",
      "        [0.5391],\n",
      "        [0.4045],\n",
      "        [0.5192],\n",
      "        [0.4468],\n",
      "        [0.5208],\n",
      "        [0.4614],\n",
      "        [0.6193],\n",
      "        [0.5858],\n",
      "        [0.5740],\n",
      "        [0.4804],\n",
      "        [0.3816],\n",
      "        [0.4337],\n",
      "        [0.4440],\n",
      "        [0.4299],\n",
      "        [0.4150],\n",
      "        [0.5486],\n",
      "        [0.3992],\n",
      "        [0.5517],\n",
      "        [0.4625],\n",
      "        [0.5609],\n",
      "        [0.5542],\n",
      "        [0.4106],\n",
      "        [0.5735],\n",
      "        [0.3859],\n",
      "        [0.4835],\n",
      "        [0.4370],\n",
      "        [0.3952],\n",
      "        [0.4500],\n",
      "        [0.3898],\n",
      "        [0.4836],\n",
      "        [0.4992],\n",
      "        [0.4668],\n",
      "        [0.4930],\n",
      "        [0.5341],\n",
      "        [0.2655],\n",
      "        [0.4539],\n",
      "        [0.4765],\n",
      "        [0.4010],\n",
      "        [0.5775],\n",
      "        [0.4242],\n",
      "        [0.4566],\n",
      "        [0.3728],\n",
      "        [0.4561],\n",
      "        [0.3413],\n",
      "        [0.4243],\n",
      "        [0.4131],\n",
      "        [0.4804],\n",
      "        [0.3505],\n",
      "        [0.4197],\n",
      "        [0.4137],\n",
      "        [0.4074],\n",
      "        [0.4560],\n",
      "        [0.4640],\n",
      "        [0.5022],\n",
      "        [0.4603],\n",
      "        [0.5149],\n",
      "        [0.4662],\n",
      "        [0.5919],\n",
      "        [0.5625],\n",
      "        [0.4292],\n",
      "        [0.4530],\n",
      "        [0.4307],\n",
      "        [0.4837],\n",
      "        [0.3953],\n",
      "        [0.5054],\n",
      "        [0.4396],\n",
      "        [0.3197],\n",
      "        [0.4393],\n",
      "        [0.4219],\n",
      "        [0.5391],\n",
      "        [0.4663],\n",
      "        [0.4760],\n",
      "        [0.3233],\n",
      "        [0.5674],\n",
      "        [0.3249],\n",
      "        [0.3976],\n",
      "        [0.6733],\n",
      "        [0.4906],\n",
      "        [0.5376],\n",
      "        [0.3615],\n",
      "        [0.3718],\n",
      "        [0.3923],\n",
      "        [0.4709],\n",
      "        [0.5343],\n",
      "        [0.4719],\n",
      "        [0.4540],\n",
      "        [0.4677],\n",
      "        [0.5102],\n",
      "        [0.5308],\n",
      "        [0.3780],\n",
      "        [0.3786],\n",
      "        [0.4495],\n",
      "        [0.4111],\n",
      "        [0.4094],\n",
      "        [0.3616],\n",
      "        [0.3152],\n",
      "        [0.4446],\n",
      "        [0.2912],\n",
      "        [0.4058],\n",
      "        [0.4004],\n",
      "        [0.4742],\n",
      "        [0.5013],\n",
      "        [0.5112],\n",
      "        [0.5754],\n",
      "        [0.3287],\n",
      "        [0.4194],\n",
      "        [0.3961],\n",
      "        [0.4120],\n",
      "        [0.3849],\n",
      "        [0.3859],\n",
      "        [0.4353],\n",
      "        [0.4307],\n",
      "        [0.3882],\n",
      "        [0.4122],\n",
      "        [0.3735],\n",
      "        [0.3928],\n",
      "        [0.4503],\n",
      "        [0.4269],\n",
      "        [0.3740],\n",
      "        [0.4144],\n",
      "        [0.3546],\n",
      "        [0.4247],\n",
      "        [0.4396],\n",
      "        [0.3901],\n",
      "        [0.4353],\n",
      "        [0.3950],\n",
      "        [0.5739],\n",
      "        [0.4260],\n",
      "        [0.5487],\n",
      "        [0.4310],\n",
      "        [0.4828],\n",
      "        [0.3951],\n",
      "        [0.5264],\n",
      "        [0.4070],\n",
      "        [0.4376],\n",
      "        [0.3949],\n",
      "        [0.5170],\n",
      "        [0.3633],\n",
      "        [0.5080],\n",
      "        [0.4156],\n",
      "        [0.5947],\n",
      "        [0.4470],\n",
      "        [0.3786],\n",
      "        [0.3767],\n",
      "        [0.4851],\n",
      "        [0.5875],\n",
      "        [0.5105],\n",
      "        [0.3726],\n",
      "        [0.4917],\n",
      "        [0.4017],\n",
      "        [0.4376],\n",
      "        [0.4165],\n",
      "        [0.3965],\n",
      "        [0.3876],\n",
      "        [0.3130],\n",
      "        [0.3229],\n",
      "        [0.5450],\n",
      "        [0.4336],\n",
      "        [0.3934],\n",
      "        [0.5436],\n",
      "        [0.4547],\n",
      "        [0.4855],\n",
      "        [0.5779],\n",
      "        [0.3028],\n",
      "        [0.4053],\n",
      "        [0.4345],\n",
      "        [0.3278],\n",
      "        [0.4350],\n",
      "        [0.4815],\n",
      "        [0.4203],\n",
      "        [0.4174],\n",
      "        [0.4367],\n",
      "        [0.3354],\n",
      "        [0.4220],\n",
      "        [0.4260],\n",
      "        [0.4502],\n",
      "        [0.3739],\n",
      "        [0.3666],\n",
      "        [0.3406],\n",
      "        [0.4063],\n",
      "        [0.4955],\n",
      "        [0.3972],\n",
      "        [0.4868],\n",
      "        [0.3812],\n",
      "        [0.4282],\n",
      "        [0.4214],\n",
      "        [0.3565],\n",
      "        [0.6012],\n",
      "        [0.4613],\n",
      "        [0.5050],\n",
      "        [0.5192],\n",
      "        [0.3661],\n",
      "        [0.4904],\n",
      "        [0.3505],\n",
      "        [0.4165],\n",
      "        [0.4102],\n",
      "        [0.3070],\n",
      "        [0.4521],\n",
      "        [0.3868],\n",
      "        [0.4292],\n",
      "        [0.3787],\n",
      "        [0.4126],\n",
      "        [0.4653],\n",
      "        [0.3764],\n",
      "        [0.3850],\n",
      "        [0.4988],\n",
      "        [0.5173],\n",
      "        [0.4400],\n",
      "        [0.4589],\n",
      "        [0.4075],\n",
      "        [0.4749],\n",
      "        [0.4566],\n",
      "        [0.4461],\n",
      "        [0.3556],\n",
      "        [0.4545],\n",
      "        [0.4078],\n",
      "        [0.4429],\n",
      "        [0.5807],\n",
      "        [0.4167],\n",
      "        [0.4566],\n",
      "        [0.5202],\n",
      "        [0.3617],\n",
      "        [0.4211],\n",
      "        [0.5318],\n",
      "        [0.4366],\n",
      "        [0.4606],\n",
      "        [0.4912],\n",
      "        [0.4439],\n",
      "        [0.4168],\n",
      "        [0.4186],\n",
      "        [0.4087],\n",
      "        [0.4055],\n",
      "        [0.4960],\n",
      "        [0.5183],\n",
      "        [0.3781],\n",
      "        [0.5460],\n",
      "        [0.5185],\n",
      "        [0.4842],\n",
      "        [0.3756],\n",
      "        [0.4721],\n",
      "        [0.5195],\n",
      "        [0.4819],\n",
      "        [0.3954],\n",
      "        [0.4513],\n",
      "        [0.4993],\n",
      "        [0.4953],\n",
      "        [0.4831],\n",
      "        [0.3211],\n",
      "        [0.5297],\n",
      "        [0.4642],\n",
      "        [0.4334],\n",
      "        [0.3436],\n",
      "        [0.4608],\n",
      "        [0.4426],\n",
      "        [0.4049],\n",
      "        [0.3528],\n",
      "        [0.4897],\n",
      "        [0.4673],\n",
      "        [0.4006],\n",
      "        [0.4040],\n",
      "        [0.3076],\n",
      "        [0.3248],\n",
      "        [0.5190],\n",
      "        [0.4972],\n",
      "        [0.3814],\n",
      "        [0.4705],\n",
      "        [0.2897],\n",
      "        [0.3369],\n",
      "        [0.4815],\n",
      "        [0.4801],\n",
      "        [0.5430],\n",
      "        [0.3202],\n",
      "        [0.3352],\n",
      "        [0.3635],\n",
      "        [0.4858],\n",
      "        [0.3089],\n",
      "        [0.3771],\n",
      "        [0.3688],\n",
      "        [0.4889],\n",
      "        [0.3547],\n",
      "        [0.4294],\n",
      "        [0.4281],\n",
      "        [0.4334],\n",
      "        [0.2625],\n",
      "        [0.5066],\n",
      "        [0.3757],\n",
      "        [0.4742],\n",
      "        [0.6729],\n",
      "        [0.4411],\n",
      "        [0.4196],\n",
      "        [0.4313],\n",
      "        [0.5683],\n",
      "        [0.3795],\n",
      "        [0.2844],\n",
      "        [0.6132],\n",
      "        [0.3766],\n",
      "        [0.5197],\n",
      "        [0.3365],\n",
      "        [0.3593],\n",
      "        [0.4599],\n",
      "        [0.5364],\n",
      "        [0.4093],\n",
      "        [0.3552],\n",
      "        [0.4569],\n",
      "        [0.5007],\n",
      "        [0.2890],\n",
      "        [0.3501],\n",
      "        [0.5468],\n",
      "        [0.3670],\n",
      "        [0.4617],\n",
      "        [0.5194],\n",
      "        [0.3985],\n",
      "        [0.3949],\n",
      "        [0.5233],\n",
      "        [0.3507],\n",
      "        [0.3910],\n",
      "        [0.3628],\n",
      "        [0.4788],\n",
      "        [0.5239],\n",
      "        [0.4461],\n",
      "        [0.5151],\n",
      "        [0.4324],\n",
      "        [0.2863],\n",
      "        [0.4939],\n",
      "        [0.4127],\n",
      "        [0.3740],\n",
      "        [0.4561],\n",
      "        [0.3403],\n",
      "        [0.2582],\n",
      "        [0.4765],\n",
      "        [0.4452],\n",
      "        [0.4200],\n",
      "        [0.5239],\n",
      "        [0.3326],\n",
      "        [0.3505],\n",
      "        [0.4271],\n",
      "        [0.4565],\n",
      "        [0.4189],\n",
      "        [0.3524],\n",
      "        [0.3842],\n",
      "        [0.3562],\n",
      "        [0.3879],\n",
      "        [0.4973],\n",
      "        [0.5390],\n",
      "        [0.5608],\n",
      "        [0.3232],\n",
      "        [0.3316],\n",
      "        [0.4610],\n",
      "        [0.4404],\n",
      "        [0.3835],\n",
      "        [0.5086],\n",
      "        [0.4371],\n",
      "        [0.4115],\n",
      "        [0.4914],\n",
      "        [0.4688],\n",
      "        [0.3951],\n",
      "        [0.5472],\n",
      "        [0.4122],\n",
      "        [0.4378],\n",
      "        [0.4366],\n",
      "        [0.4590],\n",
      "        [0.4040],\n",
      "        [0.5475],\n",
      "        [0.3592],\n",
      "        [0.4331],\n",
      "        [0.4598],\n",
      "        [0.4890],\n",
      "        [0.4759],\n",
      "        [0.4044],\n",
      "        [0.5386],\n",
      "        [0.3578],\n",
      "        [0.4736],\n",
      "        [0.4819],\n",
      "        [0.3402],\n",
      "        [0.6153],\n",
      "        [0.4907],\n",
      "        [0.4932],\n",
      "        [0.5415],\n",
      "        [0.3971],\n",
      "        [0.4042],\n",
      "        [0.4193],\n",
      "        [0.5160],\n",
      "        [0.5212],\n",
      "        [0.3638],\n",
      "        [0.3906],\n",
      "        [0.3333],\n",
      "        [0.3414],\n",
      "        [0.5468],\n",
      "        [0.4693],\n",
      "        [0.4031],\n",
      "        [0.4726],\n",
      "        [0.4785],\n",
      "        [0.4400],\n",
      "        [0.3894],\n",
      "        [0.4000],\n",
      "        [0.3236],\n",
      "        [0.4177],\n",
      "        [0.5040],\n",
      "        [0.4490],\n",
      "        [0.4553],\n",
      "        [0.3921],\n",
      "        [0.5388],\n",
      "        [0.4969],\n",
      "        [0.3749],\n",
      "        [0.4416],\n",
      "        [0.4014],\n",
      "        [0.5041],\n",
      "        [0.4300],\n",
      "        [0.3710],\n",
      "        [0.5476],\n",
      "        [0.3573],\n",
      "        [0.4140],\n",
      "        [0.4308],\n",
      "        [0.4169],\n",
      "        [0.4263],\n",
      "        [0.4065],\n",
      "        [0.5238],\n",
      "        [0.3778],\n",
      "        [0.4445],\n",
      "        [0.4127],\n",
      "        [0.4294],\n",
      "        [0.5482],\n",
      "        [0.3724],\n",
      "        [0.4827],\n",
      "        [0.5467],\n",
      "        [0.4421],\n",
      "        [0.3986],\n",
      "        [0.3543],\n",
      "        [0.4670],\n",
      "        [0.4444],\n",
      "        [0.6340],\n",
      "        [0.3802],\n",
      "        [0.3066],\n",
      "        [0.4751],\n",
      "        [0.3816],\n",
      "        [0.3857],\n",
      "        [0.4269],\n",
      "        [0.3283],\n",
      "        [0.4066],\n",
      "        [0.4017],\n",
      "        [0.4739],\n",
      "        [0.4759],\n",
      "        [0.5302],\n",
      "        [0.5469],\n",
      "        [0.4216],\n",
      "        [0.3316],\n",
      "        [0.3710],\n",
      "        [0.5368],\n",
      "        [0.4183],\n",
      "        [0.2855],\n",
      "        [0.4667],\n",
      "        [0.4924],\n",
      "        [0.5461],\n",
      "        [0.4657],\n",
      "        [0.4885],\n",
      "        [0.3719],\n",
      "        [0.4391],\n",
      "        [0.5088],\n",
      "        [0.4306],\n",
      "        [0.3871],\n",
      "        [0.4694],\n",
      "        [0.3384],\n",
      "        [0.4380],\n",
      "        [0.3838],\n",
      "        [0.5143],\n",
      "        [0.4357],\n",
      "        [0.4309],\n",
      "        [0.3516],\n",
      "        [0.5076],\n",
      "        [0.5553],\n",
      "        [0.4331],\n",
      "        [0.4370],\n",
      "        [0.5745],\n",
      "        [0.3554],\n",
      "        [0.3721],\n",
      "        [0.3203],\n",
      "        [0.3898],\n",
      "        [0.3969],\n",
      "        [0.3179],\n",
      "        [0.5852],\n",
      "        [0.5792],\n",
      "        [0.4913],\n",
      "        [0.4996],\n",
      "        [0.3928],\n",
      "        [0.4055],\n",
      "        [0.5460],\n",
      "        [0.5703],\n",
      "        [0.4345],\n",
      "        [0.4570],\n",
      "        [0.5243],\n",
      "        [0.4698],\n",
      "        [0.4820],\n",
      "        [0.4122],\n",
      "        [0.4138],\n",
      "        [0.4066],\n",
      "        [0.5238],\n",
      "        [0.4402],\n",
      "        [0.4803],\n",
      "        [0.4424],\n",
      "        [0.5384],\n",
      "        [0.5128],\n",
      "        [0.4629],\n",
      "        [0.6091],\n",
      "        [0.5042],\n",
      "        [0.5742],\n",
      "        [0.5002],\n",
      "        [0.4024],\n",
      "        [0.4056],\n",
      "        [0.5584],\n",
      "        [0.5318],\n",
      "        [0.5227],\n",
      "        [0.4821],\n",
      "        [0.4520],\n",
      "        [0.4851],\n",
      "        [0.4517],\n",
      "        [0.3802],\n",
      "        [0.3754],\n",
      "        [0.4620],\n",
      "        [0.4506],\n",
      "        [0.4233],\n",
      "        [0.3652],\n",
      "        [0.3714],\n",
      "        [0.5273],\n",
      "        [0.3453],\n",
      "        [0.4836],\n",
      "        [0.3752],\n",
      "        [0.4365],\n",
      "        [0.4097],\n",
      "        [0.4198],\n",
      "        [0.4133],\n",
      "        [0.3716],\n",
      "        [0.4543],\n",
      "        [0.4873],\n",
      "        [0.4472],\n",
      "        [0.4011],\n",
      "        [0.3676],\n",
      "        [0.3474],\n",
      "        [0.4033],\n",
      "        [0.3286],\n",
      "        [0.4276],\n",
      "        [0.4752],\n",
      "        [0.6195],\n",
      "        [0.5109],\n",
      "        [0.5280],\n",
      "        [0.3936],\n",
      "        [0.3806],\n",
      "        [0.4097],\n",
      "        [0.4522],\n",
      "        [0.4602],\n",
      "        [0.5002],\n",
      "        [0.5222],\n",
      "        [0.3262],\n",
      "        [0.4046],\n",
      "        [0.5035],\n",
      "        [0.4956],\n",
      "        [0.4199],\n",
      "        [0.5258],\n",
      "        [0.4672],\n",
      "        [0.4190],\n",
      "        [0.5558],\n",
      "        [0.3514],\n",
      "        [0.3958],\n",
      "        [0.4419],\n",
      "        [0.5245],\n",
      "        [0.2748],\n",
      "        [0.3320],\n",
      "        [0.4210],\n",
      "        [0.5174],\n",
      "        [0.4533],\n",
      "        [0.4260],\n",
      "        [0.3085],\n",
      "        [0.5401],\n",
      "        [0.4765],\n",
      "        [0.4726],\n",
      "        [0.3937],\n",
      "        [0.4041],\n",
      "        [0.4805],\n",
      "        [0.3947],\n",
      "        [0.3822],\n",
      "        [0.5877],\n",
      "        [0.4430],\n",
      "        [0.3601],\n",
      "        [0.3071],\n",
      "        [0.3719],\n",
      "        [0.4106],\n",
      "        [0.3290],\n",
      "        [0.4211],\n",
      "        [0.5398],\n",
      "        [0.5571],\n",
      "        [0.3744],\n",
      "        [0.3342],\n",
      "        [0.3843],\n",
      "        [0.4907],\n",
      "        [0.3735],\n",
      "        [0.4037],\n",
      "        [0.4372],\n",
      "        [0.3937],\n",
      "        [0.4448],\n",
      "        [0.3924],\n",
      "        [0.4700],\n",
      "        [0.3379],\n",
      "        [0.3496],\n",
      "        [0.3987],\n",
      "        [0.4627],\n",
      "        [0.3123],\n",
      "        [0.5360],\n",
      "        [0.5299],\n",
      "        [0.5349],\n",
      "        [0.3646],\n",
      "        [0.5767],\n",
      "        [0.4651],\n",
      "        [0.6004],\n",
      "        [0.5384],\n",
      "        [0.5549],\n",
      "        [0.4102],\n",
      "        [0.5193],\n",
      "        [0.4440],\n",
      "        [0.3322],\n",
      "        [0.4466],\n",
      "        [0.4396],\n",
      "        [0.4389],\n",
      "        [0.4928],\n",
      "        [0.5258],\n",
      "        [0.4476],\n",
      "        [0.5524],\n",
      "        [0.3924],\n",
      "        [0.5778],\n",
      "        [0.4061],\n",
      "        [0.3955],\n",
      "        [0.4701],\n",
      "        [0.3739],\n",
      "        [0.4729],\n",
      "        [0.3857],\n",
      "        [0.3697],\n",
      "        [0.4861],\n",
      "        [0.4257],\n",
      "        [0.4555],\n",
      "        [0.3764],\n",
      "        [0.4004],\n",
      "        [0.4873],\n",
      "        [0.3717],\n",
      "        [0.5401],\n",
      "        [0.5077],\n",
      "        [0.4052],\n",
      "        [0.4040],\n",
      "        [0.5897],\n",
      "        [0.4215],\n",
      "        [0.4430],\n",
      "        [0.4059],\n",
      "        [0.3712],\n",
      "        [0.3780],\n",
      "        [0.4235],\n",
      "        [0.3975],\n",
      "        [0.3364],\n",
      "        [0.4434],\n",
      "        [0.3877],\n",
      "        [0.4699],\n",
      "        [0.4348],\n",
      "        [0.3696],\n",
      "        [0.3550],\n",
      "        [0.2882],\n",
      "        [0.4387],\n",
      "        [0.2408],\n",
      "        [0.5810],\n",
      "        [0.4084],\n",
      "        [0.5402],\n",
      "        [0.4574],\n",
      "        [0.5954],\n",
      "        [0.3437],\n",
      "        [0.5475],\n",
      "        [0.4739],\n",
      "        [0.5174],\n",
      "        [0.3818],\n",
      "        [0.3676],\n",
      "        [0.2750],\n",
      "        [0.3178],\n",
      "        [0.4954],\n",
      "        [0.4528],\n",
      "        [0.4960],\n",
      "        [0.3808],\n",
      "        [0.5690],\n",
      "        [0.5643],\n",
      "        [0.3420],\n",
      "        [0.5657],\n",
      "        [0.5447],\n",
      "        [0.4217],\n",
      "        [0.5636],\n",
      "        [0.5015],\n",
      "        [0.5193],\n",
      "        [0.3813],\n",
      "        [0.4031],\n",
      "        [0.4860],\n",
      "        [0.3482],\n",
      "        [0.4781],\n",
      "        [0.3551],\n",
      "        [0.3608],\n",
      "        [0.3947],\n",
      "        [0.4233],\n",
      "        [0.5107],\n",
      "        [0.5963],\n",
      "        [0.2877],\n",
      "        [0.4952],\n",
      "        [0.3472],\n",
      "        [0.4124],\n",
      "        [0.4840],\n",
      "        [0.4079],\n",
      "        [0.3816],\n",
      "        [0.3958],\n",
      "        [0.4095],\n",
      "        [0.5205],\n",
      "        [0.4024],\n",
      "        [0.5351],\n",
      "        [0.5297],\n",
      "        [0.5664],\n",
      "        [0.3380],\n",
      "        [0.3888],\n",
      "        [0.4159],\n",
      "        [0.3657],\n",
      "        [0.4971],\n",
      "        [0.5367],\n",
      "        [0.3984],\n",
      "        [0.4621],\n",
      "        [0.4429],\n",
      "        [0.4362],\n",
      "        [0.4244],\n",
      "        [0.3897],\n",
      "        [0.4805],\n",
      "        [0.3631],\n",
      "        [0.3980],\n",
      "        [0.5186],\n",
      "        [0.4831],\n",
      "        [0.5617],\n",
      "        [0.4694],\n",
      "        [0.4512],\n",
      "        [0.4364],\n",
      "        [0.3838],\n",
      "        [0.4493],\n",
      "        [0.4691],\n",
      "        [0.5110],\n",
      "        [0.4251],\n",
      "        [0.3345],\n",
      "        [0.3591],\n",
      "        [0.3035],\n",
      "        [0.4014],\n",
      "        [0.5022],\n",
      "        [0.3842],\n",
      "        [0.3727],\n",
      "        [0.4623],\n",
      "        [0.4387],\n",
      "        [0.5181],\n",
      "        [0.4179],\n",
      "        [0.5070],\n",
      "        [0.4303],\n",
      "        [0.4205],\n",
      "        [0.4736],\n",
      "        [0.5758],\n",
      "        [0.4236],\n",
      "        [0.3188],\n",
      "        [0.5719],\n",
      "        [0.5597],\n",
      "        [0.4696],\n",
      "        [0.5084],\n",
      "        [0.4326],\n",
      "        [0.5894],\n",
      "        [0.3951],\n",
      "        [0.4219],\n",
      "        [0.4393],\n",
      "        [0.4308],\n",
      "        [0.5164],\n",
      "        [0.4674],\n",
      "        [0.5289],\n",
      "        [0.5058],\n",
      "        [0.3708],\n",
      "        [0.4446],\n",
      "        [0.4288],\n",
      "        [0.3478],\n",
      "        [0.4385],\n",
      "        [0.4169],\n",
      "        [0.4090],\n",
      "        [0.4358],\n",
      "        [0.3992],\n",
      "        [0.3699],\n",
      "        [0.3413],\n",
      "        [0.3917],\n",
      "        [0.4402],\n",
      "        [0.4873],\n",
      "        [0.4089],\n",
      "        [0.4266],\n",
      "        [0.5834],\n",
      "        [0.4622],\n",
      "        [0.4238],\n",
      "        [0.4263],\n",
      "        [0.4253],\n",
      "        [0.5219],\n",
      "        [0.4363],\n",
      "        [0.4761],\n",
      "        [0.4308],\n",
      "        [0.4107],\n",
      "        [0.3806],\n",
      "        [0.4344],\n",
      "        [0.3719],\n",
      "        [0.4481],\n",
      "        [0.5072],\n",
      "        [0.5462],\n",
      "        [0.3704],\n",
      "        [0.4317],\n",
      "        [0.3989],\n",
      "        [0.4227],\n",
      "        [0.3933],\n",
      "        [0.5512],\n",
      "        [0.4242],\n",
      "        [0.4644],\n",
      "        [0.4768],\n",
      "        [0.4511],\n",
      "        [0.4190],\n",
      "        [0.4548],\n",
      "        [0.5210],\n",
      "        [0.3296],\n",
      "        [0.3802],\n",
      "        [0.4408],\n",
      "        [0.4143],\n",
      "        [0.3751],\n",
      "        [0.4588],\n",
      "        [0.4259],\n",
      "        [0.3618],\n",
      "        [0.5051],\n",
      "        [0.5080],\n",
      "        [0.3764],\n",
      "        [0.4438],\n",
      "        [0.4437],\n",
      "        [0.4868],\n",
      "        [0.4327],\n",
      "        [0.3130],\n",
      "        [0.4913],\n",
      "        [0.4038],\n",
      "        [0.3766],\n",
      "        [0.3730],\n",
      "        [0.5329],\n",
      "        [0.4128],\n",
      "        [0.4706],\n",
      "        [0.5194],\n",
      "        [0.5305],\n",
      "        [0.3970],\n",
      "        [0.3890],\n",
      "        [0.3447],\n",
      "        [0.4150],\n",
      "        [0.3433],\n",
      "        [0.6121],\n",
      "        [0.4187],\n",
      "        [0.4230],\n",
      "        [0.3437],\n",
      "        [0.4550],\n",
      "        [0.5248],\n",
      "        [0.4211],\n",
      "        [0.4454],\n",
      "        [0.3662],\n",
      "        [0.5848],\n",
      "        [0.3739],\n",
      "        [0.4830],\n",
      "        [0.4701],\n",
      "        [0.5101],\n",
      "        [0.3968],\n",
      "        [0.3930],\n",
      "        [0.5310],\n",
      "        [0.3310],\n",
      "        [0.5304],\n",
      "        [0.4739],\n",
      "        [0.5522],\n",
      "        [0.3477],\n",
      "        [0.4428],\n",
      "        [0.4743],\n",
      "        [0.4323],\n",
      "        [0.3491],\n",
      "        [0.4881],\n",
      "        [0.4339],\n",
      "        [0.3941]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "        self.out_act = torch.nn.Sigmoid()\n",
    "#         self.out_act = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.out(x)\n",
    "        x = self.out_act(x)\n",
    "        return x\n",
    "\n",
    "net = Net(n_feature=8, n_hidden=100, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.02)\n",
    "# loss_func = torch.nn.CrossEntropyLoss()  # the target label is NOT an one-hotted\n",
    "loss_func = torch.nn.BCELoss()\n",
    "\n",
    "out = net(x)                 # input x and predict based on x\n",
    "loss = loss_func(out, y_new) \n",
    "print(out)\n",
    "# print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_epoch(model, opt, criterion, batch_size=50):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for beg_i in range(0, X.size(0), batch_size):\n",
    "        x_batch = X[beg_i:beg_i + batch_size, :]\n",
    "        y_batch = Y[beg_i:beg_i + batch_size, :]\n",
    "        x_batch = Variable(x_batch)\n",
    "        y_batch = Variable(y_batch)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        # (1) Forward\n",
    "        y_hat = net(x_batch)\n",
    "        # (2) Compute diff\n",
    "        loss = criterion(y_hat, y_batch)\n",
    "        # (3) Compute gradients\n",
    "        loss.backward()\n",
    "        # (4) update weights\n",
    "        opt.step()        \n",
    "        losses.append(loss.data.numpy())\n",
    "    return losses\n",
    "\n",
    "e_losses = []\n",
    "num_epochs = 20\n",
    "for e in range(num_epochs):\n",
    "    e_losses += train_epoch(net, opt, criterion)\n",
    "plt.plot(e_losses)\n",
    "\n",
    "# plt.ion()   # something about plotting\n",
    "\n",
    "# for t in range(100):\n",
    "#     out = net(x)                 # input x and predict based on x\n",
    "#     loss = loss_func(out, y_new)     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "\n",
    "#     optimizer.zero_grad()   # clear gradients for next train\n",
    "#     loss.backward()         # backpropagation, compute gradients\n",
    "#     optimizer.step()        # apply gradients\n",
    "    \n",
    "# #     print(loss.item())\n",
    "#     print(loss.data.numpy())\n",
    "\n",
    "# #     if t % 2 == 0:\n",
    "# #         # plot and show learning process\n",
    "# #         plt.cla()\n",
    "# #         prediction = torch.max(out, 1)[1]\n",
    "# #         pred_y = prediction.data.numpy()\n",
    "# #         target_y = y.data.numpy()\n",
    "# #         plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap='RdYlGn')\n",
    "# #         accuracy = float((pred_y == target_y).astype(int).sum()) / float(target_y.size)\n",
    "# #         plt.text(1.5, -4, 'Accuracy=%.2f' % accuracy, fontdict={'size': 20, 'color':  'red'})\n",
    "# #         plt.pause(0.1)\n",
    "\n",
    "# # plt.ioff()\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(x) \n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
